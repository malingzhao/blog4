<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title></title>
    <url>%2Fblog4%2F2019%2F10%2F11%2FbigdataInterview%2F</url>
    <content type="text"><![CDATA[Michael_PK 大数据团队面试 spark Hadoop 的公有云和私有云 剑指大数据面试概述互联网寒冬 简历投递的活跃度高 大数据岗位 够硬的技术功底 表现能力 拓展和提升大数据相应的能力 java Linux的基础知识 具有一定的项目经 1 开篇就业趋势 高效Offer 2 大数据处理架构总体架构 系统设计 3 小文件引发的血案篇hdfs 小文件问题 4 SQL on Hadoop架构层面的调优 语法层面调优 执行框架底层调优 sql的实战案例 5 数据倾斜篇什么是数据倾斜及产生的原因 大数据中的shuffle 产生数据倾斜的场景 数据倾斜的解决方案 6 spark的调优篇算子的合理选择给计算性能带来的深远影响 合理的序列化整合saprk使用为性能提速 如何保证流处理过程的零数据丢失 流处理数据Sink到目的地的N中错误的操作剖析 案例实战： 如何基于Spark定制外部数据源 7 java篇javaSE 多线程 生产者和消费者 jvm 8 jvm篇ClassLoader机制 内存模型 垃圾回收 垃圾回收算法 垃圾回收器 9 其他篇 zookeeper Linux 10 技巧篇2小文件引发的血案篇1 HDFS架构 学习一个新框架 ​ 百度 文档 不推荐 ​ 开源框架存在问题 推荐 官网+源码 ​ 跪在坚持 ​ hadoop ​ spark ​ flink ​ org.apache.xxxxx hadoop :HDFS/YARN/MapReduce3 HDFS ​ NameNode ​ DataNode ​ SecondaryNameNode ​ 概念 Client NN: 一个，Single Problem Of Failure ====&gt; HA metadata： 谁 权限 文件对应block的信息 文件名 副本信息（生产环境一般3个） 副本机制 ======》》》 增加容错 DN： 多个 存储数据 和NN之间是有心跳的 Block : File存入HDFS 按照block进行拆分 128M 2 HDFS的读写流程HDFS的写流程 HDFS的读流程 请求最近的NameNode 最终 3HDFS HA NameNode挂掉 SecondaryNameNode Active Standby 同步NameNode的状态 共享 快速的切换 两个独立的机器上 一个活动状态一个备份状态 Zookeeper 实现高可用调度系统调度机 zookeeper进行管理持久性的 临时性的 HA 主 和 备的 切换 各自的进程监控zookeeper状态的切换Active Standby 数据的同步 共享 。。。。很多 Yarn的架构 ResourceManager HA NN HA 也有这个问题？ 什么导致 小文件问题RM HA 遇到的问题： 起不来NN HA 也有这个问题？ 什么导致 小文件问题 RM HA 遇到的问题： 起不来 SLA 如何保证？？ 99.99% 99.95% 4 小文件是什么小文件的定义 小文件 明显小于block size的文件 80% 129M 128M + 1M 为什么产生小文件1 批处理 离线计算 小文件 特别是spark 2 数据拷贝到HDFS 没有关注 数据搬迁（手工 Flume采集）没有做很好的设置 3 MapReduce 作业 Reduce 没有做设置 Reduce 决定了输出文件的多少 shuffle合理的设置 hadoop的目录 文件 blk是以元数据的方式存储下来的 200字节 5 小文件给hadoop集群带来的瓶颈问题 100个小文件 IO n多个小文件的IO1 IO开销大2 Map task Reduce Task 的开启和销毁 （task）jvm的启动销毁3 资源有限 3 SQL on Hadoop1 SQL on Hadoop的常用框架常用的SQL on hadoop框架 Hive sql =====》》 对应的sql转换成对应的执行引擎的作业： MapReduce/Spark/Tez Impala： 内存 Presto: 京东 Drill: 跨数据源的查询 Phoenix： HBase（RowKey） 性能高 API + 命令行 sql查询hbase的东西 Spark SQL： 查询结构化数据 MetaStore 存储表的元数据信息 框架之间是共享元数据信息的 Hive on Spark: Hive社区 MR Spark TezSpark SQL： Spark社区 Spark on Hive : X 错误的说法 2 行式存储 vs 列式存储 列式存储 带来很大的性能提升 分表SQL on Hadoop的调优策略 架构层面调优 架构调优之分表 spark ETL操作 Flume ===&gt;&gt; HDFS===&gt;&gt; spark EL ===&gt;&gt; SQL==&gt;&gt;Spark SQL ==&gt; NoSQL 前提 1 行式 2 每分钟2亿条数据 3 500个作业访问这个大表 分区表系统用户日志： who done 语法调优jvm重用 MapTask/ReduceTask都是以进程存在的 ，有多少个task就有多少个jvm 推测执行1 集群中的机器负载是不同的 2 集群中机器的配置不同 3 数据倾斜 一个job100个reduce 99个很快运行完， 只有最后一个话费很撑的执行时间，那么这个job它的运行速度是取决于最慢的一个task 长尾作业 并行执行 默认没有开启 并行的前提多个task之间是没有依赖的 jvm重用MapTask 和ReduceTask 都是以进程的形式存在的，有多少个task就有多少个jvm 当task运行完成结束之后，jvm就会被销毁，jvm的启动和销毁需要开销，每个jvm可以执行多个task 本章总结框架 MetaStore 行式存储 列式存储 调优策略 4 Spark调优篇其他篇分布式锁什么是分布式锁： 公共资源： 同步、加锁 下订单 zookeeper 实现分布式锁 创建一个springboot工程 订单： id itemid 条目： id name counts 1 Spark 10 2 Hadoop 6 3 Flink 3 Spring Data 操作数据库 springdatajpa + mysql 12345jpa: hibernate: ddl-auto: update database: mysql 同一个业务的执行 两个请求一块发起 两个订单同时创建成功 自动创建表的配置 套路 dao service controller domain 业务层 Linu的内容1 检索： 内容​ grep pk1.txt pk2.txt grep “oop” pk*|grep “Spark” 管道操作符 | 多个命令 多个指令 连接起来 前一个指令的结果作为下一个指令的输入 ps -ef ps -ef| grep zookeeper ps -ef|grep zookeeper |grep -v “grep” -v过滤东西 2 对内容的统计awk 获得第一列和第二列 tab键分割的处理 awk ‘{print $1,$2}’ file.txt 拿到头 awk “$1=8888 || NR=1” emp.txt 逗号 awk -F “,” ‘{print $1}’ sales.csv 拿出来 -F 指定分割符 awk -F “,” ‘{print $0}’一行所有的数据 3 对内容的替换sed java的格式转换成scala的 sed ‘s/String/val/‘ test.txt 第一个原来的 替换成 val sed -i ‘s/String/val’ test.txt 完成第二步的操作 sed -i ‘s/;/ /‘ test.txt 默认替换第一个 sed -i ‘s/Hadoop/hadoop/‘ test.txt 想要全局替换 sed -i ‘s/Hadoop/hadoop/g’ test.txt 技巧篇技术一票决定的权利公司的价值观 你的职业规划为什么要离职薪资问题 加班（抗压能力） 周边的同事 领导 项目 ==》》》》》》》》 个人的发展： 加分 个人的成长空间 离职原因不是个人原因 不是频繁的离职 找到更适合自己的平台 家庭的原因： 对象 你对加班的看法常态 公司基本上加班是早上11点 住的地方就在公司附近 有家 单身 为什么选择我们公司公司 海投 打电话 为什么选择你们公司 建议 对于个人特别想去的 建立一定要差异化 投不同的公司 投不一样的简历 编简历 简历的包装 行业 公司 岗位的描述 喜欢 胜任 你的优缺点目的 你这个人对自己有没有对自己的清晰的认识 自我认知 实事求是 优点： 全力以赴 适应能力 缺点： 推辞能力 在意别人的看法 问问题 自己加班加点===》》（乐于助人，人缘，关系融洽）]]></content>
  </entry>
  <entry>
    <title><![CDATA[JUC]]></title>
    <url>%2Fblog4%2F2019%2F10%2F10%2FJUC%2F</url>
    <content type="text"><![CDATA[1 JUC是什么java.util.current 在并发编程中使用的工具类 进程和线程的回顾1 进程是什么 线程是什么进程是操作系统动作执行的基本单元 在传统的操作系统中，进程既是基本的分配单元，也是基本的执行单元 独立功能的程序关于某个数据集合的一次性活动 线程一个进程可以包含若干个线程，一个进程中至少有一个线程， 不然没有存在的意义 线程可以利用进程拥有的资源 进程作为分配资源的基本单位 线程作为独立运行和独立调度的基本单位，基本上不拥有系统资源， 因此对它的调度所付出的开销就小得多 2 进程和线程的例子3 线程的状态new runnable blocked waiting Timed_waiting terminated 4 wait/sleep的区别wait 线程暂停 wait 放开手去睡 放开手里的锁 sleep握紧手去睡 行了手里还有锁 5 什么是并发 什么是并行并发 同一个时刻在访问同一个 资源 并行 多项工作一起执行 之后再汇总 Lock接口复习synchronized线程操作资源类 高内聚 低耦合 实现步骤1 创建资源类 2 资源类创建同步方法，同步代码 example 买票程序 Lock Lock接口实现 可重入锁怎么用 1234567891011121314class X&#123; private final ReetrantLock lock=new ReetrantLock(); //.... public void m() &#123; lock.lock(); try&#123; //....nethod body &#125;finally&#123; lock.unlock(); &#125; &#125;&#125; synchronized和lock的区别1 首先synchronized是java的内置关键字，在jvm层面，lock是一个java类 2 synchronized无法判断是否获取锁的状态，Lock可以判断是否被获取到锁 3 首先synchronized是java的内置关键字，在jvm层面，lock是一个java类 synchronized无法判断是否获取锁的状态，Lock可以判断是否被获取到锁 4 用synchronized关键字的两个线程线程1 和线程2 ，如果线程1 获得锁 线程2 等待 。如果线程1阻塞线程2 会一致等待下去，Lock就不一定会等待下去，如果一致获取不到锁的话，线程可以不用一直等待就结束了 5 synchronized的锁可重入 不可中断，非公平 而Lock锁可重入，可判断， 可公平（两者皆可） 6Lock锁适合大量同步代码的同步问题 synchronized适合代码量少的同步问题 创建线程的方式集成Thread 实现Runnable方法 新建类实现Runnnable接口 12ckass MyThread umplements Runnablenew Thread() 匿名内部类 123456new Thread(new Runnable()&#123; @Override public void run()&#123;&#125;&#125;) lambda表达式 123new Thread(()-&gt;&#123; &#125;,"your thread name").start() Lambda表达式lambda是一个匿名函数 我们可以把lambda表达式理解为可以传递的代码 左侧 lambda表达式需要的参数 右侧 lambda题 lambda表达式要执行的功能 Runnable接口为什么可以使用lambda表达式]]></content>
  </entry>
  <entry>
    <title><![CDATA[weixin]]></title>
    <url>%2Fblog4%2F2019%2F10%2F10%2Fweixin%2F</url>
    <content type="text"><![CDATA[创建公众号的目标 理论指导 基本操作 工具技巧 引流方式 注册流程 如何登陆]]></content>
  </entry>
  <entry>
    <title><![CDATA[git]]></title>
    <url>%2Fblog4%2F2019%2F10%2F10%2Fgit%2F</url>
    <content type="text"><![CDATA[一、Git的基础1 认识git有了SVN 为什么还需要git svn增量式管理 GIT采取文件系统快照的方式 svn是集中式管理 git是分布式管理 其他的优势 大部分的操作在本地不需要互联网，只有pull和push时需要 2 git的安装1 Linux的环境./config make sudo make install gt config –global user.name “YourName” 我们注意到了global参数 用了这个参数，表示这台加的所有的git仓库都会使用这个配置 3 git的结构工作区 git add的暂存区 （临时存储） git commit 本地库（历史版本） 很多人喜欢直接commit到本地库 是因为用svn比较多的缘故 工作区 (.git)=.svn git的版本库里面包含了很多的东西，其中最重要的就是成为stage的暂存区， 还有Git为我们自动创建的第一个分支master 以及指向master的一个指针叫Head svn是没有暂存区的 文件git的版本库里添加的时候 分两步执行 实际上就是把文件修改添加到暂存区 4 Git和代码托管中心局域网 - GitLab服务器 外网- GitLab 、 码云 本地库与远程协作的方式 1 团队的内部协作 2 跨团队的协作 5 Git的命令行操作克隆远程仓库12git clone https://github.com/malingzhao/tuchaung.gitcd 本地仓库 本地库的初始化git add 设置签名作用 区分开发人员的身份 设置的额签名和远程 登录的仓库的账号和密码没有关系 命令 123git config user.name tom_progit config user.email [good_pro@126.com](mailto:good_pro@126.com) cd .git 观察配置信息 项目的优先于用户的优先级 二 命令行的常用的操作1 上传文件git clone cd 仓库地址 gti add 文件名 git commit -m “描述” git push orign master 第一次出现user.name user.email 配置输入即可// 查看路径echo $HOME 查看路径git config –global credential.helper store 创建此文件时一依据此路径]]></content>
  </entry>
  <entry>
    <title><![CDATA[categories]]></title>
    <url>%2Fblog4%2F2019%2F10%2F09%2Fcategories%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Kylin]]></title>
    <url>%2Fblog4%2F2019%2F10%2F09%2FKylin%2F</url>
    <content type="text"><![CDATA[1 概述1.1 Kylin定义开源 分布式 分析引擎 提供基于Hadoop/spark的sql查询和多维分析 OLAP能力（联机分析处理） 亚秒内查询巨大的Hive表 1.2 架构 1 RestServer Rsultful接口 提供查询 2 查询引擎 解析用户查询 3 路由器 在发行版是默认关闭的 体验不好 Hive的速度和Kylin的速度相差加大 4 元数据管理工具 元数据驱动应用程序 kylin的元数据存储在hbase中 5 分析引擎 处理离线任务 shell脚本 java API MapReduce任务 任务引擎需要对kylin中的任务进行协调和管理 1.3特点SQL接口 超大规模数据集 亚秒级的响应 ​ 很多复杂的计算 连接 聚合 离线的预计算过程就完成了 可伸缩性和高吞吐率 搭建集群 每秒70个查询 BI ODBC tableau JDBC RestPI Kylin 2 环境搭建官方文档即可 3 入门4 Cube构建原理5 cube构建优化6 BI集成]]></content>
  </entry>
  <entry>
    <title><![CDATA[zookeeper]]></title>
    <url>%2Fblog4%2F2019%2F10%2F09%2Fzookeeper%2F</url>
    <content type="text"><![CDATA[1 zookeeper入门1.1概述开源 为分布式应用提供协调服 Zookeeper=文件系统+通知机制 1.2 特点Leader follower 半数机制 每个server保存的数据一致（数据备份） 实时性 更新请求顺序执行 1.3 数据结构unix文件系统 形结构 1.4 应用场景统一的配置管理和配置服务 1.5下载地址https://zookeeper.apache.org/ 2 zookeeper的安装2.1 本地模式zoo.cfg bin/zksSrver.sh start|status|stop 出现bash4.1 .bash_profile 每次登陆 .bashrc 每次进入新的bash环境 .bash_logout 每次退出登录 .bash_history 每用户注销前使用的命令 进入root用户 cp .bash_profile .bashrc .bash_logout .bash_profile /home/user 问题得到解决 2.2 配置参数的解读tickTime 2000 initLimit 10 syncLimit dataDir clientPort 3 Zookeeper的内部的原理3.1 选举机制半数机制，适合安装奇数台服务器 没有mater和slave，但是zookeeper在工作的时候 一个leader 其他的都是follower 4 zookeeper的实战分布式部署安装zoo.cfg myid zkData 12345dataDir=/opt/module/zookeeper-3.4.10/zkData#######################cluster##########################server.2=hadoop102:2888:3888server.3=hadoop103:2888:3888server.4=hadoop104:2888:3888 server.A=B:C:D A myid B 主机名称 C 与Leader交换信息 D 一但leader挂掉 选举出新的leader 4.2 API操作5 面试zookeeper的选举机制zookeeper的监听原理main方法 connect线程和listener线程 connect 将监听的事件发送给zookeeper zookeeper在监控l列表上添加事件 listener线程监听到事件变化 zookeeper调用process方法 zookeeper的部署的方式本地部署 分布式部署 zookeeper的常用命令ls create get delete set]]></content>
  </entry>
  <entry>
    <title><![CDATA[dianshangtuijianxitong]]></title>
    <url>%2Fblog4%2F2019%2F10%2F09%2Fdianshangtuijianxitong%2F</url>
    <content type="text"><![CDATA[1 电商推荐系统的简介1.1 项目的架构设计1.1.2 亚马逊推荐系统的贡献 比例达到了20%到25% 在亚马逊的页面上 推荐列表占了很大的比例 真实的业务架构 构建真正的系统 基于统计的模块 实时 自定义的推荐模块 设计到机器学习 和推荐系统的相关知识 电影推荐系统 切换不同的业务场景 一通百通 对大数据的工具有一个更深刻的理解、 做的是电商推荐系统 推荐系统的具体的应用 1.1.2 分析项目框架 数据源解析 统计推荐模块+ 基于LFM的离线推荐模块 基于自定义模型的实时推荐模块 其他形式的离线相似推荐 基于内容的推荐和 ​ 基于物品的协同过滤模块 1.1.3 数据的生命周期 数据源 三大类 图片 视频 非结构化的数据 日志数据 半结构化的数据 结构化数据 关系数据 数据源 数据采集 数据存储 数据计算 数据应用 用的数据库 mongodb 运算处理 （Mahout） hadoop的 storm的流式处理 spark flink 大数据的计算框架 算完之后 存储到数据库里 分析 Echarts 等数据可视化展示 Cassandra NoSQL 1.2 大数据的处理流程 左边的实时的处理 网站 APP 前端页面 用户接口 —》》》》 http请求 —-》》》 业务系统的后台—-》》》》 调用相应的服务 响应—–》》 埋点收集日志 –》》 记录用户的行为 —》》 日志的采集 （Flume） 数据总线 —》》 KAfka （做存储） 实时消费 flume的数据 —-》》 sparkStreaming 实时计算 —-》》 数据的可视化展示 右边的离线处理的流程 业务系统 日志文件 flume 日志采集 sink 配置成hdfs存储 日志清洗ETL 做数据清洗的操作 数据仓库 最后对这些数据进行计算 对数据进行离线计算 相关的业务数据库 最后做可视化展示 推荐系统 —》》 大数据的典型应用 1.3 我们的目标 不同的地方显示不同的推荐的结果 商品的详情 评分 xxx 然后下面相似的商品推荐出来 混合推荐 典型分区混合 1.4 项目的系统架构用户可视化 Angular JS 前端 后台spring 不是我们主要考虑的地方 数据 存储 MongoDB 一些重要的数据 缓存到Redis里面 并不是一定要存MongoDB mongoDB大数据平台很主流的数据库 读写性能 支持很大的数据量 mongo是一个文档型数据库 json串 存储在里面 很多的特征 redis缓存常规操作 ES 模糊查询 条件查询 离线的推荐服务 统计服务 个性化统计服务 在线 实时 Flume kafka 缓冲 sparkStreaming 实时的推荐处理 Redis缓存的数据 写回到mongo里 用户可视化 推荐 1.4.1 离线数据加载服务 数据放到mongo里面 离线统计服务 sparkSQL 写回表 对应的查询 个性化推荐 隐语义模型的推荐模块 基于内容的推荐 最终的结果写进mongo 1.4.2 实时log Flume kafka 消息缓冲 对应的消息处理 spark Streaming对数据过滤 最后做实时的计算 redis里面拿到最近的评分数据 ‘ 商品检索 mongo es也是可以的 1.5 数据源的解析1.5.1 信息介绍 商品信息 1.用户评分信息 基于商品的信息 做基于内容推荐 用户的评分数据 隐语义模型 协同过滤的推荐 1.6主要的数据模型商品相似度 为了实时推荐做基础 实时 1.6 统计推荐模块商品相似度 1.6.1 历史热门商品统计什么样的商品是热门的 评分的多少 1select productId ,count(productId) as count from ratings group by productId order by count desc RatingMoreProducts 1.6.2近期热门商品的统计UDF函数 changeDate 时间戳 转换为年月的格式 分解成一个月的的热门商品 1.6.3商品的平均得分的统计统计的指标 相应的实现 1.7 离线推荐模块ALS算法进行隐语义模型训练 ALS.train() lambda 正则化参数 iterations 迭代次数 隐语义模型定义的隐特征的个数 涉及到了模型评估和参数调整 RMSE 考察预测评分和实际评分的误差 得到选取什么样的参数是最好的 1.7.1 计算用户推荐矩阵 user的RDD 和product的RDD做了一个笛卡尔积 物品两两匹配得到的结果 model.predict uid 聚合 sortBy（“score”.take(20) sparkSession.write 1.7.2计算商品相似度矩阵特征向量 矩阵分解 用户的特征向量矩阵 商品的特征向量矩阵 笛卡尔积 给实时推荐做基础 1.8 基于模型的实时推荐模块 1.8.1 需求计算速度快 结果可以不是特别精确 有预先设计好的推荐模型 评分数据 flume kafka log mongo 里面 redis里 结果写回mongo 1.8.2推荐优先级的计算刚看的商品 差评的物品 综合考虑相似度和评分 对剑优先级 计算公式 .推荐的基础评分项 奖励 惩罚 incount 评分里面的高分项 AB 高分 奖励 C 低分 惩罚 前面的基础项加权 lg 2 AB高分 C是低分 1.9 其他形式的离线相似推荐点开用户的商品详情页 出现相似的内容 基于用户购买了哪些商品 1.9.1 基于内容的推荐与A有相同的标签的商品 TF-IDF算法 根据UGC的特征提取item-CF算法 喜欢商品A的人还喜欢哪些商品 TF 词频 每一个标签 词语 每一个商品获得的一个标签 提取出物品的特征向量 余弦相似度 1.9.3 基于物品的协同过滤 1.10 混合推荐分区推荐 同时购买了商品i和商品j 2 环境的搭建2.1 安装monhgodb1234567891011wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel62-3.4.3.tgztar xvf 到 ./cluster然后mv /usr/local/mongodbcd /usr/local/mongodb创建 /usr/local/mongodb/data/usr/local/mongodb/data/db/usr/local/mongodb/data/logs/usr/local/mongodb/data/logs/mongodb.log/usr/local/mongodb/data/mongodb.conf mongodb.conf 1234567891011#端口号port = 27017#数据目录dbpath = /usr/local/mongodb/data/db#日志目录logpath = /usr/local/mongodb/data/logs/mongodb.log#设置后台运行fork = true#日志输出方式logappend = true#开启认证#auth = true 2.1.1 mongodb的启动1sudo /usr/local/mongodb/bin/mongod -config /usr/local/mongodb/data/mongodb.conf 2.1.2 mongodb的访问1/usr/local/mongodb/bin/mongo 2.1.3 mongodb的停止1/usr/local/mongodb/bin/mongo 2.2 安装Redis2.2.1 Redis的基本安装获得安装包 12wget http://download.redis.io/releases/redis-4.0.2.tar.gz 解压 tar xvf xxx ./clsuetr cd redis 1sudo yum install gcc 编译源代码 1make MALLOC=libc 编译安装 1sudo make install 创建配置文件 1sudo cp ./redis-4.0.2/redis.conf /etc/redis.conf 修改配置文件 12345daemonize yes #37行 #是否以后台daemon方式运行，默认不是后台运行pidfile /var/run/redis/redis.pid #41行 #redis的PID文件路径（可选）bind 0.0.0.0 #64行 #绑定主机IP，默认值为127.0.0.1，我们是跨机器运行，所以需要更改logfile /var/log/redis/redis.log #104行i #定义log文件位置，模式log信息定向到stdout，输出到/dev/null（可选）dir “/usr/local/rdbfile” #188行 #本地数据库存放路径，默认为./，编译安装默认存在在/usr/local/bin下（可选） 2.2.2 Redis的基本使用启动redis服务器 1redis-server /etc/redis.conf 连接redis服务器 1redis-cli 停止redis服务器 1redis-cli shutdown 2.3 安装Spark（单节点） 上传安装包 ./cluster 配置slaves 添加主机名 hadoop101 配置spark的主机名称和端口号 spark-env.sh 12SPARK_MASTER_HOST=linux #添加spark master的主机名SPARK_MASTER_PORT=7077 #添加spark master的端口号 14. 启动 1sbin/start-all.sh 2.4 安装 Zookeeper 单节点2.5 安装Flume-ng 单节点 上传安装包 1wget http://www.apache.org/dyn/closer.lua/flume/1.8.0/apache-flume-1.8.0-bin.tar.gz ​ 2. 解压 即可 2.6 安装kafka 单节点1 解压 2 配置 server.properties 3 kafka节点的使用 3 项目的搭建3.1 新建maven项目com.xxx 最外层pom.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889 &lt;properties&gt; &lt;log4j.version&gt;1.2.17&lt;/log4j.version&gt; &lt;slf4j.version&gt;1.7.22&lt;/slf4j.version&gt; &lt;mongodb-spark.version&gt;2.0.0&lt;/mongodb-spark.version&gt; &lt;casbah.version&gt;3.1.1&lt;/casbah.version&gt; &lt;redis.version&gt;2.9.0&lt;/redis.version&gt; &lt;kafka.version&gt;0.10.2.1&lt;/kafka.version&gt; &lt;spark.version&gt;2.1.1&lt;/spark.version&gt; &lt;scala.version&gt;2.11.8&lt;/scala.version&gt; &lt;jblas.version&gt;1.2.1&lt;/jblas.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;jcl-over-slf4j&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;!--maven项目 各种各样的声明周期编译的插件--&gt; &lt;build&gt; &lt;!--声明并引入子项目共有的插件--&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.6.1&lt;/version&gt; &lt;!--所有的编译用JDK1.8--&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;!--maven的打包插件--&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!--该插件用于将scala代码编译成class文件--&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt; &lt;executions&gt; &lt;!--绑定到maven的编译阶段--&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;/build&gt; 2.2reoommender 的pom.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- 引入Spark相关的Jar包 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-mllib_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-graphx_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;build&gt; &lt;plugins&gt; &lt;!-- 父项目已声明该plugin，子项目在引入的时候，不用声明版本和已经声明的配置 --&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 2.3DataLoader 的搭建pom.xml 123456789101112131415161718192021222324252627&lt;dependencies&gt; &lt;!-- Spark的依赖引入 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 引入Scala --&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 加入MongoDB的驱动 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;casbah-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;casbah.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt; &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;mongodb-spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; log4j.properties的配置 1234log4j.rootLogger=info, stdoutlog4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; %5p --- [%50t] %-80c(line:%5L) : %m%n 导入数据集 新建类 样例类 搭建基本的程序框架 隐式类的加入 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154package com.atguigu.recommenderimport com.mongodb.casbah.commons.MongoDBObjectimport com.mongodb.casbah.&#123;MongoClient, MongoClientURI&#125;import org.apache.spark.SparkConfimport org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;/* * @创建人: MaLingZhao * @创建时间: 2019/9/29 * @描述： 推荐系统的加载服务 *//*Producct 数据集21643^ 商品ID 0巴黎欧莱雅男士劲能醒肤露 8重功效50ml^ 商品名称 1916,502,352 商品的分类ID 不需要 2B0010MBKHO^ 亚马逊ID 不需要 3Y300_QL70_.jpg^ 商品的图URL 为了和业务系统相关联小说|文学艺术|图书音像 商品分类 5男士乳液/面霜|男士护肤| 商品的UGC标签 6*/case class Product(productId :Int,name:String, imageUrl:String,categories:String,tags:String)/*Rating数据集407423, 用户ID457976, 商品ID5.0, 评分数据1379001600 时间戳 *///Int 类型case class Rating(userId:Int, productId:Int,score:Double,timestamp:Int)/*uri mongoDb的ueldb 要操作的db *///mongo的配置封装样例类case class MongoConfig(uri:String,db:String)object DataLoader &#123; //定义数据文件路径 val PRODUCT_DATA_PATH="D:\\xiangmu\\ECommerceRecommendSystem\\recommender\\DataLoader\\src\\main\\resources\\products.csv" val RATING_DATA_PATH="D:\\xiangmu\\ECommerceRecommendSystem\\recommender\\DataLoader\\src\\main\\resources\\ratings.csv" //定义mongoDB中存储的表名 val MONGODB_PRODUCT_COLLECTION="Product" val MONGODB_RATING_COLLECTION="Rating" def main(args: Array[String]): Unit = &#123; val config=Map( "spark.cores" -&gt; "local[*]", "mongo.uri" -&gt; "mongodb://linux:27017/recommender", "mongo.db" -&gt;"recommender" ) //创建一个spark Config val sparkConf=new SparkConf().setMaster(config("spark.cores")).setAppName("DataLoader"); //创建sparkSession val spark=SparkSession.builder().config(sparkConf).getOrCreate() //隐式转换的包 import spark.implicits._ //加载数据 val productRDD= spark.sparkContext.textFile(PRODUCT_DATA_PATH) //转换成表结构 val productDF=productRDD.map(item =&gt;&#123; //第一个反斜杠 转义后面的反斜杠 数据通过^分割 val attr=item.split("\\^") //转换成product类 最后一行代表代码的返回 Product(attr(0).toInt,attr(1).trim,attr(4).trim,attr(5).trim,attr(6).trim) &#125;).toDF() val ratingRDD=spark.sparkContext.textFile(RATING_DATA_PATH) val ratingDF=ratingRDD.map(item=&gt;&#123; //逗号不用转义 val attr=item.split("," ) Rating(attr(0).toInt,attr(1).toInt,attr(2).toDouble,attr(3).toInt) &#125;).toDF() //调用多次的活 不用每次传入参数 implicit val mongoConfig=MongoConfig(config("mongo.uri"),config("mongo.db")) storeDataInMongoDB(productDF,ratingDF) //spark.stop()、 &#125; //隐式参数的使用 //以后调用的参数 def storeDataInMongoDB(productDF:DataFrame,raingDF:DataFrame)(implicit mongoConfig: MongoConfig): Unit = &#123; //新建和一个mongoDb的连接 客户端 val mongoClient = MongoClient(MongoClientURI(mongoConfig.uri)) //定义要操作的mongoDb中的表 val productCollection = mongoClient(mongoConfig.db)(MONGODB_PRODUCT_COLLECTION) val ratingCollection = mongoClient(mongoConfig.db)(MONGODB_RATING_COLLECTION) //如果表存在的话 删除 productCollection.dropCollection() ratingCollection.dropCollection() //将当前的数据存入对应的表中 productDF.write .option("uri", mongoConfig.uri) .option("collection", MONGODB_PRODUCT_COLLECTION) .mode("overwrite") .format("com.mongodb.spark.sql") .save() raingDF.write .option("uri", mongoConfig.uri) .option("collection", MONGODB_RATING_COLLECTION) .mode("overwrite") .format("com.mongodb.spark.sql") .save() //创建索引 //字段 productCollection.createIndex(MongoDBObject("productId" -&gt; 1)) ratingCollection.createIndex(MongoDBObject("productId" -&gt; 1)) ratingCollection.createIndex(MongoDBObject("userId" -&gt; 1)) mongoClient.close() &#125; &#125; 运行程序 测试代码吗 启动本地的mongo的测试 4 离线推荐服务 代码4.1 创建maven项目任务调度工具 离线统计服务 pom.xml 123456789101112131415161718192021222324252627&lt;dependencies&gt; &lt;!-- Spark的依赖引入 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 引入Scala --&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 加入MongoDB的驱动 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;casbah-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;casbah.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt; &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;mongodb-spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 统计学习模块 没有MLib log4j 复制过来 建立类 4.2 创建离线统计 服务 StatisticsRecommender代码部分 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142package com.atguigu.statisticsimport java.text.SimpleDateFormatimport java.util.Dateimport org.apache.spark.SparkConfimport org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;/* * @创建人: MaLingZhao * @创建时间: 2019/9/30 * @描述： *//*商品的信息不是特别的重要只需要评分的数据集 */case class Rating(userId:Int, productId:Int,score:Double,timestamp:Int)/*uri mongoDb的ueldb 要操作的db *///mongo的配置封装样例类case class MongoConfig(uri:String,db:String)object StatisticsRecommender &#123; val MONGODB_RATING_COLLECTION="Rating" //统计的表的名称 val RATE_MORE_PRODUCTS = "RateMoreProducts" val RATE_MORE_RECENTLY_PRODUCTS = "RateMoreRecentlyProducts" val AVERAGE_PRODUCTS = "AverageProducts" def main(args: Array[String]): Unit = &#123; val config = Map( "spark.cores" -&gt; "local[*]", "mongo.uri" -&gt; "mongodb://linux:27017/recommender", "mongo.db" -&gt; "recommender" ) //创建一个spark Config val sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("StatisticsRecommender"); //创建sparkSession val spark = SparkSession.builder().config(sparkConf).getOrCreate() //隐式转换的包 import spark.implicits._ //调用自己的包里面的 //调用多次的活 不用每次传入参数 implicit val mongoConfig=MongoConfig(config("mongo.uri"),config("mongo.db")) //加载数据 val ratingDF = spark.read .option("uri",mongoConfig.uri) .option("collection",MONGODB_RATING_COLLECTION) .format("com.mongodb.spark.sql") .load() //转换为定义的数据结构 .as[Rating] .toDF() //创建一张ratings的临时表 ratingDF.createOrReplaceTempView("ratings") //TODO 用spark sql 去做不同的统计推荐 //1. 历史热门商品 ，按照评分个数统计 //id 聚合 降序排列 productId count val rateMoreProductsDF =spark.sql("select productId,count(productId) as count from ratings group by productId order by count desc") //一一对应 storeDFInMongoDB(rateMoreProductsDF,RATE_MORE_PRODUCTS) //2. 近期热门商品，把时间戳转换成yyyyMM 年月格式进行评分个数统计 最终得到的是 productId count yearmonth // 创建一个日期的格式化工具 val simpleDateFormat=new SimpleDateFormat("yyyyMM") //注册UDF，将timestamp转化为年月格式yyyyMM //x为毫秒 x的格式为长整型 spark.udf.register("changeDate",(x:Int)=&gt;simpleDateFormat.format(new Date(x*1000L)).toInt) //把ratings 数据转换成想要的格式 productId，score yearmonth val ratingOfYearMonthDF=spark.sql("select productId,score,changeDate(timestamp) as yearmonth from ratings") ratingOfYearMonthDF.createOrReplaceTempView("ratingOfMonth") val rateMoreRecentlyProductDF =spark.sql("select productId, count(productId) as count ,yearmonth from ratingOfMonth group by yearmonth,productId order by yearmonth desc,count desc ") //把DF保存到mongoDB storeDFInMongoDB(rateMoreProductsDF,RATE_MORE_RECENTLY_PRODUCTS) //3. 优质商品统计 ，商品的平均得分 val averageProductsDF=spark.sql("select productId,avg(score) as avg from ratings group by productId order by avg desc ") storeDFInMongoDB(averageProductsDF,AVERAGE_PRODUCTS) spark.stop() &#125; def storeDFInMongoDB(df: DataFrame, collection_name: String)(implicit mongoConfig: MongoConfig): Unit = &#123; df.write .option("uri",mongoConfig.uri) .option("collection",collection_name) .mode("overwrite") .format("com.mongodb.spark.sql") .save()&#125;&#125;测试代码 4.3 基于隐语义模型的推荐评分矩阵 分解成两矩阵 用户商品推荐列表 ALS训练出来的Model 计算当前用户推荐列表 userId 和productId 产生(userId,productId)的分组 预测评分 评分排序 返回分支最大的K个商品 作为当前用户的推荐列表 UserRevcs 4.3.2 商品相似度矩阵 存储结构 4.3.4 创建模块 pom.xml 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scalanlp&lt;/groupId&gt; &lt;artifactId&gt;jblas&lt;/artifactId&gt; &lt;version&gt;$&#123;jblas.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Spark的依赖引入 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-mllib_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 引入Scala --&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 加入MongoDB的驱动 --&gt; &lt;!-- 用于代码方式连接MongoDB --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;casbah-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;casbah.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 用于Spark和MongoDB的对接 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt; &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;mongodb-spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; log4j配置文件的复制 4.3.5ALS算法代码部分 包装样例类 12345678910//定义标准推荐对象case class Recommendation(productId:Int,scre:Double) //定义用户的推荐列表case class UserRecs(userId:Int,recs:Seq[Recommendation]) //定义商品相似度列表case class ProductRecs(productId:Int,recs:Seq[Recommendation]) scala代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165case class ProductRating(userId:Int, productId:Int,score:Double,timestamp:Int)//定义标准推荐对象case class Recommendation(productId:Int,scre:Double)//定义用户的推荐列表case class UserRecs(userId:Int,recs:Seq[Recommendation])//定义商品相似度列表case class ProductRecs(productId:Int,recs:Seq[Recommendation])/*uri mongoDb的ueldb 要操作的db *///mongo的配置封装样例类case class MongoConfig(uri:String,db:String)object OfflineRecommender &#123; //定义mongodb中存储的表名 val MONGODB_RATING_COLLECTION = "Rating" //用户的推荐列表 val USER_RECS = "UserRecs" //商品相似度表 val PRODUCT_RECS = "ProductRecs" //最大的返回数量 val USER_MAX_RECOMMENDATION = 20 def main(args: Array[String]): Unit = &#123; val config = Map( "spark.cores" -&gt; "local[*]", "mongo.uri" -&gt; "mongodb://linux:27017/recommender", "mongo.db" -&gt; "recommender" ) //创建一个spark Config val sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("StatisticsRecommender"); //创建sparkSession val spark = SparkSession.builder().config(sparkConf).getOrCreate() //隐式转换的包 import spark.implicits._ //调用自己的包里面的 //调用多次的活 不用每次传入参数 implicit val mongoConfig = MongoConfig(config("mongo.uri"), config("mongo.db")) //加载数据 RDD ALS训练的时候使用 val ratingRDD = spark.read .option("uri", mongoConfig.uri) .option("collection", MONGODB_RATING_COLLECTION) .format("com.mongodb.spark.sql") .load() //转换为定义的数据结构 .as[ProductRating] .rdd .map( rating =&gt; (rating.userId, rating.productId, rating.score) ) .cache() //避免RDD的重复计算 // 提取出用户和商品的所有数据集 val userRDD = ratingRDD.map(_._1).distinct() val productRDD = ratingRDD.map(_._2).distinct() //TODO 核心计算过程 //1 训练隐语义模型 val trainData = ratingRDD.map(x =&gt; Rating(x._1, x._2, x._3)) //定义模型训练参数 rank 隐语义的隐特征的个数 iterations 迭代次数 lambda 正则化项系数 val (rank, iterations, lambda) = (5, 10, 0.01) val model = ALS.train(trainData, rank, iterations, lambda) //2 获得预测评分矩阵，得到用户的推荐列表 //userRDD 和productRDD 做笛卡尔积 得到空的userProductRDD表示的评分矩阵 val userProducts = userRDD.cartesian(productRDD) val preRating = model.predict(userProducts) //得到从预测评分矩阵提得到用户推荐列表 val userRecs = preRating.filter(_.rating &gt; 0) //每个用户id 物品对应的id .map( rating =&gt; (rating.user, (rating.product, rating.rating))) .groupByKey() .map &#123; case (userId, recs) =&gt; //降序排列 sort UserRecs(userId, recs.toList.sortWith(_._2 &gt; _._2).take(USER_MAX_RECOMMENDATION) .map(x =&gt; Recommendation(x._1, x._2))) &#125; .toDF() //写回到mongoDB 数据库中 userRecs.write .option("uri", mongoConfig.uri) .option("collection", USER_RECS) .mode("overwrite") .format("com.mongodb.spark.sql") .save() //3 利用商品的特征向量，计算商品的相似度列表 // 3. 利用商品的特征向量，计算商品的相似度列表 val productFeatures = model.productFeatures.map&#123; case (productId, features) =&gt; ( productId, new DoubleMatrix(features) ) &#125; // 两两配对商品，计算余弦相似度 val productRecs = productFeatures.cartesian(productFeatures) .filter&#123; case (a, b) =&gt; a._1 != b._1 &#125; // 计算余弦相似度 .map&#123; case (a, b) =&gt; val simScore = consinSim( a._2, b._2 ) ( a._1, ( b._1, simScore ) ) &#125; .filter(_._2._2 &gt; 0.4) .groupByKey() .map&#123; case (productId, recs) =&gt; ProductRecs( productId, recs.toList.sortWith(_._2&gt;_._2).map(x=&gt;Recommendation(x._1,x._2)) ) &#125; .toDF() productRecs.write .option("uri", mongoConfig.uri) .option("collection", PRODUCT_RECS) .mode("overwrite") .format("com.mongodb.spark.sql") .save() spark.stop() &#125; def consinSim(product1: DoubleMatrix, product2: DoubleMatrix): Double = &#123; product1.dot(product2)/(product1.norm2()) * product2.norm2()&#125;&#125; 4.3.3 模型参数评估选取最优的参数 RMSE 求得误差 考察 12 ALSTrainer 代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071 object ALSTrainer&#123;def main(args: Array[String]): Unit = &#123; val config = Map( "spark.cores" -&gt; "local[*]", "mongo.uri" -&gt; "mongodb://localhost:27017/recommender", "mongo.db" -&gt; "recommender" ) // 创建一个spark config val sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("OfflineRecommender") // 创建spark session val spark = SparkSession.builder().config(sparkConf).getOrCreate() import spark.implicits._ implicit val mongoConfig = MongoConfig( config("mongo.uri"), config("mongo.db") ) // 加载数据 val ratingRDD = spark.read .option("uri", mongoConfig.uri) .option("collection", MONGODB_RATING_COLLECTION) .format("com.mongodb.spark.sql") .load() .as[ProductRating] .rdd .map( rating =&gt; Rating(rating.userId, rating.productId, rating.score) ).cache() // 数据集切分成训练集和测试集 val splits = ratingRDD.randomSplit(Array(0.8, 0.2)) val trainingRDD = splits(0) val testingRDD = splits(1) // 核心实现：输出最优参数 adjustALSParams( trainingRDD, testingRDD ) spark.stop()&#125; def adjustALSParams(trainData: RDD[Rating], testData: RDD[Rating]): Unit =&#123; // 遍历数组中定义的参数取值 val result = for( rank &lt;- Array(5, 10, 20, 50); lambda &lt;- Array(1, 0.1, 0.01) ) yield &#123; val model = ALS.train(trainData, rank, 10, lambda) val rmse = getRMSE( model, testData ) ( rank, lambda, rmse )&#125; // 按照rmse排序并输出最优参数 println(result.minBy(_._3))&#125; def getRMSE(model: MatrixFactorizationModel, data: RDD[Rating]): Double = &#123; //构建UserProducts， 得到预测的评分矩阵 val userProducts: RDD[(Int, Int)] = data.map(item=&gt;(item.user,item.product)) val predictRating: RDD[Rating] = model.predict(userProducts) //按照公式计算 RMSE，首先把预测评分和实际评分表做一个连接 以（userID和productID）做一个连接 val observed : RDD[((Int, Int), Double)] = data.map(item=&gt;((item.user,item.product),item.rating)) val predict: RDD[((Int, Int), Double)] = predictRating.map(item=&gt;((item.user,item.product),item.rating)) sqrt(observed.join(predict).map&#123; case((userId,productId),(actual,pre)) =&gt; val error = actual -pre error*error &#125;.mean()) &#125;&#125; 5 实时推荐模块5.1 分析用户最近的偏好 之前买过什么商品 基于这样的思想 构建模型 不需要那么精确 基于模型的实时架构 mongo redis 日志 评分数据 userId productId 时间戳 flume kafka Stream 过滤 recommender spark streaming 定义实时推荐算法 用户最近的评分 redis 推荐优先级的计算 基本原理 用户最近一段时间的口味是相似的 最近的k次评分 好评 差评 5.1.2 推荐优先级计算 5.1.3 实现创建项目 pom.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&lt;dependencies&gt; &lt;!-- Spark的依赖引入 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 引入Scala --&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 加入MongoDB的驱动 --&gt; &lt;!-- 用于代码方式连接MongoDB --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;casbah-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;casbah.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 用于Spark和MongoDB的对接 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt; &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;mongodb-spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- redis --&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- kafka --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.10.2.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 实时系统的搭建 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213package com.atguigu.onlineimport com.mongodb.casbah.commons.MongoDBObjectimport com.mongodb.casbah.&#123;MongoClient, MongoClientURI&#125;import org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.SparkConfimport org.apache.spark.sql.SparkSessionimport org.apache.spark.streaming.kafka010.&#123;ConsumerStrategies, KafkaUtils, LocationStrategies&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import redis.clients.jedis.Jedis// 定义一个连接助手对象，建立到redis和mongodb的连接object ConnHelper extends Serializable&#123; // 懒变量定义，使用的时候才初始化 lazy val jedis = new Jedis("localhost") lazy val mongoClient = MongoClient(MongoClientURI("mongodb://linux:27017/recommender"))&#125;case class MongoConfig( uri: String, db: String )// 定义标准推荐对象case class Recommendation( productId: Int, score: Double )// 定义用户的推荐列表case class UserRecs( userId: Int, recs: Seq[Recommendation] )// 定义商品相似度列表case class ProductRecs( productId: Int, recs: Seq[Recommendation] )object OnlineRecommender &#123; // 定义常量和表名 val MONGODB_RATING_COLLECTION = "Rating" val STREAM_RECS = "StreamRecs" val PRODUCT_RECS = "ProductRecs" val MAX_USER_RATING_NUM = 20 val MAX_SIM_PRODUCTS_NUM = 20 def main(args: Array[String]): Unit = &#123; val config = Map( "spark.cores" -&gt; "local[*]", "mongo.uri" -&gt; "mongodb://localhost:27017/recommender", "mongo.db" -&gt; "recommender", "kafka.topic" -&gt; "recommender" ) // 创建spark conf val sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("OnlineRecommender") val spark = SparkSession.builder().config(sparkConf).getOrCreate() val sc = spark.sparkContext val ssc = new StreamingContext(sc, Seconds(2)) import spark.implicits._ implicit val mongoConfig = MongoConfig( config("mongo.uri"), config("mongo.db") ) // 加载数据，相似度矩阵，广播出去 val simProductsMatrix = spark.read .option("uri", mongoConfig.uri) .option("collection", PRODUCT_RECS) .format("com.mongodb.spark.sql") .load() .as[ProductRecs] .rdd // 为了后续查询相似度方便，把数据转换成map形式 .map&#123;item =&gt; ( item.productId, item.recs.map( x=&gt;(x.productId, x.score) ).toMap ) &#125; .collectAsMap() // 定义广播变量 val simProcutsMatrixBC = sc.broadcast(simProductsMatrix) // 创建kafka配置参数 val kafkaParam = Map( "bootstrap.servers" -&gt; "linux:9092", "key.deserializer" -&gt; classOf[StringDeserializer], "value.deserializer" -&gt; classOf[StringDeserializer], "group.id" -&gt; "recommender", "auto.offset.reset" -&gt; "latest" ) // 创建一个DStream val kafkaStream = KafkaUtils.createDirectStream[String, String]( ssc, LocationStrategies.PreferConsistent, ConsumerStrategies.Subscribe[String, String]( Array(config("kafka.topic")), kafkaParam ) ) // 对kafkaStream进行处理，产生评分流，userId|productId|score|timestamp val ratingStream = kafkaStream.map&#123;msg=&gt; var attr = msg.value().split("\\|") ( attr(0).toInt, attr(1).toInt, attr(2).toDouble, attr(3).toInt ) &#125; // 核心算法部分，定义评分流的处理流程 ratingStream.foreachRDD&#123; rdds =&gt; rdds.foreach&#123; case ( userId, productId, score, timestamp ) =&gt; println("rating data coming!&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;") // TODO: 核心算法流程 // 1. 从redis里取出当前用户的最近评分，保存成一个数组Array[(productId, score)] val userRecentlyRatings = getUserRecentlyRatings( MAX_USER_RATING_NUM, userId, ConnHelper.jedis ) // 2. 从相似度矩阵中获取当前商品最相似的商品列表，作为备选列表，保存成一个数组Array[productId] val candidateProducts = getTopSimProducts( MAX_SIM_PRODUCTS_NUM, productId, userId, simProcutsMatrixBC.value ) // 3. 计算每个备选商品的推荐优先级，得到当前用户的实时推荐列表，保存成 Array[(productId, score)] val streamRecs = computeProductScore( candidateProducts, userRecentlyRatings, simProcutsMatrixBC.value ) // 4. 把推荐列表保存到mongodb saveDataToMongoDB( userId, streamRecs ) &#125; &#125; // 启动streaming ssc.start() println("streaming started!") ssc.awaitTermination() &#125; /** * 从redis里获取最近num次评分 */ import scala.collection.JavaConversions._ def getUserRecentlyRatings(num: Int, userId: Int, jedis: Jedis): Array[(Int, Double)] = &#123; // 从redis中用户的评分队列里获取评分数据，list键名为uid:USERID，值格式是 PRODUCTID:SCORE jedis.lrange( "userId:" + userId.toString, 0, num ) .map&#123; item =&gt; val attr = item.split("\\:") ( attr(0).trim.toInt, attr(1).trim.toDouble ) &#125; .toArray &#125; // 获取当前商品的相似列表，并过滤掉用户已经评分过的，作为备选列表 def getTopSimProducts(num: Int, productId: Int, userId: Int, simProducts: scala.collection.Map[Int, scala.collection.immutable.Map[Int, Double]]) (implicit mongoConfig: MongoConfig): Array[Int] =&#123; // 从广播变量相似度矩阵中拿到当前商品的相似度列表 val allSimProducts = simProducts(productId).toArray // 获得用户已经评分过的商品，过滤掉，排序输出 val ratingCollection = ConnHelper.mongoClient( mongoConfig.db )( MONGODB_RATING_COLLECTION ) val ratingExist = ratingCollection.find( MongoDBObject("userId"-&gt;userId) ) .toArray .map&#123;item=&gt; // 只需要productId item.get("productId").toString.toInt &#125; // 从所有的相似商品中进行过滤 allSimProducts.filter( x =&gt; ! ratingExist.contains(x._1) ) .sortWith(_._2 &gt; _._2) .take(num) .map(x=&gt;x._1) &#125; // 计算每个备选商品的推荐得分 def computeProductScore(candidateProducts: Array[Int], userRecentlyRatings: Array[(Int, Double)], simProducts: scala.collection.Map[Int, scala.collection.immutable.Map[Int, Double]]) : Array[(Int, Double)] =&#123; // 定义一个长度可变数组ArrayBuffer，用于保存每一个备选商品的基础得分，(productId, score) val scores = scala.collection.mutable.ArrayBuffer[(Int, Double)]() // 定义两个map，用于保存每个商品的高分和低分的计数器，productId -&gt; count val increMap = scala.collection.mutable.HashMap[Int, Int]() val decreMap = scala.collection.mutable.HashMap[Int, Int]() // 遍历每个备选商品，计算和已评分商品的相似度 for( candidateProduct &lt;- candidateProducts; userRecentlyRating &lt;- userRecentlyRatings )&#123; // 从相似度矩阵中获取当前备选商品和当前已评分商品间的相似度 val simScore = getProductsSimScore( candidateProduct, userRecentlyRating._1, simProducts ) if( simScore &gt; 0.4 )&#123; // 按照公式进行加权计算，得到基础评分 scores += ( (candidateProduct, simScore * userRecentlyRating._2) ) if( userRecentlyRating._2 &gt; 3 )&#123; increMap(candidateProduct) = increMap.getOrDefault(candidateProduct, 0) + 1 &#125; else &#123; decreMap(candidateProduct) = decreMap.getOrDefault(candidateProduct, 0) + 1 &#125; &#125; &#125; // 根据公式计算所有的推荐优先级，首先以productId做groupby scores.groupBy(_._1).map&#123; case (productId, scoreList) =&gt; ( productId, scoreList.map(_._2).sum/scoreList.length + log(increMap.getOrDefault(productId, 1)) - log(decreMap.getOrDefault(productId, 1)) ) &#125; // 返回推荐列表，按照得分排序 .toArray .sortWith(_._2&gt;_._2) &#125; def getProductsSimScore(product1: Int, product2: Int, simProducts: scala.collection.Map[Int, scala.collection.immutable.Map[Int, Double]]): Double =&#123; simProducts.get(product1) match &#123; case Some(sims) =&gt; sims.get(product2) match &#123; case Some(score) =&gt; score case None =&gt; 0.0 &#125; case None =&gt; 0.0 &#125; &#125; // 自定义log函数，以N为底 def log(m: Int): Double = &#123; val N = 10 math.log(m)/math.log(N) &#125; // 写入mongodb def saveDataToMongoDB(userId: Int, streamRecs: Array[(Int, Double)])(implicit mongoConfig: MongoConfig): Unit =&#123; val streamRecsCollection = ConnHelper.mongoClient(mongoConfig.db)(STREAM_RECS) // 按照userId查询并更新 streamRecsCollection.findAndRemove( MongoDBObject( "userId" -&gt; userId ) ) streamRecsCollection.insert( MongoDBObject( "userId" -&gt; userId, "recs" -&gt; streamRecs.map(x=&gt;MongoDBObject("productId"-&gt;x._1, "score"-&gt;x._2)) ) ) &#125;&#125; 6 实时系统的联调KafkaStreaming模块的搭建 pom.xml 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-streams&lt;/artifactId&gt; &lt;version&gt;0.10.2.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.10.2.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;kafkastream&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.atguigu.kafkastream.Application&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 代码的实现 Application 12345678910111213141516171819202122232425262728293031323334353637383940package com.atguigu.kafkastream; import org.apache.kafka.streams.KafkaStreams;import org.apache.kafka.streams.StreamsConfig;import org.apache.kafka.streams.processor.TopologyBuilder;import java.util.Properties;public class Application &#123; public static void main(String[] args) &#123; String brokers = "linux:9092"; String zookeepers = "linux:2181"; // 定义输入和输出的topic String from = "log"; String to = "recommender"; // 定义kafka stream 配置参数 Properties settings = new Properties(); settings.put(StreamsConfig.APPLICATION_ID_CONFIG, "logFilter"); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, brokers); settings.put(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeepers); // 创建kafka stream 配置对象 StreamsConfig config = new StreamsConfig(settings); // 定义拓扑构建器 TopologyBuilder builder = new TopologyBuilder(); builder.addSource("SOURCE", from) .addProcessor("PROCESSOR", ()-&gt;new LogProcessor(), "SOURCE") .addSink("SINK", to, "PROCESSOR"); // 创建kafka stream KafkaStreams streams = new KafkaStreams( builder, config ); streams.start(); System.out.println("kafka stream started!"); &#125;&#125; LogProcessor 12345678910111213141516171819202122232425262728293031323334353637383940package com.atguigu.kafkastream;import org.apache.kafka.streams.processor.Processor;import org.apache.kafka.streams.processor.ProcessorContext;/** * @ClassName: LogProcessor * @Description: * @Author: wushengran on 2019/4/28 15:08 * @Version: 1.0 */public class LogProcessor implements Processor&lt;byte[], byte[]&gt;&#123; private ProcessorContext context; @Override public void init(ProcessorContext processorContext) &#123; this.context = processorContext; &#125; @Override public void process(byte[] dummy, byte[] line) &#123; // 核心处理流程 String input = new String(line); // 提取数据，以固定前缀过滤日志信息 if( input.contains("PRODUCT_RATING_PREFIX:") )&#123; System.out.println("product rating data coming! " + input); input = input.split("PRODUCT_RATING_PREFIX:")[1].trim(); context.forward("logProcessor".getBytes(), input.getBytes()); &#125; &#125; @Override public void punctuate(long l) &#123; &#125; @Override public void close() &#123; &#125;&#125; 6.2 配置启动Flumeflume的conf 目录下 123456789101112131415161718192021222324agent.sources = exectailagent.channels = memoryChannelagent.sinks = kafkasink# For each one of the sources, the type is definedagent.sources.exectail.type = exec# 下面这个路径是需要收集日志的绝对路径，改为自己的日志目录agent.sources.exectail.command = tail –f/mnt/d/Projects/BigData/ECommerceRecommenderSystem/businessServer/src/main/log/agent.logagent.sources.exectail.interceptors=i1agent.sources.exectail.interceptors.i1.type=regex_filter# 定义日志过滤前缀的正则agent.sources.exectail.interceptors.i1.regex=.+PRODUCT_RATING_PREFIX.+# The channel can be defined as follows.agent.sources.exectail.channels = memoryChannel# Each sink's type must be definedagent.sinks.kafkasink.type = org.apache.flume.sink.kafka.KafkaSinkagent.sinks.kafkasink.kafka.topic = logagent.sinks.kafkasink.kafka.bootstrap.servers = localhost:9092agent.sinks.kafkasink.kafka.producer.acks = 1agent.sinks.kafkasink.kafka.flumeBatchSize = 20#Specify the channel the sink should use 12 启动flume 1./bin/flume-ng agent -c ./conf/ -f ./conf/log-kafka.properties -n agent -Dflume.root.logger=INFO,console 启动zookeeper 1234bin/zkServer.sh start启动kafkabin/kafka-server-start.sh -daemon ./config/server.properties 测试实时推荐模块 连接的东西 先启动zookeeper 和kafka 在启动redis 然后redis的客户端 6 冷启动问题的解决实际项目中遇到的问题 冷启动的问题 算法是基于隐语义模型的 冷启动问题的处理 进来注册的时候 让你勾选项 了解即可 7 其他形式的离线相似推荐买了这个商品的他、用户 跟商品的相似 对应的相似的产品作出推荐 推荐算法分类的时候 用户画像 基于商品内容推荐 7.1 基于内容的相似度推荐id 名称 图片url 分类 ugc标签 主要基于UGC标签 创建项目 pom文件 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scalanlp&lt;/groupId&gt; &lt;artifactId&gt;jblas&lt;/artifactId&gt; &lt;version&gt;$&#123;jblas.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Spark的依赖引入 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-mllib_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 引入Scala --&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 加入MongoDB的驱动 --&gt; &lt;!-- 用于代码方式连接MongoDB --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;casbah-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;casbah.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 用于Spark和MongoDB的对接 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt; &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;mongodb-spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 实现代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108package com.atguigu.contentimport org.apache.spark.SparkConfimport org.apache.spark.ml.feature.&#123;HashingTF, IDF, Tokenizer&#125;import org.apache.spark.ml.linalg.SparseVectorimport org.apache.spark.sql.SparkSessionimport org.jblas.DoubleMatrixcase class Product( productId: Int, name: String, imageUrl: String, categories: String, tags: String )case class MongoConfig( uri: String, db: String )// 定义标准推荐对象case class Recommendation( productId: Int, score: Double )// 定义商品相似度列表case class ProductRecs( productId: Int, recs: Seq[Recommendation] )object ContentRecommender &#123; // 定义mongodb中存储的表名 val MONGODB_PRODUCT_COLLECTION = "Product" val CONTENT_PRODUCT_RECS = "ContentBasedProductRecs" def main(args: Array[String]): Unit = &#123; val config = Map( "spark.cores" -&gt; "local[*]", "mongo.uri" -&gt; "mongodb://linux:27017/recommender", "mongo.db" -&gt; "recommender" ) // 创建一个spark config val sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("ContentRecommender") // 创建spark session val spark = SparkSession.builder().config(sparkConf).getOrCreate() import spark.implicits._ implicit val mongoConfig = MongoConfig( config("mongo.uri"), config("mongo.db") ) // 载入数据，做预处理 val productTagsDF = spark.read .option("uri", mongoConfig.uri) .option("collection", MONGODB_PRODUCT_COLLECTION) .format("com.mongodb.spark.sql") .load() .as[Product] .map( x =&gt; ( x.productId, x.name, x.tags.map(c=&gt; if(c=='|') ' ' else c) ) ) .toDF("productId", "name", "tags") .cache() // TODO: 用TF-IDF提取商品特征向量 // 1. 实例化一个分词器，用来做分词，默认按照空格分 val tokenizer = new Tokenizer().setInputCol("tags").setOutputCol("words") // 用分词器做转换，得到增加一个新列words的DF val wordsDataDF = tokenizer.transform(productTagsDF) // 2. 定义一个HashingTF工具，计算频次 val hashingTF = new HashingTF().setInputCol("words").setOutputCol("rawFeatures").setNumFeatures(800) val featurizedDataDF = hashingTF.transform(wordsDataDF) // 3. 定义一个IDF工具，计算TF-IDF val idf = new IDF().setInputCol("rawFeatures").setOutputCol("features") // 训练一个idf模型 val idfModel = idf.fit(featurizedDataDF) // 得到增加新列features的DF val rescaledDataDF = idfModel.transform(featurizedDataDF) // 对数据进行转换，得到RDD形式的features val productFeatures = rescaledDataDF.map&#123; row =&gt; ( row.getAs[Int]("productId"), row.getAs[SparseVector]("features").toArray ) &#125; .rdd .map&#123; case (productId, features) =&gt; ( productId, new DoubleMatrix(features) ) &#125; // 两两配对商品，计算余弦相似度 val productRecs = productFeatures.cartesian(productFeatures) .filter&#123; case (a, b) =&gt; a._1 != b._1 &#125; // 计算余弦相似度 .map&#123; case (a, b) =&gt; val simScore = consinSim( a._2, b._2 ) ( a._1, ( b._1, simScore ) ) &#125; .filter(_._2._2 &gt; 0.4) .groupByKey() .map&#123; case (productId, recs) =&gt; ProductRecs( productId, recs.toList.sortWith(_._2&gt;_._2).map(x=&gt;Recommendation(x._1,x._2)) ) &#125; .toDF() productRecs.write .option("uri", mongoConfig.uri) .option("collection", CONTENT_PRODUCT_RECS) .mode("overwrite") .format("com.mongodb.spark.sql") .save() spark.stop() &#125; def consinSim(product1: DoubleMatrix, product2: DoubleMatrix): Double =&#123; product1.dot(product2)/ ( product1.norm2() * product2.norm2() ) &#125;&#125; 7.2 基于ItemCF的推荐算法 123456789101112131415161718192021222324252627282930&lt;dependencies&gt; &lt;!-- Spark的依赖引入 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 引入Scala --&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 加入MongoDB的驱动 --&gt; &lt;!-- 用于代码方式连接MongoDB --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;casbah-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;casbah.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 用于Spark和MongoDB的对接 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt; &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;mongodb-spark.version&#125;&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109package com.atguigu.itemcfimport org.apache.spark.SparkConfimport org.apache.spark.sql.SparkSessioncase class ProductRating( userId: Int, productId: Int, score: Double, timestamp: Int )case class MongoConfig( uri: String, db: String )// 定义标准推荐对象case class Recommendation( productId: Int, score: Double )// 定义商品相似度列表case class ProductRecs( productId: Int, recs: Seq[Recommendation] )object ItemCFRecommender &#123; // 定义常量和表名 val MONGODB_RATING_COLLECTION = "Rating" val ITEM_CF_PRODUCT_RECS = "ItemCFProductRecs" val MAX_RECOMMENDATION = 10 def main(args: Array[String]): Unit = &#123; val config = Map( "spark.cores" -&gt; "local[*]", "mongo.uri" -&gt; "mongodb://linux:27017/recommender", "mongo.db" -&gt; "recommender" ) // 创建一个spark config val sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("ItemCFRecommender") // 创建spark session val spark = SparkSession.builder().config(sparkConf).getOrCreate() import spark.implicits._ implicit val mongoConfig = MongoConfig( config("mongo.uri"), config("mongo.db") ) // 加载数据，转换成DF进行处理 val ratingDF = spark.read .option("uri", mongoConfig.uri) .option("collection", MONGODB_RATING_COLLECTION) .format("com.mongodb.spark.sql") .load() .as[ProductRating] .map( x =&gt; ( x.userId, x.productId, x.score ) ) .toDF("userId", "productId", "score") .cache() // TODO: 核心算法，计算同现相似度，得到商品的相似列表 // 统计每个商品的评分个数，按照productId来做group by val productRatingCountDF = ratingDF.groupBy("productId").count() // 在原有的评分表上rating添加count val ratingWithCountDF = ratingDF.join(productRatingCountDF, "productId") // 将评分按照用户id两两配对，统计两个商品被同一个用户评分过的次数 val joinedDF = ratingWithCountDF.join(ratingWithCountDF, "userId") .toDF("userId","product1","score1","count1","product2","score2","count2") .select("userId","product1","count1","product2","count2") // 创建一张临时表，用于写sql查询 joinedDF.createOrReplaceTempView("joined") // 按照product1,product2 做group by，统计userId的数量，就是对两个商品同时评分的人数 val cooccurrenceDF = spark.sql( """ |select product1 |, product2 |, count(userId) as cocount |, first(count1) as count1 |, first(count2) as count2 |from joined |group by product1, product2 """.stripMargin ).cache() // 提取需要的数据，包装成( productId1, (productId2, score) ) val simDF = cooccurrenceDF.map&#123; row =&gt; val coocSim = cooccurrenceSim( row.getAs[Long]("cocount"), row.getAs[Long]("count1"), row.getAs[Long]("count2") ) ( row.getInt(0), ( row.getInt(1), coocSim ) ) &#125; .rdd .groupByKey() .map&#123; case (productId, recs) =&gt; ProductRecs( productId, recs.toList .filter(x=&gt;x._1 != productId) .sortWith(_._2&gt;_._2) .take(MAX_RECOMMENDATION) .map(x=&gt;Recommendation(x._1,x._2)) ) &#125; .toDF() // 保存到mongodb simDF.write .option("uri", mongoConfig.uri) .option("collection", ITEM_CF_PRODUCT_RECS) .mode("overwrite") .format("com.mongodb.spark.sql") .save() spark.stop() &#125; // 按照公式计算同现相似度 def cooccurrenceSim(coCount: Long, count1: Long, count2: Long): Double =&#123; coCount / math.sqrt( count1 * count2 ) &#125;&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[java]]></title>
    <url>%2Fblog4%2F2019%2F10%2F08%2Fjava%2F</url>
    <content type="text"><![CDATA[1 各大厂的面试题1.1 蚂蚁花呗一个小时 1.2美团的一面 垃圾回收器 1.3 百度 java集合类 synchronized 什么是对象锁 什么是全局锁 1.4 头条 1.5 美团的面试汇总 1.6 蚂蚁金服二面 1.7 讲解1 关于2018.12 月份 ，。互联网公司大规模的缩招 裁员 缩招不是不招聘 而是招聘更多的优质的咖啡啊工程师 2 将最近半年的大厂面试题进行了整理和划分 1 1.8 3 1 个人 1.8倍的工资 干三个人的活 3 第一次 提出高频最多的常见笔试面试题目 ArrayList HashMap 底层是什么东西 4 JVM/GC 多线程与高并发 1.8 技术框架 大厂的面试题 90% 1.9 redis的相关题目 哪些数据存mysql 哪些 redis 如何保持移植性 redis缓存给了多大的总内存 命中率多高 超大Value打满网卡的问题 1.10 消息中间件MQrabibtMQ 消息中间件 消息积压了两个小时 消息中间件只有一个 挂掉 影响业务 1.11 JVM+GC的解析 oom 的东西 了解 Out of Memory Error java内存溢出 四大引用 强 软 弱 虚 水平 泯然众人与 性能检测工具 1.12 JUC多线程及高并发1.13 面试重点jvm + GC JUC多线程高并发 本次讲解 互联网笔试题第二季 JVM/GC的知识 JUC的前提只是 超级熟悉java8的与、新特性 （Stream+lambdaExpress+函数接口+方法引用） 2 JUC多线程及高并发 current 并发 高并发 秒杀 多个线程访问 统一个资源 并行 各种事情一路并行去做 节水 道调料 atomic 院原子性 AtomicInteger 原子引用 2.1 请你谈谈你对volatile的理解 同步 synchronized 轻量级 什么是轻 三大特性 volatile是轻量级的同步机制 不是文科 理解 jMM关于同步的 2.1.1 JMM值内存可见性 volatile 可见性1 理论线程 —-》》》 工作内存 —-》》 每个线程的私有内存区域 变量 —-》》 主内存 —》》 共享内存区域 线程可以访问 t1改成37 了 t2 t3不知道 1 t1 拷走25 2 t1改成37 3 把37 写回主内存 线程2 和线程3 不知道 只要有一个线程修改完自己的工作空间值之后写回主内存 及时通知其他的线程 这种机制 JMM之内存模型的第一个特性可见性 主内存的值只要被修改 其他的线程马上获得通知 改课 干活 —》》线程 自己的工作内存 私有数据 new 3 个线程 java内存模型规定所有的变量在主内训 主内存 拷贝到自己的工作内存空间 不同的线程无法访问对方的工作内存 线程间的通信（传值） 必须靠主内存来完成 拷贝共享变量的初始值 线程的工作内存里面 西城区和东城区的售票员 电话确认不可能 线程运算完 写回主内存 可见性 有了最新消息 第一个通知 论证 2 代码灭有volatile的操作 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/8 * @描述： */import java.util.concurrent.TimeUnit;class MyData&#123; int number=0; public void addTO60() &#123; this.number=60; &#125;&#125;/** 1 验证volatile的可见性* 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰** */public class VolatileDemo &#123; public static void main(String[] args) &#123; //main是一切方法的运行入口 MyData myData=new MyData();//资源类 new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+"\t come in"); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; myData.addTO60(); System.out.println(Thread.currentThread().getName()+ "\t updated number value:"+ myData.number); &#125;,"AAA").start(); //第二个线程就是我们的main线程 while (myData.number==0) &#123; //main线程一直在这里等待循环 直到这个number不在等于0 没有可见性 &#125; System.out.println(Thread.currentThread().getName()+"\t misson is over"); &#125;&#125; 最后一句没有打印 没有人通知 main线程傻傻的等待 没有人通知我改了 我不知道 加上volatile的代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/8 * @描述： */import java.util.concurrent.TimeUnit;class MyData&#123; volatile int number=0; public void addTO60() &#123; this.number=60; &#125;&#125;/** 1 验证volatile的可见性* 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰** */public class VolatileDemo &#123; public static void main(String[] args) &#123; //main是一切方法的运行入口 MyData myData=new MyData();//资源类 new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+"\t come in"); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; myData.addTO60(); System.out.println(Thread.currentThread().getName()+ "\t updated number value:"+ myData.number); &#125;,"AAA").start(); //第二个线程就是我们的main线程 while (myData.number==0) &#123; //main线程一直在这里等待循环 直到这个number不在等于0 没有可见性 &#125; System.out.println(Thread.currentThread().getName()+"\t misson is over"+"main get value:"+myData.number); &#125;&#125; 可见性证明 缓存 JMM的一种内存抽象机制 抽象的概念 再次阅读 轻量级 乞丐版的synchronized 2.1.2 JMM之原子性 volatile不保证原子性volatile是不保证原子性的 1 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/8 * @描述： */import java.util.concurrent.TimeUnit;class MyData&#123; volatile int number=0; public void addTO60() &#123; this.number=60; &#125; /* 此时number的前面是加了voatile关键字的修饰的，volatile不保证原子性 */public void addPlus()&#123; number++;&#125;&#125;/** 1 验证volatile的可见性* 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰** */public class VolatileDemo &#123;//main一切方法的入口 public static void main(String[] args) &#123; MyData myData=new MyData(); for (int i = 1; i &lt;=20 ; i++) &#123; new Thread(()-&gt;&#123; for (int j = 1; j &lt;=1000 ; j++) &#123; myData.addPlus(); &#125; &#125;,String.valueOf(i) ).start(); &#125; //等待上面的20个线程全部计算完成 再用main线程取得的最终的结果值是什么 //后台的线程 /*1 main 2 后台GC线程 * */ while(Thread.activeCount()&gt;2) &#123; Thread.yield(); &#125; System.out.println(Thread.currentThread().getName()+"\t finally n umber value: "+ myData.number); &#125; /*volatile可以保证可见性 可以通知其线程主物理内存的值已经被修改 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰 2 验证volatile不保证原子性 2.1 原子性指的是什么意思？ 不可分割 完整性 也即某个线程在做某个具体的业务的时候 中间不可以被加塞或者分割 需要整体完整 要么同时成功 要么同时失败 一段时间内只允许一个操作 2.2 volatile 是否可以保证原子性 */ public static void seeOkByVolatile() &#123; MyData myData=new MyData();//资源类 new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+"\t come in"); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; myData.addTO60(); System.out.println(Thread.currentThread().getName()+ "\t updated number value:"+ myData.number); &#125;,"AAA").start(); //第二个线程就是我们的main线程 while (myData.number==0) &#123; //main线程一直在这里等待循环 直到这个number不在等于0 没有可见性 &#125; System.out.println(Thread.currentThread().getName()+"\t misson is over"+"main get value:"+myData.number); &#125;&#125; 出来的不是2万 说明volatile不能够保证原子性 有没有可能加到2万 但是有极其特殊的情况是可能达到2万的 加了synchronized 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/8 * @描述： */import java.util.concurrent.TimeUnit;class MyData&#123; volatile int number=0; public void addTO60() &#123; this.number=60; &#125; /* 此时number的前面是加了voatile关键字的修饰的，volatile不保证原子性 */public synchronized void addPlus()&#123; number++;&#125;&#125;/** 1 验证volatile的可见性* 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰** */public class VolatileDemo &#123;//main一切方法的入口 public static void main(String[] args) &#123; MyData myData=new MyData(); for (int i = 1; i &lt;=20 ; i++) &#123; new Thread(()-&gt;&#123; for (int j = 1; j &lt;=1000 ; j++) &#123; myData.addPlus(); &#125; &#125;,String.valueOf(i) ).start(); &#125; //等待上面的20个线程全部计算完成 再用main线程取得的最终的结果值是什么 //后台的线程 /*1 main 2 后台GC线程 * */ while(Thread.activeCount()&gt;2) &#123; Thread.yield(); &#125; System.out.println(Thread.currentThread().getName()+"\t finally n umber value: "+ myData.number); &#125; /*volatile可以保证可见性 可以通知其线程主物理内存的值已经被修改 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰 2 验证volatile不保证原子性 2.1 原子性指的是什么意思？ 不可分割 完整性 也即某个线程在做某个具体的业务的时候 中间不可以被加塞或者分割 需要整体完整 要么同时成功 要么同时失败 一段时间内只允许一个操作 2.2 volatile 不保证原子性的案例的演示 */ public static void seeOkByVolatile() &#123; MyData myData=new MyData();//资源类 new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+"\t come in"); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; myData.addTO60(); System.out.println(Thread.currentThread().getName()+ "\t updated number value:"+ myData.number); &#125;,"AAA").start(); //第二个线程就是我们的main线程 while (myData.number==0) &#123; //main线程一直在这里等待循环 直到这个number不在等于0 没有可见性 &#125; System.out.println(Thread.currentThread().getName()+"\t misson is over"+"main get value:"+myData.number); &#125;&#125; 所以说volatile是 轻量级的同步机制 java c java verbose 12345678910public class T1 &#123;volatile int n =0;public void add()&#123; n++;&#125;&#125; 写覆盖 1 1 1 操作了3次 加1 各自的工作内存加1 由于synchronized 不能保证原子性 getfield iadd putfield 你先写 你先写 原子性 没有写完的时候 另外一个线程已经被唤醒 putfield 可能线程写入 丢失 后面的线程可能会把前面的线程写覆盖掉 JMM内存模型要求保证原子性 volatile不保证原子性 运行程序保证是2万 方法1 synchronized xxx 功能|+性能 原子包装的整型类 AtomicInteger的引入 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/8 * @描述： */import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicInteger;class MyData&#123; //MyData.java ===&gt; MyData.class =====&gt; jvm字节码 volatile int number=0; public void addTO60() &#123; this.number=60; &#125; /* 此时number的前面是加了voatile关键字的修饰的，volatile不保证原子性 */public void addPlus()&#123; number++;&#125;AtomicInteger atomicInteger=new AtomicInteger(); public void addAtomic() &#123;atomicInteger.getAndIncrement(); &#125;&#125;/** 1 验证volatile的可见性* 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰** */public class VolatileDemo &#123;//main一切方法的入口 public static void main(String[] args) &#123; MyData myData=new MyData(); for (int i = 1; i &lt;=20 ; i++) &#123; new Thread(()-&gt;&#123; for (int j = 1; j &lt;=1000 ; j++) &#123; myData.addPlus(); myData.addAtomic(); &#125; &#125;,String.valueOf(i) ).start(); &#125; //等待上面的20个线程全部计算完成 再用main线程取得的最终的结果值是什么 //后台的线程 /*1 main 2 后台GC线程 * */ while(Thread.activeCount()&gt;2) &#123; Thread.yield(); &#125; System.out.println(Thread.currentThread().getName()+"\t finally n umber value: "+ myData.number); System.out.println(Thread.currentThread().getName()+"\t AtomicInteger type finally n umber value: "+ myData.atomicInteger); &#125; /*volatile可以保证可见性 可以通知其线程主物理内存的值已经被修改 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰 2 验证volatile不保证原子性 2.1 原子性指的是什么意思？ 不可分割 完整性 也即某个线程在做某个具体的业务的时候 中间不可以被加塞或者分割 需要整体完整 要么同时成功 要么同时失败 一段时间内只允许一个操作 2.2 volatile 不保证原子性的案例的演示 */ public static void seeOkByVolatile() &#123; MyData myData=new MyData();//资源类 new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+"\t come in"); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; myData.addTO60(); System.out.println(Thread.currentThread().getName()+ "\t updated number value:"+ myData.number); &#125;,"AAA").start(); //第二个线程就是我们的main线程 while (myData.number==0) &#123; //main线程一直在这里等待循环 直到这个number不在等于0 没有可见性 &#125; System.out.println(Thread.currentThread().getName()+"\t misson is over"+"main get value:"+myData.number); &#125;&#125; 2.1.3 volatile指令重排 编译器 和处理器 常常做指令重排 源代码 编译器的优化的重排 指令并行的重排 内存系统的重排 最终执行的命令 数据依赖性 单线程 —-》》 多线程 源代码 12345678 底层 不一定是按照这个顺序 而是多线程环境 答题的顺序和 重排1 语句四存在数据的依赖性不能排到第一条 重排2 非计算机的了解即可 volatile禁止指令重排 线程的安全得到保证 a的前面加不加volatile a拿到的不是1 是 0 单线程环境不用担心指令重排 多线程 单线程编译器优化 编译器优化 指令并行重排的优化 内存系统的重排 3个重排 volatile可见性 Volatile的三大特性的讲解 JUC的包里面大规模使用到了单例模式 2.1.4 volatile的单例模式1单机版的单线程 12345678910111213141516171819202122232425262728293031323334353637383940package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/9 * @描述： */import javafx.scene.SnapshotParameters;/** 单机版的单线程* */public class SingleDemo &#123; private static SingleDemo instance=null; private SingleDemo()&#123; System.out.println(Thread.currentThread().getName()+"\t 我是构造方法Singledemo()" ); &#125; public static SingleDemo getInstance() &#123; if(instance==null) &#123; instance=new SingleDemo(); &#125; return instance; &#125; public static void main(String[] args) &#123; System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance()); System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance()); System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance()); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/9 * @描述： */import javafx.scene.SnapshotParameters;/** 单机版的单线程* */public class SingleDemo &#123; private static SingleDemo instance=null; private SingleDemo()&#123; System.out.println(Thread.currentThread().getName()+"\t 我是构造方法Singledemo()" ); &#125; public static SingleDemo getInstance() &#123; if(instance==null) &#123; instance=new SingleDemo(); &#125; return instance; &#125; public static void main(String[] args) &#123;// System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance());// System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance());// System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance());//// System.out.println();// System.out.println();// System.out.println(); for (int i = 0; i &lt;10 ; i++) &#123; new Thread(()-&gt; &#123; SingleDemo.getInstance(); &#125;,String.valueOf(i)).start(); &#125; &#125;&#125; 多线程 10个现在变成 6条 加上synchronized 解决问题 synchronized 整个代码都锁了 加上synchronized单例volatile的解析DCL（双端检测）介绍DCL的单例模式 高并发的环境下企业推崇的 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/9 * @描述： */import javafx.scene.SnapshotParameters;/** 单机版的单线程* */public class SingleDemo &#123; /* * 保证在多线程的指令不重排 * */ private static volatile SingleDemo instance=null; private SingleDemo()&#123; System.out.println(Thread.currentThread().getName()+"\t 我是构造方法Singledemo()" ); &#125; //DCL 模式 Double Check Lock 双端检索机制 //加锁前判断 //加锁判断 //在多线程的条件下 底层有指令重排 //如果没有控制好指令重排 public static SingleDemo getInstance() &#123; if (instance == null) &#123; synchronized (SingleDemo.class) &#123; if (instance == null) &#123; instance = new SingleDemo(); &#125; &#125; &#125; return instance; &#125; public static void main(String[] args) &#123;// System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance());// System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance());// System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance());//// System.out.println();// System.out.println();// System.out.println(); for (int i = 0; i &lt;10 ; i++) &#123; new Thread(()-&gt; &#123; SingleDemo.getInstance(); &#125;,String.valueOf(i)).start(); &#125; &#125;&#125; 先获得 再去加 i++ 2.2 CAS1 比较并交换什么是比较和交换呢 期望值和物理内存的真实值一样 修改为更新值 期望值和物理内存的真实值不一样 重新获得真实值 CASDemo源码123456789101112131415161718192021222324252627282930package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/9 * @描述： *//** CAS 是什么？ ==》》 compare and set* 先比较后交换* 比较并交换** */import java.util.concurrent.atomic.AtomicInteger;public class CASDemo &#123; public static void main(String[] args) &#123; AtomicInteger atomicInteger=new AtomicInteger(5); //劳动成果写进主物理内存 System.out.println(atomicInteger.compareAndSet(5,2019 )+"\t current data:"+atomicInteger.get()); System.out.println(); System.out.println(atomicInteger.compareAndSet(5,1024 )+"\t current data:"+atomicInteger.get()); System.out.println(); &#125;&#125; 结果 compare and set 同 修改成功 不同 修改失败 为什么用synchronized 不用CAS CAS原理 谈谈unsafe类1.UnSafe 是CAS的核心类 由于Java 方法无法直接访问底层 ,需要通过本地(native)方法来访问,UnSafe相当于一个后面,基于该类可以直接操作特额定的内存数据.UnSafe类在于sun.misc包中,其内部方法操作可以向C的指针一样直接操作内存,因为Java中CAS操作的助兴依赖于UNSafe类的方法. 2.变量ValueOffset,便是该变量在内存中的偏移地址,因为UnSafe就是根据内存偏移地址获取数据的 3.变量value和volatile修饰,保证了多线程之间的可见性. 什么是CASCAS的全称为Compare-And-Swap ,它是一条CPU并发原语.它的功能是判断内存某个位置的值是否为预期值,如果是则更新为新的值,这个过程是原子的. CAS并发原语提现在Java语言中就是sun.miscUnSaffe类中的各个方法.调用UnSafe类中的CAS方法,JVM会帮我实现CAS汇编指令.这是一种完全依赖于硬件 功能,通过它实现了原子操作,再次强调,由于CAS是一种系统原语,原语属于操作系统用于范畴,是由若干条指令组成,用于完成某个功能的一个过程,并且原语的执行必须是连续的,在执行过程中不允许中断,也即是说CAS是一条原子指令,不会造成所谓的数据不一致的问题. ABA问题 集合内的线程不安全问题 CAS的缺点 时间开销大 有个do whie 只能保证一个共享变量的原子性 ​ 一个变量可以保证原子性，多个变量可以靠锁来保证原子性 引发出的ABA问题 2.3 原子AtomicInteger的ABA问题 谈谈ABA问题的产生CAS算法实现一个重要的前提取出内存中的某个时刻的数据并比较替换，那么在这个时间差里会导致数据的变化 线程one 在位置V取出A 线程 two在V位置取出A two，线程B进行了一些操作将数值变成了B，然后线程two又将V位置的数据编程A， 这个时候线程one 进CAS操作发现内存中仍然是A，然后one操作成功 原子引用 AtomicReference12345678910111213141516171819202122232425262728293031323334353637import lombok.AllArgsConstructor;import lombok.Getter;import lombok.Setter;import lombok.ToString;import java.util.concurrent.atomic.AtomicReference;@Setter@Getter@AllArgsConstructor@ToStringclass User &#123; String userName; int age;&#125;public class AtomicReferenceDemo &#123; public static void main(String[] args) throws InterruptedException &#123; //建立用户zs User zs = new User("zs", 22); //用户ls User ls = new User("ls", 25); //原子引用 AtomicReference&lt;User&gt; userAtomicReference=new AtomicReference&lt;&gt;(); userAtomicReference.set(zs); System.out.println(userAtomicReference.compareAndSet(zs,ls)+"\t"+userAtomicReference.get().toString()); // System.out.println(userAtomicReference.compareAndSet(zs, ls)+"\t"+userAtomicReference.get().toString()); &#125;&#125; 时间戳原子引用AtomicStampedReference中间不知道修改了多少次 修改的时候加时间 新增一种机制 修改版本号（类似时间戳） T1 100 1 T2 100 1 101 2 100 3 ABA 乐观锁 自己的值总是不会被改变 自己很自信 JUC工具包 AtomicStampedReference ABA问题的解决1234/** * An &#123;@code AtomicStampedReference&#125; maintains an object reference * along with an integer "stamp", that can be updated atomically. */ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/10 * @描述： *//*开始是100到101101 到* */import org.omg.PortableInterceptor.INACTIVE;import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicReference;import java.util.concurrent.atomic.AtomicStampedReference;public class ABADemo &#123; static AtomicReference&lt;Integer&gt; atomicReference = new AtomicReference&lt;&gt;(100); static AtomicStampedReference&lt;Integer&gt; atomicStampedReference = new AtomicStampedReference&lt;&gt;(100, 1); public static void main(String[] args) &#123; System.out.println("=====以下是ABA问题的产生======================"); new Thread(() -&gt; &#123; //期望值真实值一样改成101 atomicReference.compareAndSet(100, 101); atomicReference.compareAndSet(101, 100); &#125;, "t1").start(); new Thread(() -&gt; &#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //暂停一秒钟 保证上面的1线程完成了一次ABA操作 System.out.println(atomicReference.compareAndSet(100, 2019) + "\t" + atomicReference.get()); &#125;, "t2").start(); //暂停一会进程 try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("======以下是ABA问题的解决========="); /* * * 100 101 100 * 1 2 3 * */ new Thread(() -&gt; &#123; int stamp = atomicStampedReference.getStamp(); System.out.println(Thread.currentThread().getName() + "\t第1次版本号" + stamp); //暂停1s t3线程 try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //期望值 更新值 期望的版本号 更新的版本号 atomicStampedReference.compareAndSet(100,101,atomicStampedReference.getStamp(),atomicStampedReference.getStamp()+1); System.out.println(Thread.currentThread().getName() + "\t第2次版本号" + atomicStampedReference.getStamp()); atomicStampedReference.compareAndSet(101,100,atomicStampedReference.getStamp(),atomicStampedReference.getStamp()+1); System.out.println(Thread.currentThread().getName() + "\t第3次版本号" +atomicStampedReference.getStamp()); &#125;, "t3").start(); /* * * 期望值100 2019 * * 1 想改成2 * */ new Thread(() -&gt; &#123; int stamp = atomicStampedReference.getStamp(); System.out.println(Thread.currentThread().getName() + "\t第1次版本号" + stamp); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; boolean result=atomicStampedReference.compareAndSet(100,2019,stamp,stamp+1); System.out.println(Thread.currentThread().getName() + "\t修改成功否：" +result+"\t当前最新实际版本号"+atomicStampedReference.getStamp()); System.out.println(Thread.currentThread().getName()+"当前实际最新值:"+atomicStampedReference.getReference()); &#125;,"t4").start(); &#125;&#125; 2.4 ArryList是线程不安全的，请大家编写一个 3 集合类ArrayListArrayList 前身是Vector vector 有synchronized Collections接口 Collection API的熟练运用程度 123public HashSet(int initialCapacity) &#123; map = new HashMap&lt;&gt;(initialCapacity); &#125; hashSet的底层是Map 为什么一个装1 一个装2 因为 hasSet添加的时候前面是key 后面是一个Object对象 Object对象是null的 在多线程的情况下的时候 我们使用ArrayList会出现 java.util.ConcurrentMoidificationException 这个异常 我们的解决策略有3个 1 new Vector Vector在ArrayList之前就出现了 那么ArrayList出现的意义是什么呢 Vector 保证了安全性 但是却使用了synchronized加锁，虽然保证了一定的安全性，但是并发性却下降了 2 Collections.synchronize3dList(new ArrayList) Collection 集合的接口 Collections 集合接口的一些补充 有高并发的条件下为ArrayList实现线程安全的类 写时复制 3 new CopyOnWriteArrayList() 集合类不安全之Set集合类不安全之Map2.4 Transfervalue醒脑小练习栈运行 堆存储 main方法的20复印了一份 去执行栈中的方法changeValue1 最后存在main方法中的值还是20 方法的作域和jvm的分布 基本类型传的是复印件 引用类型传递的是地址 person的值被改过了 String str=”abc”; test.changeValue3(“String…….”+str) String的特殊性 一种是字符串常量池 去池子里面检查有没有abc String特殊 String去找xxx 没有新建xxx 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/10 * @描述： */import com.mlz.study.entity.Person;import java.util.concurrent.TransferQueue;public class TestTransferValue &#123; public void changeValue1(int age) &#123; age = 30; &#125; private void changeValue2(Person str) &#123; str.setPersonName("xxx"); &#125; private void changeValue3(String str) &#123; str = "xxx"; &#125; /** * 栈运行 * 堆 存储 */ public static void main(String[] args) &#123; TestTransferValue test = new TestTransferValue(); int age = 20; test.changeValue1(age); System.out.println("age-----" + age); Person person = new Person("abc"); test.changeValue2(person); System.out.println("personName-----" + person.getPersonName()); String str = "abc"; test.changeValue3(str); System.out.println("String........." + str); &#125;&#125; 分析图片 2.5 java锁之公平锁和非公平锁公平锁/非公平锁/可重入锁/递归锁/自旋锁谈谈你的理解?请手写一个自旋锁 order policy 排序策略 公平锁 队列 先来后到 课间休息 默认非公平 加塞加上 加不上 非公平锁 加塞提问 快 定义公平锁 是指多个线程按照申请锁的顺序来获取锁类似排队打饭 先来后到非公平锁 是指在多线程获取锁的顺序并不是按照申请锁的顺序,有可能后申请的线程比先申请的线程优先获取到锁,在高并发的情况下,有可能造成优先级反转或者饥饿现象 反转 饥饿的理解 两者的区别公平锁性能下降 公平锁/非公平锁 并发包ReentrantLock的创建可以指定构造函数的boolean类型来得到公平锁或者非公平锁 默认是非公平锁 公平锁 先来后到 非公平锁 先抢先得 性能提升 题外话Java ReentrantLock而言,通过构造哈数指定该锁是否是公平锁 默认是非公平锁 非公平锁的优点在于吞吐量必公平锁大. 对于synchronized而言 也是一种非公平锁. 可重入锁（递归锁）是什么 京津冀黔 生活 12345678910111213public sync void method01()&#123;method02()&#125;public sync void method02()&#123;&#125; method01() 就是大门 但是里面的锁是不上的 默认是非公平的可重入锁 method01（）本身是同步 method2（） 又是同步 内层递归函数能获取method01的方法 ReentrantLock/synchronized就是一个典型的可重入锁知识代码说明 代码部分 synchronized 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/10 * @描述： *//* * 线程操作资源类 * *//** 同一个线程访问两个同步方法 但是他们确实访问同一把锁* 外层synchronized 内层的synchronized* */class Phone &#123; public synchronized void sendSMS() throws Exception &#123; System.out.println(Thread.currentThread().getName() + "\t invoked sendSMS"); sendEmail(); &#125; public synchronized void sendEmail() throws Exception &#123; System.out.println(Thread.currentThread().getName() + "\t #####invoked sendEmail"); &#125;&#125;/* * 外层函数获得锁之后 内层递归函数仍然能够获取该锁的代码 * * */public class ReenterLockDemo &#123; public static void main(String[] args) &#123; Phone phone = new Phone(); new Thread(() -&gt; &#123; try &#123; phone.sendSMS(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, "t1").start(); new Thread(() -&gt; &#123; try &#123; phone.sendSMS(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, "t2").start(); &#125;&#125; ReentrantLock 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/10 * @描述： *//* * 线程操作资源类 * */import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/* * 同一个线程访问两个同步方法 但是他们确实访问同一把锁 * 外层synchronized 内层的synchronized * * case two * * ReentLock就是一个典型的可重入锁 * */class Phone implements Runnable &#123; public synchronized void sendSMS() throws Exception &#123; System.out.println(Thread.currentThread().getName() + "\t invoked sendSMS"); sendEmail(); &#125; public synchronized void sendEmail() throws Exception &#123; System.out.println(Thread.currentThread().getName() + "\t #####invoked sendEmail"); &#125; Lock lock = new ReentrantLock(); @Override public void run() &#123; get(); &#125; public void get() &#123; lock.lock(); try &#123; System.out.println(Thread.currentThread().getName() + "\t invoked get()"); set(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void set() &#123; lock.lock(); lock.lock(); try &#123; System.out.println(Thread.currentThread().getName() + "\t #####invoked set()"); &#125; finally &#123; lock.unlock(); lock.unlock(); &#125; &#125;&#125;/* * 外层函数获得锁之后 内层递归函数仍然能够获取该锁的代码 * * */public class ReenterLockDemo &#123; public static void main(String[] args) &#123; Phone phone = new Phone(); new Thread(() -&gt; &#123; try &#123; phone.sendSMS(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, "t1").start(); new Thread(() -&gt; &#123; try &#123; phone.sendSMS(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, "t2").start(); /* * 只要锁时匹配的有配对就行 * 两把锁 * 3把锁 * n把锁 * */ System.out.println(); System.out.println(); System.out.println(); //暂停一会 try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; /*phone实现了Runnable接口 * */ Thread t3 = new Thread(phone); Thread t4 = new Thread(phone); t3.start(); t4.start(); &#125;&#125; 可重入锁最大的作用就是避免死锁配对 加两对 编译]]></content>
  </entry>
  <entry>
    <title><![CDATA[springcloud]]></title>
    <url>%2Fblog4%2F2019%2F10%2F08%2Fspringcloud%2F</url>
    <content type="text"><![CDATA[1 微服务与SpringCloud1.1 马丁.福勒强调的是服务的大小，它关注的是某一个点，是具体解决某一个问题/提供落地对应服务的一个服务应用,狭意的看,可以看作Eclipse里面的一个个微服务工程/或者Module 1.2 微服务的技术栈2 Rest服务构建案例工程模块3 Eureka服务注册与发现4 Ribbon负载均衡5Feign负载均衡6 Hystrix熔断器7 Zuul路由网关]]></content>
  </entry>
  <entry>
    <title><![CDATA[dubbo]]></title>
    <url>%2Fblog4%2F2019%2F10%2F08%2Fdubbo%2F</url>
    <content type="text"><![CDATA[1. dubbo的hello-world 消费者 服务的提供者 解决名称空间导入的问题 https://blog.csdn.net/feinifi/article/details/86677534]]></content>
  </entry>
</search>
