<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Kylin]]></title>
    <url>%2Fblog4%2F2019%2F10%2F09%2FKylin%2F</url>
    <content type="text"><![CDATA[1 概述1.1 Kylin定义开源 分布式 分析引擎 提供基于Hadoop/spark的sql查询和多维分析 OLAP能力（联机分析处理） 亚秒内查询巨大的Hive表 1.2 架构 1 RestServer Rsultful接口 提供查询 2 查询引擎 解析用户查询 3 路由器 在发行版是默认关闭的 体验不好 Hive的速度和Kylin的速度相差加大 4 元数据管理工具 元数据驱动应用程序 kylin的元数据存储在hbase中 5 分析引擎 处理离线任务 shell脚本 java API MapReduce任务 任务引擎需要对kylin中的任务进行协调和管理 1.3特点SQL接口 超大规模数据集 亚秒级的响应 ​ 很多复杂的计算 连接 聚合 离线的预计算过程就完成了 可伸缩性和高吞吐率 搭建集群 每秒70个查询 BI ODBC tableau JDBC RestPI Kylin 2 环境搭建官方文档即可 3 入门4 Cube构建原理5 cube构建优化6 BI集成]]></content>
  </entry>
  <entry>
    <title><![CDATA[zookeeper]]></title>
    <url>%2Fblog4%2F2019%2F10%2F09%2Fzookeeper%2F</url>
    <content type="text"><![CDATA[1 zookeeper入门1.1概述开源 为分布式应用提供协调服 Zookeeper=文件系统+通知机制 1.2 特点Leader follower 半数机制 每个server保存的数据一致（数据备份） 实时性 更新请求顺序执行 1.3 数据结构unix文件系统 形结构 1.4 应用场景统一的配置管理和配置服务 1.5下载地址https://zookeeper.apache.org/ 2 zookeeper的安装2.1 本地模式zoo.cfg bin/zksSrver.sh start|status|stop 出现bash4.1 .bash_profile 每次登陆 .bashrc 每次进入新的bash环境 .bash_logout 每次退出登录 .bash_history 每用户注销前使用的命令 进入root用户 cp .bash_profile .bashrc .bash_logout .bash_profile /home/user 问题得到解决 2.2 配置参数的解读tickTime 2000 initLimit 10 syncLimit dataDir clientPort 3 Zookeeper的内部的原理3.1 选举机制半数机制，适合安装奇数台服务器 没有mater和slave，但是zookeeper在工作的时候 一个leader 其他的都是follower 4 zookeeper的实战分布式部署安装zoo.cfg myid zkData 12345dataDir=/opt/module/zookeeper-3.4.10/zkData#######################cluster##########################server.2=hadoop102:2888:3888server.3=hadoop103:2888:3888server.4=hadoop104:2888:3888 server.A=B:C:D A myid B 主机名称 C 与Leader交换信息 D 一但leader挂掉 选举出新的leader 4.2 API操作5 面试zookeeper的选举机制zookeeper的监听原理main方法 connect线程和listener线程 connect 将监听的事件发送给zookeeper zookeeper在监控l列表上添加事件 listener线程监听到事件变化 zookeeper调用process方法 zookeeper的部署的方式本地部署 分布式部署 zookeeper的常用命令ls create get delete set]]></content>
  </entry>
  <entry>
    <title><![CDATA[dianshangtuijianxitong]]></title>
    <url>%2Fblog4%2F2019%2F10%2F09%2Fdianshangtuijianxitong%2F</url>
    <content type="text"><![CDATA[1 电商推荐系统的简介1.1 项目的架构设计1.1.2 亚马逊推荐系统的贡献 比例达到了20%到25% 在亚马逊的页面上 推荐列表占了很大的比例 真实的业务架构 构建真正的系统 基于统计的模块 实时 自定义的推荐模块 设计到机器学习 和推荐系统的相关知识 电影推荐系统 切换不同的业务场景 一通百通 对大数据的工具有一个更深刻的理解、 做的是电商推荐系统 推荐系统的具体的应用 1.1.2 分析项目框架 数据源解析 统计推荐模块+ 基于LFM的离线推荐模块 基于自定义模型的实时推荐模块 其他形式的离线相似推荐 基于内容的推荐和 ​ 基于物品的协同过滤模块 1.1.3 数据的生命周期 数据源 三大类 图片 视频 非结构化的数据 日志数据 半结构化的数据 结构化数据 关系数据 数据源 数据采集 数据存储 数据计算 数据应用 用的数据库 mongodb 运算处理 （Mahout） hadoop的 storm的流式处理 spark flink 大数据的计算框架 算完之后 存储到数据库里 分析 Echarts 等数据可视化展示 Cassandra NoSQL 1.2 大数据的处理流程 左边的实时的处理 网站 APP 前端页面 用户接口 —》》》》 http请求 —-》》》 业务系统的后台—-》》》》 调用相应的服务 响应—–》》 埋点收集日志 –》》 记录用户的行为 —》》 日志的采集 （Flume） 数据总线 —》》 KAfka （做存储） 实时消费 flume的数据 —-》》 sparkStreaming 实时计算 —-》》 数据的可视化展示 右边的离线处理的流程 业务系统 日志文件 flume 日志采集 sink 配置成hdfs存储 日志清洗ETL 做数据清洗的操作 数据仓库 最后对这些数据进行计算 对数据进行离线计算 相关的业务数据库 最后做可视化展示 推荐系统 —》》 大数据的典型应用 1.3 我们的目标 不同的地方显示不同的推荐的结果 商品的详情 评分 xxx 然后下面相似的商品推荐出来 混合推荐 典型分区混合 1.4 项目的系统架构用户可视化 Angular JS 前端 后台spring 不是我们主要考虑的地方 数据 存储 MongoDB 一些重要的数据 缓存到Redis里面 并不是一定要存MongoDB mongoDB大数据平台很主流的数据库 读写性能 支持很大的数据量 mongo是一个文档型数据库 json串 存储在里面 很多的特征 redis缓存常规操作 ES 模糊查询 条件查询 离线的推荐服务 统计服务 个性化统计服务 在线 实时 Flume kafka 缓冲 sparkStreaming 实时的推荐处理 Redis缓存的数据 写回到mongo里 用户可视化 推荐 1.4.1 离线数据加载服务 数据放到mongo里面 离线统计服务 sparkSQL 写回表 对应的查询 个性化推荐 隐语义模型的推荐模块 基于内容的推荐 最终的结果写进mongo 1.4.2 实时log Flume kafka 消息缓冲 对应的消息处理 spark Streaming对数据过滤 最后做实时的计算 redis里面拿到最近的评分数据 ‘ 商品检索 mongo es也是可以的 1.5 数据源的解析1.5.1 信息介绍 商品信息 1.用户评分信息 基于商品的信息 做基于内容推荐 用户的评分数据 隐语义模型 协同过滤的推荐 1.6主要的数据模型商品相似度 为了实时推荐做基础 实时 1.6 统计推荐模块商品相似度 1.6.1 历史热门商品统计什么样的商品是热门的 评分的多少 1select productId ,count(productId) as count from ratings group by productId order by count desc RatingMoreProducts 1.6.2近期热门商品的统计UDF函数 changeDate 时间戳 转换为年月的格式 分解成一个月的的热门商品 1.6.3商品的平均得分的统计统计的指标 相应的实现 1.7 离线推荐模块ALS算法进行隐语义模型训练 ALS.train() lambda 正则化参数 iterations 迭代次数 隐语义模型定义的隐特征的个数 涉及到了模型评估和参数调整 RMSE 考察预测评分和实际评分的误差 得到选取什么样的参数是最好的 1.7.1 计算用户推荐矩阵 user的RDD 和product的RDD做了一个笛卡尔积 物品两两匹配得到的结果 model.predict uid 聚合 sortBy（“score”.take(20) sparkSession.write 1.7.2计算商品相似度矩阵特征向量 矩阵分解 用户的特征向量矩阵 商品的特征向量矩阵 笛卡尔积 给实时推荐做基础 1.8 基于模型的实时推荐模块 1.8.1 需求计算速度快 结果可以不是特别精确 有预先设计好的推荐模型 评分数据 flume kafka log mongo 里面 redis里 结果写回mongo 1.8.2推荐优先级的计算刚看的商品 差评的物品 综合考虑相似度和评分 对剑优先级 计算公式 .推荐的基础评分项 奖励 惩罚 incount 评分里面的高分项 AB 高分 奖励 C 低分 惩罚 前面的基础项加权 lg 2 AB高分 C是低分 1.9 其他形式的离线相似推荐点开用户的商品详情页 出现相似的内容 基于用户购买了哪些商品 1.9.1 基于内容的推荐与A有相同的标签的商品 TF-IDF算法 根据UGC的特征提取item-CF算法 喜欢商品A的人还喜欢哪些商品 TF 词频 每一个标签 词语 每一个商品获得的一个标签 提取出物品的特征向量 余弦相似度 1.9.3 基于物品的协同过滤 1.10 混合推荐分区推荐 同时购买了商品i和商品j 2 环境的搭建2.1 安装monhgodb1234567891011wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel62-3.4.3.tgztar xvf 到 ./cluster然后mv /usr/local/mongodbcd /usr/local/mongodb创建 /usr/local/mongodb/data/usr/local/mongodb/data/db/usr/local/mongodb/data/logs/usr/local/mongodb/data/logs/mongodb.log/usr/local/mongodb/data/mongodb.conf mongodb.conf 1234567891011#端口号port = 27017#数据目录dbpath = /usr/local/mongodb/data/db#日志目录logpath = /usr/local/mongodb/data/logs/mongodb.log#设置后台运行fork = true#日志输出方式logappend = true#开启认证#auth = true 2.1.1 mongodb的启动1sudo /usr/local/mongodb/bin/mongod -config /usr/local/mongodb/data/mongodb.conf 2.1.2 mongodb的访问1/usr/local/mongodb/bin/mongo 2.1.3 mongodb的停止1/usr/local/mongodb/bin/mongo 2.2 安装Redis2.2.1 Redis的基本安装获得安装包 12wget http://download.redis.io/releases/redis-4.0.2.tar.gz 解压 tar xvf xxx ./clsuetr cd redis 1sudo yum install gcc 编译源代码 1make MALLOC=libc 编译安装 1sudo make install 创建配置文件 1sudo cp ./redis-4.0.2/redis.conf /etc/redis.conf 修改配置文件 12345daemonize yes #37行 #是否以后台daemon方式运行，默认不是后台运行pidfile /var/run/redis/redis.pid #41行 #redis的PID文件路径（可选）bind 0.0.0.0 #64行 #绑定主机IP，默认值为127.0.0.1，我们是跨机器运行，所以需要更改logfile /var/log/redis/redis.log #104行i #定义log文件位置，模式log信息定向到stdout，输出到/dev/null（可选）dir “/usr/local/rdbfile” #188行 #本地数据库存放路径，默认为./，编译安装默认存在在/usr/local/bin下（可选） 2.2.2 Redis的基本使用启动redis服务器 1redis-server /etc/redis.conf 连接redis服务器 1redis-cli 停止redis服务器 1redis-cli shutdown 2.3 安装Spark（单节点） 上传安装包 ./cluster 配置slaves 添加主机名 hadoop101 配置spark的主机名称和端口号 spark-env.sh 12SPARK_MASTER_HOST=linux #添加spark master的主机名SPARK_MASTER_PORT=7077 #添加spark master的端口号 14. 启动 1sbin/start-all.sh 2.4 安装 Zookeeper 单节点2.5 安装Flume-ng 单节点 上传安装包 1wget http://www.apache.org/dyn/closer.lua/flume/1.8.0/apache-flume-1.8.0-bin.tar.gz ​ 2. 解压 即可 2.6 安装kafka 单节点1 解压 2 配置 server.properties 3 kafka节点的使用 3 项目的搭建3.1 新建maven项目com.xxx 最外层pom.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889 &lt;properties&gt; &lt;log4j.version&gt;1.2.17&lt;/log4j.version&gt; &lt;slf4j.version&gt;1.7.22&lt;/slf4j.version&gt; &lt;mongodb-spark.version&gt;2.0.0&lt;/mongodb-spark.version&gt; &lt;casbah.version&gt;3.1.1&lt;/casbah.version&gt; &lt;redis.version&gt;2.9.0&lt;/redis.version&gt; &lt;kafka.version&gt;0.10.2.1&lt;/kafka.version&gt; &lt;spark.version&gt;2.1.1&lt;/spark.version&gt; &lt;scala.version&gt;2.11.8&lt;/scala.version&gt; &lt;jblas.version&gt;1.2.1&lt;/jblas.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;jcl-over-slf4j&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;!--maven项目 各种各样的声明周期编译的插件--&gt; &lt;build&gt; &lt;!--声明并引入子项目共有的插件--&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.6.1&lt;/version&gt; &lt;!--所有的编译用JDK1.8--&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;!--maven的打包插件--&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!--该插件用于将scala代码编译成class文件--&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt; &lt;executions&gt; &lt;!--绑定到maven的编译阶段--&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;/build&gt; 2.2reoommender 的pom.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- 引入Spark相关的Jar包 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-mllib_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-graphx_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;build&gt; &lt;plugins&gt; &lt;!-- 父项目已声明该plugin，子项目在引入的时候，不用声明版本和已经声明的配置 --&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 2.3DataLoader 的搭建pom.xml 123456789101112131415161718192021222324252627&lt;dependencies&gt; &lt;!-- Spark的依赖引入 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 引入Scala --&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 加入MongoDB的驱动 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;casbah-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;casbah.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt; &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;mongodb-spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; log4j.properties的配置 1234log4j.rootLogger=info, stdoutlog4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; %5p --- [%50t] %-80c(line:%5L) : %m%n 导入数据集 新建类 样例类 搭建基本的程序框架 隐式类的加入 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154package com.atguigu.recommenderimport com.mongodb.casbah.commons.MongoDBObjectimport com.mongodb.casbah.&#123;MongoClient, MongoClientURI&#125;import org.apache.spark.SparkConfimport org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;/* * @创建人: MaLingZhao * @创建时间: 2019/9/29 * @描述： 推荐系统的加载服务 *//*Producct 数据集21643^ 商品ID 0巴黎欧莱雅男士劲能醒肤露 8重功效50ml^ 商品名称 1916,502,352 商品的分类ID 不需要 2B0010MBKHO^ 亚马逊ID 不需要 3Y300_QL70_.jpg^ 商品的图URL 为了和业务系统相关联小说|文学艺术|图书音像 商品分类 5男士乳液/面霜|男士护肤| 商品的UGC标签 6*/case class Product(productId :Int,name:String, imageUrl:String,categories:String,tags:String)/*Rating数据集407423, 用户ID457976, 商品ID5.0, 评分数据1379001600 时间戳 *///Int 类型case class Rating(userId:Int, productId:Int,score:Double,timestamp:Int)/*uri mongoDb的ueldb 要操作的db *///mongo的配置封装样例类case class MongoConfig(uri:String,db:String)object DataLoader &#123; //定义数据文件路径 val PRODUCT_DATA_PATH="D:\\xiangmu\\ECommerceRecommendSystem\\recommender\\DataLoader\\src\\main\\resources\\products.csv" val RATING_DATA_PATH="D:\\xiangmu\\ECommerceRecommendSystem\\recommender\\DataLoader\\src\\main\\resources\\ratings.csv" //定义mongoDB中存储的表名 val MONGODB_PRODUCT_COLLECTION="Product" val MONGODB_RATING_COLLECTION="Rating" def main(args: Array[String]): Unit = &#123; val config=Map( "spark.cores" -&gt; "local[*]", "mongo.uri" -&gt; "mongodb://linux:27017/recommender", "mongo.db" -&gt;"recommender" ) //创建一个spark Config val sparkConf=new SparkConf().setMaster(config("spark.cores")).setAppName("DataLoader"); //创建sparkSession val spark=SparkSession.builder().config(sparkConf).getOrCreate() //隐式转换的包 import spark.implicits._ //加载数据 val productRDD= spark.sparkContext.textFile(PRODUCT_DATA_PATH) //转换成表结构 val productDF=productRDD.map(item =&gt;&#123; //第一个反斜杠 转义后面的反斜杠 数据通过^分割 val attr=item.split("\\^") //转换成product类 最后一行代表代码的返回 Product(attr(0).toInt,attr(1).trim,attr(4).trim,attr(5).trim,attr(6).trim) &#125;).toDF() val ratingRDD=spark.sparkContext.textFile(RATING_DATA_PATH) val ratingDF=ratingRDD.map(item=&gt;&#123; //逗号不用转义 val attr=item.split("," ) Rating(attr(0).toInt,attr(1).toInt,attr(2).toDouble,attr(3).toInt) &#125;).toDF() //调用多次的活 不用每次传入参数 implicit val mongoConfig=MongoConfig(config("mongo.uri"),config("mongo.db")) storeDataInMongoDB(productDF,ratingDF) //spark.stop()、 &#125; //隐式参数的使用 //以后调用的参数 def storeDataInMongoDB(productDF:DataFrame,raingDF:DataFrame)(implicit mongoConfig: MongoConfig): Unit = &#123; //新建和一个mongoDb的连接 客户端 val mongoClient = MongoClient(MongoClientURI(mongoConfig.uri)) //定义要操作的mongoDb中的表 val productCollection = mongoClient(mongoConfig.db)(MONGODB_PRODUCT_COLLECTION) val ratingCollection = mongoClient(mongoConfig.db)(MONGODB_RATING_COLLECTION) //如果表存在的话 删除 productCollection.dropCollection() ratingCollection.dropCollection() //将当前的数据存入对应的表中 productDF.write .option("uri", mongoConfig.uri) .option("collection", MONGODB_PRODUCT_COLLECTION) .mode("overwrite") .format("com.mongodb.spark.sql") .save() raingDF.write .option("uri", mongoConfig.uri) .option("collection", MONGODB_RATING_COLLECTION) .mode("overwrite") .format("com.mongodb.spark.sql") .save() //创建索引 //字段 productCollection.createIndex(MongoDBObject("productId" -&gt; 1)) ratingCollection.createIndex(MongoDBObject("productId" -&gt; 1)) ratingCollection.createIndex(MongoDBObject("userId" -&gt; 1)) mongoClient.close() &#125; &#125; 运行程序 测试代码吗 启动本地的mongo的测试 4 离线推荐服务 代码4.1 创建maven项目任务调度工具 离线统计服务 pom.xml 123456789101112131415161718192021222324252627&lt;dependencies&gt; &lt;!-- Spark的依赖引入 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 引入Scala --&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 加入MongoDB的驱动 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;casbah-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;casbah.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt; &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;mongodb-spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 统计学习模块 没有MLib log4j 复制过来 建立类 4.2 创建离线统计 服务 StatisticsRecommender代码部分 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142package com.atguigu.statisticsimport java.text.SimpleDateFormatimport java.util.Dateimport org.apache.spark.SparkConfimport org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;/* * @创建人: MaLingZhao * @创建时间: 2019/9/30 * @描述： *//*商品的信息不是特别的重要只需要评分的数据集 */case class Rating(userId:Int, productId:Int,score:Double,timestamp:Int)/*uri mongoDb的ueldb 要操作的db *///mongo的配置封装样例类case class MongoConfig(uri:String,db:String)object StatisticsRecommender &#123; val MONGODB_RATING_COLLECTION="Rating" //统计的表的名称 val RATE_MORE_PRODUCTS = "RateMoreProducts" val RATE_MORE_RECENTLY_PRODUCTS = "RateMoreRecentlyProducts" val AVERAGE_PRODUCTS = "AverageProducts" def main(args: Array[String]): Unit = &#123; val config = Map( "spark.cores" -&gt; "local[*]", "mongo.uri" -&gt; "mongodb://linux:27017/recommender", "mongo.db" -&gt; "recommender" ) //创建一个spark Config val sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("StatisticsRecommender"); //创建sparkSession val spark = SparkSession.builder().config(sparkConf).getOrCreate() //隐式转换的包 import spark.implicits._ //调用自己的包里面的 //调用多次的活 不用每次传入参数 implicit val mongoConfig=MongoConfig(config("mongo.uri"),config("mongo.db")) //加载数据 val ratingDF = spark.read .option("uri",mongoConfig.uri) .option("collection",MONGODB_RATING_COLLECTION) .format("com.mongodb.spark.sql") .load() //转换为定义的数据结构 .as[Rating] .toDF() //创建一张ratings的临时表 ratingDF.createOrReplaceTempView("ratings") //TODO 用spark sql 去做不同的统计推荐 //1. 历史热门商品 ，按照评分个数统计 //id 聚合 降序排列 productId count val rateMoreProductsDF =spark.sql("select productId,count(productId) as count from ratings group by productId order by count desc") //一一对应 storeDFInMongoDB(rateMoreProductsDF,RATE_MORE_PRODUCTS) //2. 近期热门商品，把时间戳转换成yyyyMM 年月格式进行评分个数统计 最终得到的是 productId count yearmonth // 创建一个日期的格式化工具 val simpleDateFormat=new SimpleDateFormat("yyyyMM") //注册UDF，将timestamp转化为年月格式yyyyMM //x为毫秒 x的格式为长整型 spark.udf.register("changeDate",(x:Int)=&gt;simpleDateFormat.format(new Date(x*1000L)).toInt) //把ratings 数据转换成想要的格式 productId，score yearmonth val ratingOfYearMonthDF=spark.sql("select productId,score,changeDate(timestamp) as yearmonth from ratings") ratingOfYearMonthDF.createOrReplaceTempView("ratingOfMonth") val rateMoreRecentlyProductDF =spark.sql("select productId, count(productId) as count ,yearmonth from ratingOfMonth group by yearmonth,productId order by yearmonth desc,count desc ") //把DF保存到mongoDB storeDFInMongoDB(rateMoreProductsDF,RATE_MORE_RECENTLY_PRODUCTS) //3. 优质商品统计 ，商品的平均得分 val averageProductsDF=spark.sql("select productId,avg(score) as avg from ratings group by productId order by avg desc ") storeDFInMongoDB(averageProductsDF,AVERAGE_PRODUCTS) spark.stop() &#125; def storeDFInMongoDB(df: DataFrame, collection_name: String)(implicit mongoConfig: MongoConfig): Unit = &#123; df.write .option("uri",mongoConfig.uri) .option("collection",collection_name) .mode("overwrite") .format("com.mongodb.spark.sql") .save()&#125;&#125;测试代码 4.3 基于隐语义模型的推荐评分矩阵 分解成两矩阵 用户商品推荐列表 ALS训练出来的Model 计算当前用户推荐列表 userId 和productId 产生(userId,productId)的分组 预测评分 评分排序 返回分支最大的K个商品 作为当前用户的推荐列表 UserRevcs 4.3.2 商品相似度矩阵 存储结构 4.3.4 创建模块 pom.xml 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scalanlp&lt;/groupId&gt; &lt;artifactId&gt;jblas&lt;/artifactId&gt; &lt;version&gt;$&#123;jblas.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Spark的依赖引入 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-mllib_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 引入Scala --&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 加入MongoDB的驱动 --&gt; &lt;!-- 用于代码方式连接MongoDB --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;casbah-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;casbah.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 用于Spark和MongoDB的对接 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt; &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;mongodb-spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; log4j配置文件的复制 4.3.5ALS算法代码部分 包装样例类 12345678910//定义标准推荐对象case class Recommendation(productId:Int,scre:Double) //定义用户的推荐列表case class UserRecs(userId:Int,recs:Seq[Recommendation]) //定义商品相似度列表case class ProductRecs(productId:Int,recs:Seq[Recommendation]) scala代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165case class ProductRating(userId:Int, productId:Int,score:Double,timestamp:Int)//定义标准推荐对象case class Recommendation(productId:Int,scre:Double)//定义用户的推荐列表case class UserRecs(userId:Int,recs:Seq[Recommendation])//定义商品相似度列表case class ProductRecs(productId:Int,recs:Seq[Recommendation])/*uri mongoDb的ueldb 要操作的db *///mongo的配置封装样例类case class MongoConfig(uri:String,db:String)object OfflineRecommender &#123; //定义mongodb中存储的表名 val MONGODB_RATING_COLLECTION = "Rating" //用户的推荐列表 val USER_RECS = "UserRecs" //商品相似度表 val PRODUCT_RECS = "ProductRecs" //最大的返回数量 val USER_MAX_RECOMMENDATION = 20 def main(args: Array[String]): Unit = &#123; val config = Map( "spark.cores" -&gt; "local[*]", "mongo.uri" -&gt; "mongodb://linux:27017/recommender", "mongo.db" -&gt; "recommender" ) //创建一个spark Config val sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("StatisticsRecommender"); //创建sparkSession val spark = SparkSession.builder().config(sparkConf).getOrCreate() //隐式转换的包 import spark.implicits._ //调用自己的包里面的 //调用多次的活 不用每次传入参数 implicit val mongoConfig = MongoConfig(config("mongo.uri"), config("mongo.db")) //加载数据 RDD ALS训练的时候使用 val ratingRDD = spark.read .option("uri", mongoConfig.uri) .option("collection", MONGODB_RATING_COLLECTION) .format("com.mongodb.spark.sql") .load() //转换为定义的数据结构 .as[ProductRating] .rdd .map( rating =&gt; (rating.userId, rating.productId, rating.score) ) .cache() //避免RDD的重复计算 // 提取出用户和商品的所有数据集 val userRDD = ratingRDD.map(_._1).distinct() val productRDD = ratingRDD.map(_._2).distinct() //TODO 核心计算过程 //1 训练隐语义模型 val trainData = ratingRDD.map(x =&gt; Rating(x._1, x._2, x._3)) //定义模型训练参数 rank 隐语义的隐特征的个数 iterations 迭代次数 lambda 正则化项系数 val (rank, iterations, lambda) = (5, 10, 0.01) val model = ALS.train(trainData, rank, iterations, lambda) //2 获得预测评分矩阵，得到用户的推荐列表 //userRDD 和productRDD 做笛卡尔积 得到空的userProductRDD表示的评分矩阵 val userProducts = userRDD.cartesian(productRDD) val preRating = model.predict(userProducts) //得到从预测评分矩阵提得到用户推荐列表 val userRecs = preRating.filter(_.rating &gt; 0) //每个用户id 物品对应的id .map( rating =&gt; (rating.user, (rating.product, rating.rating))) .groupByKey() .map &#123; case (userId, recs) =&gt; //降序排列 sort UserRecs(userId, recs.toList.sortWith(_._2 &gt; _._2).take(USER_MAX_RECOMMENDATION) .map(x =&gt; Recommendation(x._1, x._2))) &#125; .toDF() //写回到mongoDB 数据库中 userRecs.write .option("uri", mongoConfig.uri) .option("collection", USER_RECS) .mode("overwrite") .format("com.mongodb.spark.sql") .save() //3 利用商品的特征向量，计算商品的相似度列表 // 3. 利用商品的特征向量，计算商品的相似度列表 val productFeatures = model.productFeatures.map&#123; case (productId, features) =&gt; ( productId, new DoubleMatrix(features) ) &#125; // 两两配对商品，计算余弦相似度 val productRecs = productFeatures.cartesian(productFeatures) .filter&#123; case (a, b) =&gt; a._1 != b._1 &#125; // 计算余弦相似度 .map&#123; case (a, b) =&gt; val simScore = consinSim( a._2, b._2 ) ( a._1, ( b._1, simScore ) ) &#125; .filter(_._2._2 &gt; 0.4) .groupByKey() .map&#123; case (productId, recs) =&gt; ProductRecs( productId, recs.toList.sortWith(_._2&gt;_._2).map(x=&gt;Recommendation(x._1,x._2)) ) &#125; .toDF() productRecs.write .option("uri", mongoConfig.uri) .option("collection", PRODUCT_RECS) .mode("overwrite") .format("com.mongodb.spark.sql") .save() spark.stop() &#125; def consinSim(product1: DoubleMatrix, product2: DoubleMatrix): Double = &#123; product1.dot(product2)/(product1.norm2()) * product2.norm2()&#125;&#125; 4.3.3 模型参数评估选取最优的参数 RMSE 求得误差 考察 12 ALSTrainer 代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071 object ALSTrainer&#123;def main(args: Array[String]): Unit = &#123; val config = Map( "spark.cores" -&gt; "local[*]", "mongo.uri" -&gt; "mongodb://localhost:27017/recommender", "mongo.db" -&gt; "recommender" ) // 创建一个spark config val sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("OfflineRecommender") // 创建spark session val spark = SparkSession.builder().config(sparkConf).getOrCreate() import spark.implicits._ implicit val mongoConfig = MongoConfig( config("mongo.uri"), config("mongo.db") ) // 加载数据 val ratingRDD = spark.read .option("uri", mongoConfig.uri) .option("collection", MONGODB_RATING_COLLECTION) .format("com.mongodb.spark.sql") .load() .as[ProductRating] .rdd .map( rating =&gt; Rating(rating.userId, rating.productId, rating.score) ).cache() // 数据集切分成训练集和测试集 val splits = ratingRDD.randomSplit(Array(0.8, 0.2)) val trainingRDD = splits(0) val testingRDD = splits(1) // 核心实现：输出最优参数 adjustALSParams( trainingRDD, testingRDD ) spark.stop()&#125; def adjustALSParams(trainData: RDD[Rating], testData: RDD[Rating]): Unit =&#123; // 遍历数组中定义的参数取值 val result = for( rank &lt;- Array(5, 10, 20, 50); lambda &lt;- Array(1, 0.1, 0.01) ) yield &#123; val model = ALS.train(trainData, rank, 10, lambda) val rmse = getRMSE( model, testData ) ( rank, lambda, rmse )&#125; // 按照rmse排序并输出最优参数 println(result.minBy(_._3))&#125; def getRMSE(model: MatrixFactorizationModel, data: RDD[Rating]): Double = &#123; //构建UserProducts， 得到预测的评分矩阵 val userProducts: RDD[(Int, Int)] = data.map(item=&gt;(item.user,item.product)) val predictRating: RDD[Rating] = model.predict(userProducts) //按照公式计算 RMSE，首先把预测评分和实际评分表做一个连接 以（userID和productID）做一个连接 val observed : RDD[((Int, Int), Double)] = data.map(item=&gt;((item.user,item.product),item.rating)) val predict: RDD[((Int, Int), Double)] = predictRating.map(item=&gt;((item.user,item.product),item.rating)) sqrt(observed.join(predict).map&#123; case((userId,productId),(actual,pre)) =&gt; val error = actual -pre error*error &#125;.mean()) &#125;&#125; 5 实时推荐模块5.1 分析用户最近的偏好 之前买过什么商品 基于这样的思想 构建模型 不需要那么精确 基于模型的实时架构 mongo redis 日志 评分数据 userId productId 时间戳 flume kafka Stream 过滤 recommender spark streaming 定义实时推荐算法 用户最近的评分 redis 推荐优先级的计算 基本原理 用户最近一段时间的口味是相似的 最近的k次评分 好评 差评 5.1.2 推荐优先级计算 5.1.3 实现创建项目 pom.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&lt;dependencies&gt; &lt;!-- Spark的依赖引入 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 引入Scala --&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 加入MongoDB的驱动 --&gt; &lt;!-- 用于代码方式连接MongoDB --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;casbah-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;casbah.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 用于Spark和MongoDB的对接 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt; &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;mongodb-spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- redis --&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- kafka --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.10.2.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 实时系统的搭建 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213package com.atguigu.onlineimport com.mongodb.casbah.commons.MongoDBObjectimport com.mongodb.casbah.&#123;MongoClient, MongoClientURI&#125;import org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.SparkConfimport org.apache.spark.sql.SparkSessionimport org.apache.spark.streaming.kafka010.&#123;ConsumerStrategies, KafkaUtils, LocationStrategies&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import redis.clients.jedis.Jedis// 定义一个连接助手对象，建立到redis和mongodb的连接object ConnHelper extends Serializable&#123; // 懒变量定义，使用的时候才初始化 lazy val jedis = new Jedis("localhost") lazy val mongoClient = MongoClient(MongoClientURI("mongodb://linux:27017/recommender"))&#125;case class MongoConfig( uri: String, db: String )// 定义标准推荐对象case class Recommendation( productId: Int, score: Double )// 定义用户的推荐列表case class UserRecs( userId: Int, recs: Seq[Recommendation] )// 定义商品相似度列表case class ProductRecs( productId: Int, recs: Seq[Recommendation] )object OnlineRecommender &#123; // 定义常量和表名 val MONGODB_RATING_COLLECTION = "Rating" val STREAM_RECS = "StreamRecs" val PRODUCT_RECS = "ProductRecs" val MAX_USER_RATING_NUM = 20 val MAX_SIM_PRODUCTS_NUM = 20 def main(args: Array[String]): Unit = &#123; val config = Map( "spark.cores" -&gt; "local[*]", "mongo.uri" -&gt; "mongodb://localhost:27017/recommender", "mongo.db" -&gt; "recommender", "kafka.topic" -&gt; "recommender" ) // 创建spark conf val sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("OnlineRecommender") val spark = SparkSession.builder().config(sparkConf).getOrCreate() val sc = spark.sparkContext val ssc = new StreamingContext(sc, Seconds(2)) import spark.implicits._ implicit val mongoConfig = MongoConfig( config("mongo.uri"), config("mongo.db") ) // 加载数据，相似度矩阵，广播出去 val simProductsMatrix = spark.read .option("uri", mongoConfig.uri) .option("collection", PRODUCT_RECS) .format("com.mongodb.spark.sql") .load() .as[ProductRecs] .rdd // 为了后续查询相似度方便，把数据转换成map形式 .map&#123;item =&gt; ( item.productId, item.recs.map( x=&gt;(x.productId, x.score) ).toMap ) &#125; .collectAsMap() // 定义广播变量 val simProcutsMatrixBC = sc.broadcast(simProductsMatrix) // 创建kafka配置参数 val kafkaParam = Map( "bootstrap.servers" -&gt; "linux:9092", "key.deserializer" -&gt; classOf[StringDeserializer], "value.deserializer" -&gt; classOf[StringDeserializer], "group.id" -&gt; "recommender", "auto.offset.reset" -&gt; "latest" ) // 创建一个DStream val kafkaStream = KafkaUtils.createDirectStream[String, String]( ssc, LocationStrategies.PreferConsistent, ConsumerStrategies.Subscribe[String, String]( Array(config("kafka.topic")), kafkaParam ) ) // 对kafkaStream进行处理，产生评分流，userId|productId|score|timestamp val ratingStream = kafkaStream.map&#123;msg=&gt; var attr = msg.value().split("\\|") ( attr(0).toInt, attr(1).toInt, attr(2).toDouble, attr(3).toInt ) &#125; // 核心算法部分，定义评分流的处理流程 ratingStream.foreachRDD&#123; rdds =&gt; rdds.foreach&#123; case ( userId, productId, score, timestamp ) =&gt; println("rating data coming!&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;") // TODO: 核心算法流程 // 1. 从redis里取出当前用户的最近评分，保存成一个数组Array[(productId, score)] val userRecentlyRatings = getUserRecentlyRatings( MAX_USER_RATING_NUM, userId, ConnHelper.jedis ) // 2. 从相似度矩阵中获取当前商品最相似的商品列表，作为备选列表，保存成一个数组Array[productId] val candidateProducts = getTopSimProducts( MAX_SIM_PRODUCTS_NUM, productId, userId, simProcutsMatrixBC.value ) // 3. 计算每个备选商品的推荐优先级，得到当前用户的实时推荐列表，保存成 Array[(productId, score)] val streamRecs = computeProductScore( candidateProducts, userRecentlyRatings, simProcutsMatrixBC.value ) // 4. 把推荐列表保存到mongodb saveDataToMongoDB( userId, streamRecs ) &#125; &#125; // 启动streaming ssc.start() println("streaming started!") ssc.awaitTermination() &#125; /** * 从redis里获取最近num次评分 */ import scala.collection.JavaConversions._ def getUserRecentlyRatings(num: Int, userId: Int, jedis: Jedis): Array[(Int, Double)] = &#123; // 从redis中用户的评分队列里获取评分数据，list键名为uid:USERID，值格式是 PRODUCTID:SCORE jedis.lrange( "userId:" + userId.toString, 0, num ) .map&#123; item =&gt; val attr = item.split("\\:") ( attr(0).trim.toInt, attr(1).trim.toDouble ) &#125; .toArray &#125; // 获取当前商品的相似列表，并过滤掉用户已经评分过的，作为备选列表 def getTopSimProducts(num: Int, productId: Int, userId: Int, simProducts: scala.collection.Map[Int, scala.collection.immutable.Map[Int, Double]]) (implicit mongoConfig: MongoConfig): Array[Int] =&#123; // 从广播变量相似度矩阵中拿到当前商品的相似度列表 val allSimProducts = simProducts(productId).toArray // 获得用户已经评分过的商品，过滤掉，排序输出 val ratingCollection = ConnHelper.mongoClient( mongoConfig.db )( MONGODB_RATING_COLLECTION ) val ratingExist = ratingCollection.find( MongoDBObject("userId"-&gt;userId) ) .toArray .map&#123;item=&gt; // 只需要productId item.get("productId").toString.toInt &#125; // 从所有的相似商品中进行过滤 allSimProducts.filter( x =&gt; ! ratingExist.contains(x._1) ) .sortWith(_._2 &gt; _._2) .take(num) .map(x=&gt;x._1) &#125; // 计算每个备选商品的推荐得分 def computeProductScore(candidateProducts: Array[Int], userRecentlyRatings: Array[(Int, Double)], simProducts: scala.collection.Map[Int, scala.collection.immutable.Map[Int, Double]]) : Array[(Int, Double)] =&#123; // 定义一个长度可变数组ArrayBuffer，用于保存每一个备选商品的基础得分，(productId, score) val scores = scala.collection.mutable.ArrayBuffer[(Int, Double)]() // 定义两个map，用于保存每个商品的高分和低分的计数器，productId -&gt; count val increMap = scala.collection.mutable.HashMap[Int, Int]() val decreMap = scala.collection.mutable.HashMap[Int, Int]() // 遍历每个备选商品，计算和已评分商品的相似度 for( candidateProduct &lt;- candidateProducts; userRecentlyRating &lt;- userRecentlyRatings )&#123; // 从相似度矩阵中获取当前备选商品和当前已评分商品间的相似度 val simScore = getProductsSimScore( candidateProduct, userRecentlyRating._1, simProducts ) if( simScore &gt; 0.4 )&#123; // 按照公式进行加权计算，得到基础评分 scores += ( (candidateProduct, simScore * userRecentlyRating._2) ) if( userRecentlyRating._2 &gt; 3 )&#123; increMap(candidateProduct) = increMap.getOrDefault(candidateProduct, 0) + 1 &#125; else &#123; decreMap(candidateProduct) = decreMap.getOrDefault(candidateProduct, 0) + 1 &#125; &#125; &#125; // 根据公式计算所有的推荐优先级，首先以productId做groupby scores.groupBy(_._1).map&#123; case (productId, scoreList) =&gt; ( productId, scoreList.map(_._2).sum/scoreList.length + log(increMap.getOrDefault(productId, 1)) - log(decreMap.getOrDefault(productId, 1)) ) &#125; // 返回推荐列表，按照得分排序 .toArray .sortWith(_._2&gt;_._2) &#125; def getProductsSimScore(product1: Int, product2: Int, simProducts: scala.collection.Map[Int, scala.collection.immutable.Map[Int, Double]]): Double =&#123; simProducts.get(product1) match &#123; case Some(sims) =&gt; sims.get(product2) match &#123; case Some(score) =&gt; score case None =&gt; 0.0 &#125; case None =&gt; 0.0 &#125; &#125; // 自定义log函数，以N为底 def log(m: Int): Double = &#123; val N = 10 math.log(m)/math.log(N) &#125; // 写入mongodb def saveDataToMongoDB(userId: Int, streamRecs: Array[(Int, Double)])(implicit mongoConfig: MongoConfig): Unit =&#123; val streamRecsCollection = ConnHelper.mongoClient(mongoConfig.db)(STREAM_RECS) // 按照userId查询并更新 streamRecsCollection.findAndRemove( MongoDBObject( "userId" -&gt; userId ) ) streamRecsCollection.insert( MongoDBObject( "userId" -&gt; userId, "recs" -&gt; streamRecs.map(x=&gt;MongoDBObject("productId"-&gt;x._1, "score"-&gt;x._2)) ) ) &#125;&#125; 6 实时系统的联调KafkaStreaming模块的搭建 pom.xml 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-streams&lt;/artifactId&gt; &lt;version&gt;0.10.2.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.10.2.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;kafkastream&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.atguigu.kafkastream.Application&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 代码的实现 Application 12345678910111213141516171819202122232425262728293031323334353637383940package com.atguigu.kafkastream; import org.apache.kafka.streams.KafkaStreams;import org.apache.kafka.streams.StreamsConfig;import org.apache.kafka.streams.processor.TopologyBuilder;import java.util.Properties;public class Application &#123; public static void main(String[] args) &#123; String brokers = "linux:9092"; String zookeepers = "linux:2181"; // 定义输入和输出的topic String from = "log"; String to = "recommender"; // 定义kafka stream 配置参数 Properties settings = new Properties(); settings.put(StreamsConfig.APPLICATION_ID_CONFIG, "logFilter"); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, brokers); settings.put(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeepers); // 创建kafka stream 配置对象 StreamsConfig config = new StreamsConfig(settings); // 定义拓扑构建器 TopologyBuilder builder = new TopologyBuilder(); builder.addSource("SOURCE", from) .addProcessor("PROCESSOR", ()-&gt;new LogProcessor(), "SOURCE") .addSink("SINK", to, "PROCESSOR"); // 创建kafka stream KafkaStreams streams = new KafkaStreams( builder, config ); streams.start(); System.out.println("kafka stream started!"); &#125;&#125; LogProcessor 12345678910111213141516171819202122232425262728293031323334353637383940package com.atguigu.kafkastream;import org.apache.kafka.streams.processor.Processor;import org.apache.kafka.streams.processor.ProcessorContext;/** * @ClassName: LogProcessor * @Description: * @Author: wushengran on 2019/4/28 15:08 * @Version: 1.0 */public class LogProcessor implements Processor&lt;byte[], byte[]&gt;&#123; private ProcessorContext context; @Override public void init(ProcessorContext processorContext) &#123; this.context = processorContext; &#125; @Override public void process(byte[] dummy, byte[] line) &#123; // 核心处理流程 String input = new String(line); // 提取数据，以固定前缀过滤日志信息 if( input.contains("PRODUCT_RATING_PREFIX:") )&#123; System.out.println("product rating data coming! " + input); input = input.split("PRODUCT_RATING_PREFIX:")[1].trim(); context.forward("logProcessor".getBytes(), input.getBytes()); &#125; &#125; @Override public void punctuate(long l) &#123; &#125; @Override public void close() &#123; &#125;&#125; 6.2 配置启动Flumeflume的conf 目录下 123456789101112131415161718192021222324agent.sources = exectailagent.channels = memoryChannelagent.sinks = kafkasink# For each one of the sources, the type is definedagent.sources.exectail.type = exec# 下面这个路径是需要收集日志的绝对路径，改为自己的日志目录agent.sources.exectail.command = tail –f/mnt/d/Projects/BigData/ECommerceRecommenderSystem/businessServer/src/main/log/agent.logagent.sources.exectail.interceptors=i1agent.sources.exectail.interceptors.i1.type=regex_filter# 定义日志过滤前缀的正则agent.sources.exectail.interceptors.i1.regex=.+PRODUCT_RATING_PREFIX.+# The channel can be defined as follows.agent.sources.exectail.channels = memoryChannel# Each sink's type must be definedagent.sinks.kafkasink.type = org.apache.flume.sink.kafka.KafkaSinkagent.sinks.kafkasink.kafka.topic = logagent.sinks.kafkasink.kafka.bootstrap.servers = localhost:9092agent.sinks.kafkasink.kafka.producer.acks = 1agent.sinks.kafkasink.kafka.flumeBatchSize = 20#Specify the channel the sink should use 12 启动flume 1./bin/flume-ng agent -c ./conf/ -f ./conf/log-kafka.properties -n agent -Dflume.root.logger=INFO,console 启动zookeeper 1234bin/zkServer.sh start启动kafkabin/kafka-server-start.sh -daemon ./config/server.properties 测试实时推荐模块 连接的东西 先启动zookeeper 和kafka 在启动redis 然后redis的客户端 6 冷启动问题的解决实际项目中遇到的问题 冷启动的问题 算法是基于隐语义模型的 冷启动问题的处理 进来注册的时候 让你勾选项 了解即可 7 其他形式的离线相似推荐买了这个商品的他、用户 跟商品的相似 对应的相似的产品作出推荐 推荐算法分类的时候 用户画像 基于商品内容推荐 7.1 基于内容的相似度推荐id 名称 图片url 分类 ugc标签 主要基于UGC标签 创建项目 pom文件 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scalanlp&lt;/groupId&gt; &lt;artifactId&gt;jblas&lt;/artifactId&gt; &lt;version&gt;$&#123;jblas.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Spark的依赖引入 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-mllib_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 引入Scala --&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 加入MongoDB的驱动 --&gt; &lt;!-- 用于代码方式连接MongoDB --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;casbah-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;casbah.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 用于Spark和MongoDB的对接 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt; &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;mongodb-spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 实现代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108package com.atguigu.contentimport org.apache.spark.SparkConfimport org.apache.spark.ml.feature.&#123;HashingTF, IDF, Tokenizer&#125;import org.apache.spark.ml.linalg.SparseVectorimport org.apache.spark.sql.SparkSessionimport org.jblas.DoubleMatrixcase class Product( productId: Int, name: String, imageUrl: String, categories: String, tags: String )case class MongoConfig( uri: String, db: String )// 定义标准推荐对象case class Recommendation( productId: Int, score: Double )// 定义商品相似度列表case class ProductRecs( productId: Int, recs: Seq[Recommendation] )object ContentRecommender &#123; // 定义mongodb中存储的表名 val MONGODB_PRODUCT_COLLECTION = "Product" val CONTENT_PRODUCT_RECS = "ContentBasedProductRecs" def main(args: Array[String]): Unit = &#123; val config = Map( "spark.cores" -&gt; "local[*]", "mongo.uri" -&gt; "mongodb://linux:27017/recommender", "mongo.db" -&gt; "recommender" ) // 创建一个spark config val sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("ContentRecommender") // 创建spark session val spark = SparkSession.builder().config(sparkConf).getOrCreate() import spark.implicits._ implicit val mongoConfig = MongoConfig( config("mongo.uri"), config("mongo.db") ) // 载入数据，做预处理 val productTagsDF = spark.read .option("uri", mongoConfig.uri) .option("collection", MONGODB_PRODUCT_COLLECTION) .format("com.mongodb.spark.sql") .load() .as[Product] .map( x =&gt; ( x.productId, x.name, x.tags.map(c=&gt; if(c=='|') ' ' else c) ) ) .toDF("productId", "name", "tags") .cache() // TODO: 用TF-IDF提取商品特征向量 // 1. 实例化一个分词器，用来做分词，默认按照空格分 val tokenizer = new Tokenizer().setInputCol("tags").setOutputCol("words") // 用分词器做转换，得到增加一个新列words的DF val wordsDataDF = tokenizer.transform(productTagsDF) // 2. 定义一个HashingTF工具，计算频次 val hashingTF = new HashingTF().setInputCol("words").setOutputCol("rawFeatures").setNumFeatures(800) val featurizedDataDF = hashingTF.transform(wordsDataDF) // 3. 定义一个IDF工具，计算TF-IDF val idf = new IDF().setInputCol("rawFeatures").setOutputCol("features") // 训练一个idf模型 val idfModel = idf.fit(featurizedDataDF) // 得到增加新列features的DF val rescaledDataDF = idfModel.transform(featurizedDataDF) // 对数据进行转换，得到RDD形式的features val productFeatures = rescaledDataDF.map&#123; row =&gt; ( row.getAs[Int]("productId"), row.getAs[SparseVector]("features").toArray ) &#125; .rdd .map&#123; case (productId, features) =&gt; ( productId, new DoubleMatrix(features) ) &#125; // 两两配对商品，计算余弦相似度 val productRecs = productFeatures.cartesian(productFeatures) .filter&#123; case (a, b) =&gt; a._1 != b._1 &#125; // 计算余弦相似度 .map&#123; case (a, b) =&gt; val simScore = consinSim( a._2, b._2 ) ( a._1, ( b._1, simScore ) ) &#125; .filter(_._2._2 &gt; 0.4) .groupByKey() .map&#123; case (productId, recs) =&gt; ProductRecs( productId, recs.toList.sortWith(_._2&gt;_._2).map(x=&gt;Recommendation(x._1,x._2)) ) &#125; .toDF() productRecs.write .option("uri", mongoConfig.uri) .option("collection", CONTENT_PRODUCT_RECS) .mode("overwrite") .format("com.mongodb.spark.sql") .save() spark.stop() &#125; def consinSim(product1: DoubleMatrix, product2: DoubleMatrix): Double =&#123; product1.dot(product2)/ ( product1.norm2() * product2.norm2() ) &#125;&#125; 7.2 基于ItemCF的推荐算法 123456789101112131415161718192021222324252627282930&lt;dependencies&gt; &lt;!-- Spark的依赖引入 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 引入Scala --&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 加入MongoDB的驱动 --&gt; &lt;!-- 用于代码方式连接MongoDB --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;casbah-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;casbah.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 用于Spark和MongoDB的对接 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt; &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;mongodb-spark.version&#125;&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109package com.atguigu.itemcfimport org.apache.spark.SparkConfimport org.apache.spark.sql.SparkSessioncase class ProductRating( userId: Int, productId: Int, score: Double, timestamp: Int )case class MongoConfig( uri: String, db: String )// 定义标准推荐对象case class Recommendation( productId: Int, score: Double )// 定义商品相似度列表case class ProductRecs( productId: Int, recs: Seq[Recommendation] )object ItemCFRecommender &#123; // 定义常量和表名 val MONGODB_RATING_COLLECTION = "Rating" val ITEM_CF_PRODUCT_RECS = "ItemCFProductRecs" val MAX_RECOMMENDATION = 10 def main(args: Array[String]): Unit = &#123; val config = Map( "spark.cores" -&gt; "local[*]", "mongo.uri" -&gt; "mongodb://linux:27017/recommender", "mongo.db" -&gt; "recommender" ) // 创建一个spark config val sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("ItemCFRecommender") // 创建spark session val spark = SparkSession.builder().config(sparkConf).getOrCreate() import spark.implicits._ implicit val mongoConfig = MongoConfig( config("mongo.uri"), config("mongo.db") ) // 加载数据，转换成DF进行处理 val ratingDF = spark.read .option("uri", mongoConfig.uri) .option("collection", MONGODB_RATING_COLLECTION) .format("com.mongodb.spark.sql") .load() .as[ProductRating] .map( x =&gt; ( x.userId, x.productId, x.score ) ) .toDF("userId", "productId", "score") .cache() // TODO: 核心算法，计算同现相似度，得到商品的相似列表 // 统计每个商品的评分个数，按照productId来做group by val productRatingCountDF = ratingDF.groupBy("productId").count() // 在原有的评分表上rating添加count val ratingWithCountDF = ratingDF.join(productRatingCountDF, "productId") // 将评分按照用户id两两配对，统计两个商品被同一个用户评分过的次数 val joinedDF = ratingWithCountDF.join(ratingWithCountDF, "userId") .toDF("userId","product1","score1","count1","product2","score2","count2") .select("userId","product1","count1","product2","count2") // 创建一张临时表，用于写sql查询 joinedDF.createOrReplaceTempView("joined") // 按照product1,product2 做group by，统计userId的数量，就是对两个商品同时评分的人数 val cooccurrenceDF = spark.sql( """ |select product1 |, product2 |, count(userId) as cocount |, first(count1) as count1 |, first(count2) as count2 |from joined |group by product1, product2 """.stripMargin ).cache() // 提取需要的数据，包装成( productId1, (productId2, score) ) val simDF = cooccurrenceDF.map&#123; row =&gt; val coocSim = cooccurrenceSim( row.getAs[Long]("cocount"), row.getAs[Long]("count1"), row.getAs[Long]("count2") ) ( row.getInt(0), ( row.getInt(1), coocSim ) ) &#125; .rdd .groupByKey() .map&#123; case (productId, recs) =&gt; ProductRecs( productId, recs.toList .filter(x=&gt;x._1 != productId) .sortWith(_._2&gt;_._2) .take(MAX_RECOMMENDATION) .map(x=&gt;Recommendation(x._1,x._2)) ) &#125; .toDF() // 保存到mongodb simDF.write .option("uri", mongoConfig.uri) .option("collection", ITEM_CF_PRODUCT_RECS) .mode("overwrite") .format("com.mongodb.spark.sql") .save() spark.stop() &#125; // 按照公式计算同现相似度 def cooccurrenceSim(coCount: Long, count1: Long, count2: Long): Double =&#123; coCount / math.sqrt( count1 * count2 ) &#125;&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[java]]></title>
    <url>%2Fblog4%2F2019%2F10%2F08%2Fjava%2F</url>
    <content type="text"><![CDATA[1 各大厂的面试题1.1 蚂蚁花呗一个小时 1.2美团的一面 垃圾回收器 1.3 百度 java集合类 synchronized 什么是对象锁 什么是全局锁 1.4 头条 1.5 美团的面试汇总 1.6 蚂蚁金服二面 1.7 讲解1 关于2018.12 月份 ，。互联网公司大规模的缩招 裁员 缩招不是不招聘 而是招聘更多的优质的咖啡啊工程师 2 将最近半年的大厂面试题进行了整理和划分 1 1.8 3 1 个人 1.8倍的工资 干三个人的活 3 第一次 提出高频最多的常见笔试面试题目 ArrayList HashMap 底层是什么东西 4 JVM/GC 多线程与高并发 1.8 技术框架 大厂的面试题 90% 1.9 redis的相关题目 哪些数据存mysql 哪些 redis 如何保持移植性 redis缓存给了多大的总内存 命中率多高 超大Value打满网卡的问题 1.10 消息中间件MQrabibtMQ 消息中间件 消息积压了两个小时 消息中间件只有一个 挂掉 影响业务 1.11 JVM+GC的解析 oom 的东西 了解 Out of Memory Error java内存溢出 四大引用 强 软 弱 虚 水平 泯然众人与 性能检测工具 1.12 JUC多线程及高并发1.13 面试重点jvm + GC JUC多线程高并发 本次讲解 互联网笔试题第二季 JVM/GC的知识 JUC的前提只是 超级熟悉java8的与、新特性 （Stream+lambdaExpress+函数接口+方法引用） 2 JUC多线程及高并发 current 并发 高并发 秒杀 多个线程访问 统一个资源 并行 各种事情一路并行去做 节水 道调料 atomic 院原子性 AtomicInteger 原子引用 2.1 请你谈谈你对volatile的理解 同步 synchronized 轻量级 什么是轻 三大特性 volatile是轻量级的同步机制 不是文科 理解 jMM关于同步的 2.1.1 JMM值内存可见性 volatile 可见性1 理论线程 —-》》》 工作内存 —-》》 每个线程的私有内存区域 变量 —-》》 主内存 —》》 共享内存区域 线程可以访问 t1改成37 了 t2 t3不知道 1 t1 拷走25 2 t1改成37 3 把37 写回主内存 线程2 和线程3 不知道 只要有一个线程修改完自己的工作空间值之后写回主内存 及时通知其他的线程 这种机制 JMM之内存模型的第一个特性可见性 主内存的值只要被修改 其他的线程马上获得通知 改课 干活 —》》线程 自己的工作内存 私有数据 new 3 个线程 java内存模型规定所有的变量在主内训 主内存 拷贝到自己的工作内存空间 不同的线程无法访问对方的工作内存 线程间的通信（传值） 必须靠主内存来完成 拷贝共享变量的初始值 线程的工作内存里面 西城区和东城区的售票员 电话确认不可能 线程运算完 写回主内存 可见性 有了最新消息 第一个通知 论证 2 代码灭有volatile的操作 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/8 * @描述： */import java.util.concurrent.TimeUnit;class MyData&#123; int number=0; public void addTO60() &#123; this.number=60; &#125;&#125;/** 1 验证volatile的可见性* 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰** */public class VolatileDemo &#123; public static void main(String[] args) &#123; //main是一切方法的运行入口 MyData myData=new MyData();//资源类 new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+"\t come in"); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; myData.addTO60(); System.out.println(Thread.currentThread().getName()+ "\t updated number value:"+ myData.number); &#125;,"AAA").start(); //第二个线程就是我们的main线程 while (myData.number==0) &#123; //main线程一直在这里等待循环 直到这个number不在等于0 没有可见性 &#125; System.out.println(Thread.currentThread().getName()+"\t misson is over"); &#125;&#125; 最后一句没有打印 没有人通知 main线程傻傻的等待 没有人通知我改了 我不知道 加上volatile的代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/8 * @描述： */import java.util.concurrent.TimeUnit;class MyData&#123; volatile int number=0; public void addTO60() &#123; this.number=60; &#125;&#125;/** 1 验证volatile的可见性* 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰** */public class VolatileDemo &#123; public static void main(String[] args) &#123; //main是一切方法的运行入口 MyData myData=new MyData();//资源类 new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+"\t come in"); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; myData.addTO60(); System.out.println(Thread.currentThread().getName()+ "\t updated number value:"+ myData.number); &#125;,"AAA").start(); //第二个线程就是我们的main线程 while (myData.number==0) &#123; //main线程一直在这里等待循环 直到这个number不在等于0 没有可见性 &#125; System.out.println(Thread.currentThread().getName()+"\t misson is over"+"main get value:"+myData.number); &#125;&#125; 可见性证明 缓存 JMM的一种内存抽象机制 抽象的概念 再次阅读 轻量级 乞丐版的synchronized 2.1.2 JMM之原子性 volatile不保证原子性volatile是不保证原子性的 1 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/8 * @描述： */import java.util.concurrent.TimeUnit;class MyData&#123; volatile int number=0; public void addTO60() &#123; this.number=60; &#125; /* 此时number的前面是加了voatile关键字的修饰的，volatile不保证原子性 */public void addPlus()&#123; number++;&#125;&#125;/** 1 验证volatile的可见性* 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰** */public class VolatileDemo &#123;//main一切方法的入口 public static void main(String[] args) &#123; MyData myData=new MyData(); for (int i = 1; i &lt;=20 ; i++) &#123; new Thread(()-&gt;&#123; for (int j = 1; j &lt;=1000 ; j++) &#123; myData.addPlus(); &#125; &#125;,String.valueOf(i) ).start(); &#125; //等待上面的20个线程全部计算完成 再用main线程取得的最终的结果值是什么 //后台的线程 /*1 main 2 后台GC线程 * */ while(Thread.activeCount()&gt;2) &#123; Thread.yield(); &#125; System.out.println(Thread.currentThread().getName()+"\t finally n umber value: "+ myData.number); &#125; /*volatile可以保证可见性 可以通知其线程主物理内存的值已经被修改 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰 2 验证volatile不保证原子性 2.1 原子性指的是什么意思？ 不可分割 完整性 也即某个线程在做某个具体的业务的时候 中间不可以被加塞或者分割 需要整体完整 要么同时成功 要么同时失败 一段时间内只允许一个操作 2.2 volatile 是否可以保证原子性 */ public static void seeOkByVolatile() &#123; MyData myData=new MyData();//资源类 new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+"\t come in"); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; myData.addTO60(); System.out.println(Thread.currentThread().getName()+ "\t updated number value:"+ myData.number); &#125;,"AAA").start(); //第二个线程就是我们的main线程 while (myData.number==0) &#123; //main线程一直在这里等待循环 直到这个number不在等于0 没有可见性 &#125; System.out.println(Thread.currentThread().getName()+"\t misson is over"+"main get value:"+myData.number); &#125;&#125; 出来的不是2万 说明volatile不能够保证原子性 有没有可能加到2万 但是有极其特殊的情况是可能达到2万的 加了synchronized 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/8 * @描述： */import java.util.concurrent.TimeUnit;class MyData&#123; volatile int number=0; public void addTO60() &#123; this.number=60; &#125; /* 此时number的前面是加了voatile关键字的修饰的，volatile不保证原子性 */public synchronized void addPlus()&#123; number++;&#125;&#125;/** 1 验证volatile的可见性* 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰** */public class VolatileDemo &#123;//main一切方法的入口 public static void main(String[] args) &#123; MyData myData=new MyData(); for (int i = 1; i &lt;=20 ; i++) &#123; new Thread(()-&gt;&#123; for (int j = 1; j &lt;=1000 ; j++) &#123; myData.addPlus(); &#125; &#125;,String.valueOf(i) ).start(); &#125; //等待上面的20个线程全部计算完成 再用main线程取得的最终的结果值是什么 //后台的线程 /*1 main 2 后台GC线程 * */ while(Thread.activeCount()&gt;2) &#123; Thread.yield(); &#125; System.out.println(Thread.currentThread().getName()+"\t finally n umber value: "+ myData.number); &#125; /*volatile可以保证可见性 可以通知其线程主物理内存的值已经被修改 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰 2 验证volatile不保证原子性 2.1 原子性指的是什么意思？ 不可分割 完整性 也即某个线程在做某个具体的业务的时候 中间不可以被加塞或者分割 需要整体完整 要么同时成功 要么同时失败 一段时间内只允许一个操作 2.2 volatile 不保证原子性的案例的演示 */ public static void seeOkByVolatile() &#123; MyData myData=new MyData();//资源类 new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+"\t come in"); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; myData.addTO60(); System.out.println(Thread.currentThread().getName()+ "\t updated number value:"+ myData.number); &#125;,"AAA").start(); //第二个线程就是我们的main线程 while (myData.number==0) &#123; //main线程一直在这里等待循环 直到这个number不在等于0 没有可见性 &#125; System.out.println(Thread.currentThread().getName()+"\t misson is over"+"main get value:"+myData.number); &#125;&#125; 所以说volatile是 轻量级的同步机制 java c java verbose 12345678910public class T1 &#123;volatile int n =0;public void add()&#123; n++;&#125;&#125; 写覆盖 1 1 1 操作了3次 加1 各自的工作内存加1 由于synchronized 不能保证原子性 getfield iadd putfield 你先写 你先写 原子性 没有写完的时候 另外一个线程已经被唤醒 putfield 可能线程写入 丢失 后面的线程可能会把前面的线程写覆盖掉 JMM内存模型要求保证原子性 volatile不保证原子性 运行程序保证是2万 方法1 synchronized xxx 功能|+性能 原子包装的整型类 AtomicInteger的引入 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/8 * @描述： */import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicInteger;class MyData&#123; //MyData.java ===&gt; MyData.class =====&gt; jvm字节码 volatile int number=0; public void addTO60() &#123; this.number=60; &#125; /* 此时number的前面是加了voatile关键字的修饰的，volatile不保证原子性 */public void addPlus()&#123; number++;&#125;AtomicInteger atomicInteger=new AtomicInteger(); public void addAtomic() &#123;atomicInteger.getAndIncrement(); &#125;&#125;/** 1 验证volatile的可见性* 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰** */public class VolatileDemo &#123;//main一切方法的入口 public static void main(String[] args) &#123; MyData myData=new MyData(); for (int i = 1; i &lt;=20 ; i++) &#123; new Thread(()-&gt;&#123; for (int j = 1; j &lt;=1000 ; j++) &#123; myData.addPlus(); myData.addAtomic(); &#125; &#125;,String.valueOf(i) ).start(); &#125; //等待上面的20个线程全部计算完成 再用main线程取得的最终的结果值是什么 //后台的线程 /*1 main 2 后台GC线程 * */ while(Thread.activeCount()&gt;2) &#123; Thread.yield(); &#125; System.out.println(Thread.currentThread().getName()+"\t finally n umber value: "+ myData.number); System.out.println(Thread.currentThread().getName()+"\t AtomicInteger type finally n umber value: "+ myData.atomicInteger); &#125; /*volatile可以保证可见性 可以通知其线程主物理内存的值已经被修改 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰 2 验证volatile不保证原子性 2.1 原子性指的是什么意思？ 不可分割 完整性 也即某个线程在做某个具体的业务的时候 中间不可以被加塞或者分割 需要整体完整 要么同时成功 要么同时失败 一段时间内只允许一个操作 2.2 volatile 不保证原子性的案例的演示 */ public static void seeOkByVolatile() &#123; MyData myData=new MyData();//资源类 new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+"\t come in"); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; myData.addTO60(); System.out.println(Thread.currentThread().getName()+ "\t updated number value:"+ myData.number); &#125;,"AAA").start(); //第二个线程就是我们的main线程 while (myData.number==0) &#123; //main线程一直在这里等待循环 直到这个number不在等于0 没有可见性 &#125; System.out.println(Thread.currentThread().getName()+"\t misson is over"+"main get value:"+myData.number); &#125;&#125; 2.1.3 volatile指令重排 编译器 和处理器 常常做指令重排 源代码 编译器的优化的重排 指令并行的重排 内存系统的重排 最终执行的命令 数据依赖性 单线程 —-》》 多线程 源代码 12345678 底层 不一定是按照这个顺序 而是多线程环境 答题的顺序和 重排1 语句四存在数据的依赖性不能排到第一条 重排2 非计算机的了解即可 volatile禁止指令重排 线程的安全得到保证 a的前面加不加volatile a拿到的不是1 是 0 单线程环境不用担心指令重排 多线程 单线程编译器优化 编译器优化 指令并行重排的优化 内存系统的重排 3个重排 volatile可见性 Volatile的三大特性的讲解 JUC的包里面大规模使用到了单例模式 2.1.4 volatile的单例模式1单机版的单线程 12345678910111213141516171819202122232425262728293031323334353637383940package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/9 * @描述： */import javafx.scene.SnapshotParameters;/** 单机版的单线程* */public class SingleDemo &#123; private static SingleDemo instance=null; private SingleDemo()&#123; System.out.println(Thread.currentThread().getName()+"\t 我是构造方法Singledemo()" ); &#125; public static SingleDemo getInstance() &#123; if(instance==null) &#123; instance=new SingleDemo(); &#125; return instance; &#125; public static void main(String[] args) &#123; System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance()); System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance()); System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance()); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/9 * @描述： */import javafx.scene.SnapshotParameters;/** 单机版的单线程* */public class SingleDemo &#123; private static SingleDemo instance=null; private SingleDemo()&#123; System.out.println(Thread.currentThread().getName()+"\t 我是构造方法Singledemo()" ); &#125; public static SingleDemo getInstance() &#123; if(instance==null) &#123; instance=new SingleDemo(); &#125; return instance; &#125; public static void main(String[] args) &#123;// System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance());// System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance());// System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance());//// System.out.println();// System.out.println();// System.out.println(); for (int i = 0; i &lt;10 ; i++) &#123; new Thread(()-&gt; &#123; SingleDemo.getInstance(); &#125;,String.valueOf(i)).start(); &#125; &#125;&#125; 多线程 10个现在变成 6条 加上synchronized 解决问题 synchronized 整个代码都锁了 加上synchronized单例volatile的解析DCL（双端检测）介绍DCL的单例模式 高并发的环境下企业推崇的 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/9 * @描述： */import javafx.scene.SnapshotParameters;/** 单机版的单线程* */public class SingleDemo &#123; /* * 保证在多线程的指令不重排 * */ private static volatile SingleDemo instance=null; private SingleDemo()&#123; System.out.println(Thread.currentThread().getName()+"\t 我是构造方法Singledemo()" ); &#125; //DCL 模式 Double Check Lock 双端检索机制 //加锁前判断 //加锁判断 //在多线程的条件下 底层有指令重排 //如果没有控制好指令重排 public static SingleDemo getInstance() &#123; if (instance == null) &#123; synchronized (SingleDemo.class) &#123; if (instance == null) &#123; instance = new SingleDemo(); &#125; &#125; &#125; return instance; &#125; public static void main(String[] args) &#123;// System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance());// System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance());// System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance());//// System.out.println();// System.out.println();// System.out.println(); for (int i = 0; i &lt;10 ; i++) &#123; new Thread(()-&gt; &#123; SingleDemo.getInstance(); &#125;,String.valueOf(i)).start(); &#125; &#125;&#125; 先获得 再去加 i++ 2.2 CAS比较并交换 什么是比较和交换呢 期望值和物理内存的真实值一样 修改为更新值 期望值和物理内存的真实值不一样 重新获得真实值 CASDemo源码123456789101112131415161718192021222324252627282930package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/9 * @描述： *//** CAS 是什么？ ==》》 compare and set* 先比较后交换* 比较并交换** */import java.util.concurrent.atomic.AtomicInteger;public class CASDemo &#123; public static void main(String[] args) &#123; AtomicInteger atomicInteger=new AtomicInteger(5); //劳动成果写进主物理内存 System.out.println(atomicInteger.compareAndSet(5,2019 )+"\t current data:"+atomicInteger.get()); System.out.println(); System.out.println(atomicInteger.compareAndSet(5,1024 )+"\t current data:"+atomicInteger.get()); System.out.println(); &#125;&#125; 结果 compare and set 同 修改成功 不同 修改失败 为什么用synchronized 不用CAS CAS原理 Unsafe native 本地方法栈 CAS问题 ABA问题 CAS —-》》 Unsafe —–&gt;&gt; CAS的底层思想 —-》》 ABA —-》》 原子引用更新 如何规避ABA问题 CAS会导致ABA问题 取出内存中某个时刻的数据 当下时间比较并替换 那么在这个时间差异会导致数据的变化 one取出A two取出A two进行了一些操作把B改成了A 又把A放回去 如何解决ABA问题]]></content>
  </entry>
  <entry>
    <title><![CDATA[springcloud]]></title>
    <url>%2Fblog4%2F2019%2F10%2F08%2Fspringcloud%2F</url>
    <content type="text"><![CDATA[后台 前台 用户管理 订单的管理 购物车管理 tomcat 都在一个项目里面 容易出现单点故障问题 并发数低 代码的耦合度高 了解springBoot @ComponentScan SpringBootApplication webstorm的下载]]></content>
  </entry>
  <entry>
    <title><![CDATA[dubbo]]></title>
    <url>%2Fblog4%2F2019%2F10%2F08%2Fdubbo%2F</url>
    <content type="text"><![CDATA[1. dubbo的hello-world 消费者 服务的提供者 解决名称空间导入的问题 https://blog.csdn.net/feinifi/article/details/86677534]]></content>
  </entry>
</search>
