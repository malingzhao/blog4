<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[小总结]]></title>
    <url>%2Fblog4%2F2019%2F11%2F03%2F%E5%B0%8F%E6%80%BB%E7%BB%93-1%2F</url>
    <content type="text"><![CDATA[随机森林训练过程Bagging算法 对算法重采样AdaBoost算法训练过程中基分类器的分类错误率是该分类器的 分类误差的权重之和 XGBoost使用了损失函数的二阶导数信息传统的GBDT只用了一阶导数 Spark中 stage阶段的task数量由partition决定 spark的部署模式 脚本提交 获取main方法 提交到RM RM选择一个NM作为ApplicationMaster AM启动Driver 向RM申请资源 创建Executor对象向Driver注册 Driver执行任务 遇到一个宽依赖划分一个Stagecoalsce和repartition方法的区别coalsce一般减少分区数repartition一般是增大分区数repartition不一定会发生shufflecoalsce会根据传递的参数决定是否发生shufflerepartition的底层就是调用的coalsce方法 Spark的缓存机制cache和persist与checkpoint的区别和联系 都是RDD持久化的cache缓存在内存 不会截断血缘关系checkpoint在磁盘 会截断血缘关系 checkpoint的过程中会额外的提交一次任务 Spark的共享变量（广播变量和累加器）的基本原理与用途累加器（accumulator）是一种分布式的变量机制对执行的过程中的时间进行累加而广播变量是高效分发比较大的对象 累加器和广播变量 分别为聚合和广播两种不同的通信模式 当spark涉及到和数据库的连接的时候，如何减少spark运行中的数据库的链接次数使用foreachPartitions代替foreach，在foreachPartitions内部获取数据库的链接 RDD，DataSet，和DataFrame的区别 RDD： 优点： 面向对象 编译时安全 直接通过类点名的方式操作数据 缺点： 序列化和反序列化的开销 无论是集群间的通信还是IO操作 都需要对对象的结构和数据进行序列化和反序列化 GC的性能开销，频繁的创建和销毁对象势必会增加GC DataFrame ： 引入了schema和off-heap schema 对于DataSet的每一行数据结构都是一样的，这个结构就存在schema中 spark通过schema就能读懂数据，在集群间的通信和IO的操作的时候只需要序列化和反序列化 数据，结构的部分就可以省略了DataSet： 结合了RDD和DataFrame的优点 引入了Encoder 在序列化对象的时候 Encoder字节码与off-heap进行交互能够达到按照需要访问数据的效果 而不用序列化整个对象 spark在目前还没有提供自定义的Encoder的API但是未来会引入 spark操作left join和join的区别 调优之前和调优之后的性能的对比进行coalsce的操作本来的240个小文件变成了60个，这样再去进行shuffle 产生的小文件的数目大大大的减少了提高了join的性能 手写spark代码实现WordCount var conf=new SparkConf().setMaster(“[local[*]]”).setAppName(“WordCount”) val sc=new SparkContext(context) sc.textFile(“/input”).flatMap(.split(“ “)).map((,1)).reduceByKey(+).saveAsTextFile(“/output”) sc.stop() Spark Streaming窗口函数的原理在原来定义的sparkStreamning计算批次的基础上再次进行封装，每一次计算多个批次的数据同时需要设置一个滑动步长的参数，确定计算任务完成之后下一次从什么地方开始计算 Spark的集中提交方式LocalYarnMesosStandAloneHA高可用搭建历史服务器配置 用spark实现TopN的获取 按照key进行聚合 value转换成数组 用spark的SortWith或者SortBy进行排序 数据量太大造成OOM 项目经验之元数据备份 Tez将多个有依赖的作业转换成一次作业 这样只需要写一次HDFS union和union all的区别 union会将结果进行去重 union all 不会将结果进行去重 Hadoop宕机的解决办法： kafka缓冲提高kafka内存的大小 Flume的监控器： Ganglia Flume的调优 dataDirs 顺丰大数据平台开发二面 2019-08-16https://www.nowcoder.com/discuss/225326?type=post&amp;order=time&amp;pos=&amp;page=1 1.键值对这种数据结构的实现方式 2.多线程安全的键值对数据结构实现方式 3.归并排序 4.说说数据存储和处理的历史演变 5.hadoop实现数据冗余备份的难点 6.hive了解不 7.说说大数据的理解 9.两个都含有50亿url文件2G内存求交集（不用Bloom Filter） 这么多的小文件内存肯定是不够用的 假设两个文件为a和b ，对每个url进行hash取模的运算 hash(url)%100根据url的值将url存储到1000个小文件中记为a0,a1…….a9999对于文件b 同样进行hash(url)%100的操作，存储到对应的b0 b1b2…..b999的小文件中 统计1000对小文件中的相同的url采用hash_set对a0-b0 将其中的url存储到hash_set然后遍历b0 如果url在hash_set中说明此url在a和b同存在保存到文件中即可 =======================================================================网易有道大数据开发 2019-09-03https://www.nowcoder.com/discuss/239354?type=post&amp;order=time&amp;pos=&amp;page=1 第一面（技术，1小时20分） 1.自我介绍（3分钟） 2.介绍ETL开发项目（15分钟） 3.为什么数据存入hdfs还要再倒入NoSql，为什么hdfs对olap支持不友好。 4.spark任务执行全过程。 1. 编写的程序打成jar包 2. spark-submit 脚本提交到集群上运行 3. main方法映射到主类 开始执行代码 4. 初始化spakrContext对象 5. 创建连个对象DAGScheduler和TaskScheduler 将RDD切割成一个一个的stage，然后将stage作为taskSet 提交给DriverActor 6. Driver将这些任务交给Worker执行 5.mapReduce对比spark Hadoop MapReduce采用了多进程模型，而Spark采用了多线程模型：Apache Spark的高性能一定程度上取决于它采用的异步并发模型（这里指/driver 端采用的模型），这与Hadoop 2.0（包括YARN和MapReduce）是一致的，但在任务级别（特指 Spark任务和MapReduce任务）上却采用了不同的并行机制：Hadoop MapReduce采用了多进程模型，而Spark采用了多线程模型。 RDD的一个重要特征是，分布式数据集可以在不同的并行环境当中被重复使用， 6.linux操作命令（没答上来，表示没用到过） 7.统计学中p值的意义，中位数的意义，和均值对比，使用场景。 8.业务题：1）.网易云课堂突然订阅量下降，怎么定位分析原因。2）在各个平台投放简历，怎么评估效果。 9.提问 第二面（技术 45分） 1.自我介绍（3分钟） 2.介绍ETL开发项目（15分钟） 3.介绍数据仓库开发，数据迁移项目（10分钟） 4.介绍机器学习项目，怎么和大数据开发做的结合。（5分钟） 5.kafka结构，怎样防止脑裂，为什么最新版本不用zookeeper来维护offset。 6.手撕代码，找出数组中第二大的数。 7.提问 第三面（总监面 一小时+） 1.自我介绍（3分钟） 2.介绍ETL开发项目（15分钟） 3.手撕代码：知道二叉树前中序遍历，怎么得到后续遍历，限时5分钟。 4.业务题 1）开发统计某app用户各功能使用时长，整个流程怎么实现，怎么优化，怎么横向拓展。 2）统计评估各个网页的重要性，采取的算法思路，spark代码实现。（实际就是Google page rank算法） 5，未来规划，优缺点，offer情况，能否实习 6.提问 第四面（HR 一小时） 1）自我介绍。 2）实习做的事。 3）对实习公司的看法。 4）实习公司的技术整体架构。 5）前后端，数据中心算法团队的协同配合。 6）实习做的最有成就的项目。 7）遇到过的最有压力的事，怎么客服。 8）offer情况。 小米大数据开发一面 2019-09-04https://www.nowcoder.com/discuss/245146 整个过程大概有一个小时，涉及到的有Hbase集群，MongoDB集群，MySQL的存储引擎（主要是InnoDB存储引擎），分库分表，B+树，HashMap的实现方式。首先是自我介绍，介绍完之后开始问MySQL的InnoDB存储引擎，主要是索引的实现、以及数据的存储包括在B+树存储数据以及查找等；接着是关于Hbase集群性能的优化以及单个物理节点的优化；然后是MongoDB集群整个架构和部分实现细节(包括oplog等)；最后是Java语言的认识，主要是线程池以及线程安全的容器等，我主要介绍了HashMap的实现细节以及红黑树等。 拼多多学霸批大数据开发二面 2019-08-19https://www.nowcoder.com/discuss/227412介绍项目MapReduce过程spark了解吗数据结构呢？说一说二叉树有什么要问我的吗 拼多多学霸批 大数据岗位一面 2019-08-22https://www.nowcoder.com/discuss/230498自我介绍 1.spark广播变量的实现和原理。 2.大数据量，广播变量和通过形参传递的区别 3.spark reducebykey具体实现过程 4.stage划分 5.宽依赖、窄依赖区别 6.kafka中consumer和consumer group有什么区别，为什么会有consumer group的概念 7.kafka高性能的实现机制 8.redis用过哪些数据结构？zset底层实现 9.用那种语言 stackoverflow有哪些情况 递归 10.final关键字可以修饰哪些？final修饰基本类型、方法、类的作用 11.String是线程安全的？ 12.关系型数据库用过吗？索引的作用？索引的实现 13.数据库索引为什么用B+树而不用AVL、红黑树B+树 AVL红黑树都是二叉树 他们的深度更深 ，他们都是存储在内存中才会使用的数据结构而数据库是存储在磁盘的 操作系统读写磁盘的基本单位是扇区，而文件系统的基本单位是簇(Cluster)。 也就是说，磁盘读写有一个最少内容的限制，即使我们只需要这个簇上的一个字节的内容，我们也要含着泪把一整个簇上的内容读完。由于 B+ 树分支比二叉树更多，所以相同数量的内容，B+ 树的深度更浅，深度代表什么？代表磁盘 io 次数啊！数据库设计的时候 B+ 树有多少个分支都是按照磁盘一个簇上最多能放多少节点设计的啊！ 14.算法题 查找数组的中位数 O（n） 有赞一面 2019-08-24https://www.nowcoder.com/discuss/232245 应聘岗位：大数据开发工程师体验：还可以一面（应该是已经凉了）邮件说一小时，说了半小时就结束了，我太菜了 自我介绍结束面试官：本科的专业课我：数据结构，网络，数据库，操作系统 1、说说数据库的连接 2、连接的区别 3、还有什么连接 4、说说数据库的范式 ————————–0————————–5、对照着自己说的范式，举三个例子说明一下违反了三范式的实例 属性不可切割 不存在部分函数依赖 不存在传递函数依赖 6、数据库中删除的关键字 7、他们三个的区别 8、TCP\UDP是什么？区别？场景？对应协议？ 9、TCP的三次握手、四次挥手过程 10、在time-wait状态，客户端还接受数据吗？ 11、做过某个协议的应用吗？ 12、数据结构中排序算法学过的介绍一下？ 13、你是怎样区分他们是稳定和不稳定呢？ 14、实际场景中怎样选择某种算法呢？ 15、二叉树呢？红黑树（真的没看，说了B+树）？平衡二叉树？ 16、一个场景，100万个商家，ID号是顺序的，但是其中有两个ID丢失了，问怎样快速的找到丢失的ID？（我说了因为顺序，假设ID是从1开始，那么对应的下标为0，使用二分查找就可以了） 17、那么二分查找的时间复杂度？（logn，感觉面试官不满意） 18、给我介绍一下CB、CF算法？（正说的起劲着被打断） 19、问问Java吧？ 20、overload和override的区别？（重载三个要素我竟然说成名称、顺序、个数，幸亏面试官反问一下反应过来） 21、Java的多线程会吧？（会一点点，不敢说，说平常很少用） 有赞 现场 四面（加面技术面） 大数据 2019-08-26https://www.nowcoder.com/discuss/232897?type=post&amp;order=time&amp;pos=&amp;page=1 一面： 聊实习 介绍项目 hashmap concurrenthashmap jvm内存模型 java内存模型 聊聊java并发 hadoop mr的shuffle spark shuffle 数据库的acid 四个事务隔离级别Atomic 原子性所有操作执行成功才算最终的成功 Consistency 一致性Tom和Jack转账，两个人一共2000元无论怎么进行转账他们的钱都是2000元 Isolation 隔离性并发条件下不同的事务操作相同的数据的时候，每个事务都有各自的完整的数据空间 Durability 持久性事务成功结束 他对数据库做的更新会永远的保存下来及时发生系统崩溃 重新启动数据库系统后数据库还能够恢复到事务成功结束的时候的状态 每层隔离级别是怎么解决上一层的问题 spark sql的执行原理 从anltr4 到catalyst里面的逻辑优化 一直到物理计划的生成 此外还问了一个问题：spark 物理优化做了啥， 答啥都没做 spark2.3版本源码里 我只看到了对每个具体的逻辑算子生成物理执行计划的规则 缺没有类似关系型数据库中 基于代价计算来选取 多个物理执行计划中最优的策略， spark sql默认就是选第一个 而不是多个选最优的 你才实习了两个月 你咋做了这么多？ 然后就是第二面 二面： 自我介绍 聊实习 聊项目 yarn出现的背景 yarn源码看过吧 说下你的理解，我从rpc 事件库 状态机库 还有宏观的rm nm appmaster container聊了下spark比较熟悉是吧 找一个最熟的聊一聊 spark sql原理详细说一下 sql逻辑优化做了啥 sql逻辑优化 谓词下推的意义是啥 spark job提交时 和yarn交互的细节说一下， 这个打不出来 我只简单说了 起appmaster的过程 ，面试官问的是提交到yarn的taskscheduler backend的细节。。。这谁记得 被面试官嫌弃 你咋哪里都是蜻蜓点水 hadoop其他组件熟悉吗 哪个最熟 我来问问。。。 我也不知道哪个最熟 hive算不熟的吧 毕竟实习也不是做数仓的 要不zookeeper hbase hive kafka随便问吧 hive我最不熟 。。。面试官无语=_= 时间不够啊 你说一个最熟的 我来问 。。。。。。 要不你就问hive吧 这个我最不熟 聊天鬼才 。。。。。。。。。 hive是啥 有哪些数据倾斜问题大表和小表进行关联的时候 hive如何解决数据倾斜问题map join让小的维度表缓存在内存 在map端完成join的过程)、key分布不均匀 2)、业务数据本身的特性 3)、建表时考虑不周 4)、某些SQL语句本身就有数据倾斜 你实习才一个月咋做了这么多事？ 好了 去等hr面吧 我在等hr的时候 面试官。。。把他老大拉来了（我还以为是hr） 说来这边继续面 我越想越不对。。。hr不都是小姐姐 我们hrbp咋是威严大叔 三面： 聊人生 自我介绍 感觉没有具体的技术问题都是比较宽和大的东西 技术观和心得体会啥的我越面越感觉不对劲 这肯定不是hr 我估计被加面了 然后主动问下面试官：咱这面应该不是hr面吧 大叔答：不是 有时候情况特殊 会给同学加面 我答：这样啊 那很荣幸 你实习才一个月咋做了这么多事？四面：hr小姐姐聊人生 深信服测开二面 2019-08-17https://www.nowcoder.com/discuss/2260451.死怼项目 2.死怼简历（mmp，简历吹的有点过火，被怼惨了） 3.测试框架 4.hadoop？组件？原理？ 5.数据库？(没问基础，直接问用数据库做过什么) 6.做过数据库性能测试吗？说一下？7.数据清洗？（我特么当初为什么要写大数据） 8.Linux？经常在哪个系统下开发？ 9.你认为你在你们班的优势？？没有优势 10.有学习过其他的一些东西吗？说一下 网易严选大数据岗位内推一面 2019-08-22https://www.nowcoder.com/discuss/224968 【一面】上来问了一下项目 我项目是写一个基于hive的flink sink spark和flink对比 为什么spark streaming做不到毫秒级响应 kafka大致的架构 为什么高可用 高吞吐 zero copy原理 hashmap的结构 sql题 两个表 一个是订单表 一个是部门表 找到每个部门哪个用户加起来所有的订单金额最大 【二面】二面纯项目 连自我介绍都没有 也没有算法题或者sql题 我的项目是基于hive的电商数仓 所以问hive比较多 知道hive的sql语句怎么转成MR可执行任务的吗 MR的shuffle机制了解吗 数据仓库怎么分层 你的项目里分了哪些层 DWS层和ADS层有什么区别 有没有写过hive的UDF 怎么写的 流量漏斗分析 和页面来源分析知道吗 hive什么时候会产生数据倾斜 怎么处理 项目数据来源有哪些 有没有清洗数据 怎么清洗 有没有搭过大数据集群 用到哪些组件 每个组件的作用 计算框架了解吗 flink的checkpoint机制 字节跳动（提前批）二面 2019-07-24https://www.nowcoder.com/discuss/210332 一面： 1、shell统计 统计文档中含有指定字符的行数 2、大日志文档中提取指定时间范围的内容 3、数仓建模的流程 4、星型模型和雪花模型的区别 5、sql查询，找出互相关注的用户对数 6、大日志分区后的排序 7、手写堆排序 二面： 1、word2vec原理，skip-gram和CBOW的区别，应用场景 2、批次数据和实时数据的处理工具 3、Redis的数据类型及应用场景 4、Spark RDD的算子 5、宽窄依赖，sparksql的groupby会造成宽窄依赖吗 6、Hive和Spark的fromjson区别 7、LeetCode题：recorded list链表重排序 哈啰出行数据仓库开发工程师面经（60min） 2019-06-06https://www.nowcoder.com/discuss/196591 1、自我介绍 2、数据结构（很简单） 3、HashTable和HashMap的区别、ConcurrentHashMap 4、linux命令、ack 5、项目 6、hive、hadoop、kafka、hbase（hive问了很多基础知识；hadoop搭建方式：单机、伪分布式、完全分布式、hadoop的三大组件；hbase和kafka问的不多，因为不怎么会，哈哈） 7、多线程和高并发，比如：线程创建、锁、线程池、线程通信等等 8、大文件的数据读取和存取 9、jvm、mysql的索引、存储引擎、事务的隔离级别 10、实习意向、兴趣爱好、发展规划、学习方法、公司的了解（这个有点尴尬，只知道是做单车的）、有没有offer 浦发大数据 2019-08-28https://www.nowcoder.com/discuss/2322991 自我介绍 2 xgboost要调哪些参数，作用 3 数据不平衡问题怎么解决3 svm的KKT条件 4 rf和xgb哪个对异常点更敏感 5 python深浅拷贝 6 mysql引擎相关7 给sql题，说思路 8 规划，其他实习和offer情况，为何不选择互联网？机试： 1 偶数位的数字（num[1::2] 2 字符串中的数字和字符( [i for i in s if i.isnumeric()] 3 掷n个骰子，求所有可能总和的概率（动态规划，不过题目给的用例好像有点问题 浦发银行北京大数据岗 2019-08-28https://www.nowcoder.com/discuss/236535一、面试内容：1.自我介绍一下 2.介绍一下项目，怎么做的 如何建立特征工程，操作步骤有哪些，特征的重要性怎么判断 4.判断模型的性能有哪些指标 5.听说过WOE吗？特征分箱怎么操作？ 6.对hadoop spark了解吗 7.平常对数据处理都用什么工具，python?mysql？ 做一道算法题，口述解题思想： 123对于输入一个K位的数N，找出大于这个数、各位之和等于这个数、并最接近这个数的一个数字； 追问，K位超过long类型的大小怎么处理？ 实例：输入50，输出104；输入112，输出121.看到题第一时间蒙了，大神有好的思路麻烦传授一下。 9.一些个人问题 二、笔试题目： 1.判断字符串是否是回文 2.字符串交换：0和n位交换，2和n-1位交换…… 3.二进制数据的加法，不能转成十进制加法后再转回二进制 蘑菇街现场面 大数据开发 共三面 2019-08-26https://www.nowcoder.com/discuss/234155一轮技术面: 实习做了啥, 巴拉巴拉 一顿叨叨 顺便套了个近乎, 我实习的公司的数据部门老大 之前就是蘑菇街的平台这边的老大 , 面试官说你们老大之前就是我老板 中间提到我参与的一个实习公司的框架改进 面试官说: 嗯, 是我们老大的风格 hadoop spark的一些基本原理 spark streaming如何实现exactly once: kafka的receiver API 不开WAL不能实现, 面试官问为啥, 我说的: receiver会把数据拉取到内存中, 一个DStream里面有一部分RDD可以已经提交了, 但是这时候 出问题了, 全部都会重来一遍 可能会有重复计算, 并且kafka的高级消费者api他的offset存储在kafka中, 不是自己管理的 不能针对offset做一些 exactly once处理 说了下 使用kafka低级api的方式 面试官又问: 如何让spark streaming receiver 不开WAL来支持exactly once那? 面试官好坏, 我想了下 举了个 kafka写入时保证exactly once的例子, 幂等生产者 主要就是保证写出结果的幂等性, 我提的方案是在streaming任务的下游来进行保证, 写出结果时 带上每条记录的 key 假设是类似offset的标志性key, 然后下游来兜底 面试官说这个思路也还行吧 , 不过最简单的就是引入一个事务… jvm的基本原理 内存划分 如何确定是否可回收 gc算法 (又出了一个错, 谈到老年代的gc算法, 标记收集和标记整理 这两个词想不起来了, 短路了 只能说了 一个是标记可回收后 把可回收的删除 会有空间碎片, 一个是标记可回收后 把可回收的整理 需要停下来整理 但是没有碎片==! 就是想不起来名字了…. 哎 ) hashmap concurrenthashmap数据库的隔离级别 ==! 脑子短路 当时一激动忘了 不过还好 这是最后一个问题, 问的时候 二面面试官已经在门外了, 对整体结果没有大影响 二轮技术面: 面试官比较严肃, 没敢和一面一样笑嘻嘻的 lru的实现方式, 我说了 linkedHashMap的思想 使用hashmap上套一层hashmap 其他都是一些常规问题了 总体都是java 操作系统 数据库的基本问题 没再问 大数据相关的 三轮面试:又双叒叕见到hr小姐姐了 开心 聊人生 聊规划 hr小姐姐一直打哈欠 然后一直说抱歉 然后面试就结束了 等通知吧 贝壳找房 北京现场面 大数据开发 三面 2019-08-26https://www.nowcoder.com/discuss/233224一面 技术面 hadoop mapreduce原理 这个频率真高，回去准备一个能说10分钟的大纲 争取从mr的设计和流程 到源码 到yarn的细节, 再扯到spark 如何优化的 , mr的shuffle到spark shuffle hbase原理 region合并 region拆分和合并的细节不记得了 对hive的认识 感觉没问啥东西, 介绍下项目就进二面了 聊项目 二面 leader面 说下对大数据整体架构划分的认识 我答的 大数据平台类开发在底层 上面有数据仓库 数据挖掘 bi 等等的业务线，不是太准确 后面和面试官聊了下这个 聊的啥不记得了。。。 说下对自己的定位 偏业务还是偏平台 果断说偏平台 并且举出实习的例子。做的项目和兴趣爱好 后面好像就是聊人生。。。。提前在leader面聊人生 面试官应该很熟悉我这种先天下之忧而忧， 比较了解我这种 什么事情都早做打算 早做规划的性格 因为面试官一句话说出了一个重要特质： 每个转形（老想说换型，三换型羁绊下棋下疯了）的窗口期都会十分的焦虑 然后就是转型后飞速的成长 然后迎来下一个 抉择的点 聊的很开心 感觉面试官也是我这种性格的人 遥想一下 面试官可能看到了年轻时的自己 然后顺利通过 进入hr面 hr面： 特别可爱的hrbp小哥哥 长得像我心目中的圣诞老人， 继续聊人生， 反正不知道是我发挥好 还是面试官们引导好 感觉表现爆炸 不知道能不能offer 可能是面试官职业素质太好了 引导的笔记好如果贝壳可以offer 就要考虑不在杭州工作了 带着老婆来北京奋斗两年 海康威视 研究院 大数据开发 共三面 2019-08-28https://www.nowcoder.com/discuss/235928问了项目 实习 jvm内存模型 java Scala区别 hbase 你能想出什么优化的点吗？我举了个zookeeper不能很好横行拓展 不易于存储大量数据的例子 java内存模型 实现锁的几种方法 synchronized volatile原理 等等 时间太久了 记不清了 电话面大约一个小时 小哥说了他们部门学历卡的挺严的 去年招的都是985 机会难得 把握住 等消息吧 等现场面消息等了。。。三个星期 第二面现场面 谈实习 谈项目 说说hbase小合并说说hbase读取过程 catalyst calcite对比 说说为什么要小合并 java锁的类型为什么选择海康的大数据岗 海康 大数据开发 2019-09-05https://www.nowcoder.com/discuss/238501?type=post&amp;order=time&amp;pos=&amp;page=1 一面 1 jvm内存模型 2 垃圾回收 3 b树b+树红黑树 4 es写入过程 5 写题 6 项目问细节 二面 1 项目里遇到最大的问题 2 最有趣的项目 3 最崇拜的人 4 最沮丧的事 宜信大数据中心java研发岗 2018-09-09https://www.nowcoder.com/discuss/105875 一面（1 hour）: 1、项目介绍（十分钟） 2、项目里redis了解多少？ 3、持久化方式是什么？(aop,rdb) 4、aof重写是什么？ 5、怎样保持mysql与redis的数据一致性？（最终一致性） 6、写代码，回文数判断。（回文数以链表的形式存储，怎样盼判断他是回文数。） 7、ArrayList和LinkedList的区别？ 8、二叉树有哪几种方式遍历？ 9、然后，面试官给了一个二叉树，让你输出前序和中序的结果。 10、给定一棵二叉树，和一个数值。求二叉树的路径和等于给定值的所有路径？（二叉树中可能有负数，终止节点为中间节点不算） 11、Java 泛型（new List&lt;new LinkedList&gt; 编译器会不会报错之类的） 12、final的基本用法以及会产生的后果 一面基本上都很基础，面试官人很好，会细心指导你，并且帮助你完成编程。 二面: 二面就两道算法题 1、求一个数组中右边第一个比他大的数（单调栈）（不能暴力搜索） 2、有一个先升后降的序列，求最大值（二分搜索） 这个面试官人技术很厉害，面试的时候很耐心。第一题在我没思路的时候，也耐心给我讲解。 三面： 1、项目介绍 2、jvm了解吗？内存管理机制是什么？ 3、垃圾回收机制 4、g1和cms的区别 5、数据结构了解多少（说了java collections 类里面的东西） 6、hashmap在java里面怎么实现的？（说了1.7和1.8的区别，脑残说了下红黑树） 7、红黑树怎么实现的？有什么特点？什么时候左旋？什么时候右旋？ 8、mysql四大事务特性，每个特性都是什么意思 9、mysql优化策略 10、mysql怎么做到联合索引的（b+树） 11、联合索引的特性 12、分布式锁 网易大数据开发二面 2019-08-21https://www.nowcoder.com/discuss/229184?type=post&amp;order=time&amp;pos=&amp;page=1一上来就开始介绍最熟悉的项目，然后面试管问项目细节。后面问了几个基础的问题如下: HBase的使用场景(强调，rowkey查询快) HBase的组件 Kafka的partition和顺序消费，怎么读取历史数据 Spark和hadoop区别，spark常用算子，spark做数据处理，整个流程，用了哪些api 整个集群搭建怎么分布的 比较关注的是数据最终以什么形式展现 (楼主只说了提供了查询借接口，没有展现出来) 网易大数据 2018-09-26https://www.nowcoder.com/discuss/118336 1、自我介绍一下 2、介绍一下array、set、map的继承关系，画图 3、说一下ArrayList和linklist的底层实现原理，和数据结构 4、Hashmap和hashtable的区别 5、他们的线程安全问题 6、线程的实现方式有什么，写出来 7、讲一下synchronize和volatile锁的问题 8、说一下java的内存模型 9、Spring MVC运行流程 10、了解GC吗，讲一下原理 11、MySQL的引擎介绍 12、讲解一下scala闭包 13、你理解的spark、运行模式 14、HDFS的写入过程 15、节点通信问题 16、Hbase存储原理和过程 17、Zookeeper的同步过程 18、Hive的特点都有什么 网易游戏大数据 2018-05-07https://www.nowcoder.com/discuss/79822 1自我介绍+项目经历 2Java集合介绍ArrayList和LinkedList，hashmap和chm源码看过没，答看过，那么说一下。 3操作系统 Linux命令 sed和awk 还有 core dump用过没，jvm调试工具呢，答了一些jps jmap jstat的使用方法。 4讲一下Opentack，kubenetes（项目中涉及），稍微说了一下架构，然后比较细节的东西没答上。 5Hadoop的hdfs架构，hbase设计和优缺点，nosql对比关系数据库。 6设计一个集群监控系统需要哪些模块，怎么存机器的一些指标数据，用mysql，redis，hbase的比较，提了下raid。7Linux的iptable了解么，负载均衡的工具呢，七层Nginx，四层LVS没用过。 8有啥想问的吗。 感觉对hadoop生态还是很不熟啊，还需要充充电。 网易云音乐 大数据开发 2018-07-17https://www.nowcoder.com/discuss/35415 一面： 数据倾斜怎么处理？ 写一个算法判断一个图是不是DAG？（答案是拓扑排序，我一直在说bfs，尴尬，这边花了太多时间，O__O “…） flume用过是吧？flume数据源如何监听文件夹里有新文件产生？ 用过实时计算吗？没用过 mapreduce的map进程和reducer进程的jvm垃圾回收器怎么选择可以提高吞吐量？ 二面： 会Spark吗？不会（面试官很意外） 用过Hive是吧？写HiveQL：两张表：一张歌曲表，一张专辑表，找出每张专辑的Top100。（这边花了太多时间，写出来后有2处不对，group by和select的字段不一致，join语法不对(╯﹏╰)） HBase用过是吧？画一下它的架构图 知道HBase的LSM结构吗？乱说一通后，那我这边问完了，出去等通知吧！ HR面： 想留在杭州吗？父母对你留杭州同意吗？期待的薪资多少？还拿过哪些offer？ 网易杭研大数据 2018-08-29https://www.nowcoder.com/discuss/100227一面 问题比较基础包括java，计算机网络，数据结构，操作系统，都是比较常见的问题 1.equal hashcode区别 2.进程线程区别 3.TCP三次握手 4.经典DP 最长公共子串 5.暴力搜索题一道 二面 昨天也遇到玄学的二面，全程面试官不是很care我的项目，我在引导着他提问我，出来1个半小时后，出我意料的阵亡了 我总结下二面面试官主动问我的地方，或者比较感兴趣的点 1.hadoop shuffle 象征性的问了下 2.flink他十分感兴趣，可能内部用的是flink ，可是鄙人主攻spark这块。。 3.没了。。剩下的都是我在给他讲。。 网易 大数据 2018-09-04https://www.nowcoder.com/discuss/103119 一面 1、项目。 2、spark中的shuffle有哪些？ 3、mysql、spark中的join原理。 4、算法题：矩阵乘法。 5、HBase的优势，为什么使用了HBase、设计rowkey？ 6、spark中几种partitioner、水塘抽样？ 7、算法题：有一个数组， （1）其中有一个数出现次数超过半数，找到这个数？ （2）其中有两个数出现次数都超过了30%，找到这两个数？ 二面 1、项目。 2、一个application提交运行的过程，画图。 3、算法题。 4、还有一些基础概念。 三面hr+总监？ 1、项目、项目难点在哪？ 2、数据倾斜。 3、平时怎么学习一个新技术？ 4、三个词形容一下自己。（卡了好久气氛尴尬） 5、首选哪个城市？找工作有哪些考量？期望薪水？ 6、有没有对象？ 美图秀秀java大数据开发一面 2017-10-23https://www.nowcoder.com/discuss/57768 一面，估计挂。 1、开头首先闲聊，本人非计算机专业，问我通讯的为什么想搞计算机 2、介绍项目 3、spring 源码看过没？spring有什么优势？ spring MVC源码？ 处理请求过程？ IOC如何实现？ AOP呢？ 平时是怎么学习技术的的？看源码还是看博客？ 4、jvm相关：垃圾回收算法，有什么，优缺点； 分代算法：新生代对象什么时候会上升到老年代？ 有遇到什么jvm溢出错误？如何解决？ 如何处理堆溢出？ 对象从出生到死亡这个过程，历程是什么？ 有没有用过jvm相关的调优工具？ 还有些忘记了。 5、数据库mysql相关项目中有没有使用索引？ 索引什么时候使用适合？ 性别应该使用索引不？姓名呢？为什么？ 概况索引使用场景？ 行锁表锁相关？ 组合索引是什么？说说使用场景？ 表数据量很大的时候，如何优化？ 分表相关概念？ 6、海量数据处理自己设计海量数据处理机制，如何设计？ 海量url如何查重复？ 如何进行分堆？ 哈希相关思想？ 7、说说你了解的大数据技术？ 楼主搞java后台的，不了解大数据相关，所以这里没怎么聊。 8、其他个人问题 职业发展规划？想在哪工作？深圳or 厦门？ 9、什么问题想问我？ 十余家公司大数据开发 2019-05-05https://www.nowcoder.com/discuss/109518?type=post&amp;order=time&amp;pos=&amp;page=4 腾讯IEG（offer）一面（全程问基础）：1、介绍项目2、String、StringBuffer、StringBuilder的区别，怎么理解String不变性3、==和equals的区别，如果重写了equals()不重写hashCode()会发生什么4、volatile怎么保证可见性，synchronized和lock的区别，synchronized的底层实现5、sleep和wait的区别，sleep会不会释放锁，notify和notifyAll的区别6、了不了解线程的局部变量，讲讲线程池参数7、什么情况会发生死锁，死锁的处理方法8、Cookie和Session的区别，怎么防止Cookie欺骗9、从用户在浏览器输入域名，到浏览器显示出页面的过程二面（全程怼项目，压力面）：1、看你写过UDF，谈谈对UDF的理解，写UDF的目的，代码怎么写的2、改造hive表后怎么进行数据一致性校验的，有没有自动化流程3、看你读过kafka源码，讲讲kafka broker的源码里面你最熟悉的类，以及这个类的主要方法，用的什么设计模式4、项目里面从数据采集到最终的数据可视化，每个环节都有可能丢数据，怎么判断数据有没有丢，如果丢了如何定位到在哪一个环节丢的5、项目里面为什么要用kafka stream做实时计算，而不是用spark或者flink，kafka sql和spark sql了解过吗6、项目里面用到了时序数据库opentsdb，为什么要用这个，有没有跟其它的时序数据库对比过7、平时逛不逛社区，有没有参与过开源项目三面（接着怼项目）：1、看你写了实时计算的程序，你怎么保证计算的结果肯定是对的2、数据接入的时候，怎么往kafka topic里面发的，用的什么方式，起了几个线程，producer是线程安全的吗3、kafka集群有几台机器，怎么确定你们项目需要用几台机器，有评估过吗，吞吐量测过吗4、spark streaming是怎么跟kafka交互的，具体代码怎么写的，程序执行流程是怎样的，这个过程中怎么确保数据不丢5、kafka监控是怎么做的，kafka中能彻底删除数据吗，怎么做的面委会（全程聊天）：平时是怎么学习的，爱看哪些博客，怎么看待加班，有没有成为leader的潜力 网易考拉（offer）一面：1、sql题：学生成绩表，把每科最高分前三名统计出来2、算法题：二维数组中的查找3、kafka如何保证高吞吐的，了不了解kafka零拷贝，具体怎么做的4、sql有几种join，map join了解过没5、hbase中row key该怎么设计6、hdfs文件上传流程，hdfs的容错机制7、怎么解决hive数据倾斜问题二面（全程写写写）：1、算法题：二维矩阵相乘2、算法题：链表中环的入口3、写一下mysql binlog的数据格式，怎么进行数据清洗的4、写一个正则表达式进行手机号匹配5、讲一下数据仓库层级的划分，每层的作用 美团新到店（offer）去了北京美团公司里面试，一上午面完，第二天通知高分通过一面（简单的聊了聊，10min）：1、介绍项目，以及滴滴的实习经历2、JVM内存的划分3、垃圾收集算法4、数据建模，星型模型和雪花模型5、数仓层级的划分，怎么对接到mysql拿数据二面：1、sql题：写一条sql删除订单表中重复的记录2、sql题：一张网页浏览信息表，有两列，一列是网页ip，一列是浏览网页的用户（比如a或者b、c、d直到z），求这些网页被a和b或者a和c或者b和c两两组合访问的次数3、hive数据倾斜产生的原因，怎么解决4、设计学生成绩管理系统，符合第三范式要求，并绘出UML图5、算法题：斐波那契数列6、spark程序的运行流程7、spark streaming从kafka中读数据的两种方式8、讲讲数据库索引，B树和B+树9、Elasticsearch的索引，单field索引和多field的联合索引10、linux查看某文件的大小，vim中怎么替换内容11、海量数据的Count问题（单机），如果把大文件hash成不同的小文件，此时小文件装不下某个key对应的数据，该怎么办12、智力题：8升水，有一个5L的杯子和3L的杯子，怎么得到4升水三面：1、osi七层模型，三次握手和四次挥手，为什么两次握手不行2、kafka怎么保证高吞吐量，项目中有测过吞吐量吗，相比于其它MQ，为什么会选择kafka，kafka怎么保证exactly once语义3、了解hbase吗，hbase为什么查询速度快4、hive sql怎么转换成底层的MapReduce程序，以及shuffle的过程5、算法题：被围绕的区域，leetcode第130题原题6、智力题：一头母牛每年生一头小母牛，每头小母牛从第四年开始，每年也会生一头小母牛，写个公式求第n年会有多少头牛 小米（offer）一面：1、java和python的区别，对面向对象的理解，和面向过程相比有什么区别2、java为什么不能多继承3、讲一下java抽象类和接口4、java中为什么要写非static方法5、volatile和synchronized的区别6、算法题：跳台阶问题7、算法题：树的非递归后序遍历8、设计题：一个停车场有一些大车位和小车位，大车只能停大车位，小车既能停大车位又能停小车位，实现这种场景下的调度系统二面：1、算法题：输入一个字符串，输出该字符串中字符的所有排列 贝壳（offer）一面：1、synchronized的底层实现2、线程等待时位于哪个区域，具体讲一下3、谈谈对kafka的理解，能讲多少讲多少4、算法题：二分查找5、快排的时间复杂度和空间复杂度，最优情况和最差情况分别是多少，是稳定排序吗，快排为什么快二面：1、介绍项目，项目中涉及到了一些算法，介绍一下2、两道算法题：路径问题，leetcode上63题和64题原题3、写正则表达式匹配电话号码4、智力题：一张圆桌子，我和面试官轮流往桌子上放硬币（随便放），直到桌子放不下为止，最后一个放硬币的人赢，如果我先放，怎么保证我肯定赢 华为（offer）一轮玄学面：面试官是做安卓的，瞧不起大数据，觉得大数据很虚，我跟他bb了一堆。然后问我有没有女朋友，我说以前有，现在分了；问我什么时候谈的，什么时候分的，我说本科谈的，毕业分了；问我为什么要分，此处省略一万字……问我现在想没想过再谈，我说毕竟转专业过来的，想趁在校期间利用好短暂的时光提升自己的技术水平（其实因为找不到）；然后面试官说以后工作了就不好找咯，我说您说的有道理………… 快手（offer）一面：1、jvm类加载机制，类加载器，双亲委派模型2、java实现多线程的方式3、spark怎么划分stage，宽窄依赖，各包括哪些***作4、zookeeper怎么保证原子性，怎么实现分布式锁5、写个快排，为什么要用三数取中法，好处是什么二面：1、sql题：找出单科成绩高于该科平均成绩的同学名单（无论该学生有多少科，只要有一科满足即可）2、sql题：找出单科成绩高于该科平均成绩的同学名单（该学生所有科都必须满足）3、算法题：求数组中连续子数组的最大和4、算法题：使用最小花费爬楼梯，leetcode746题原题三面：1、讲一下java IO2、算法题：输入n个整数，找出其中最大的k个数3、算法题：给一个整数数组和一个目标值，找出数组中和为目标值的两个数 阿里菜鸟（三面完已回绝）阿里的面试还是比较重视基础的，应该是bat里面问基础问的最多的一面：1、HashMap和HashTable的区别，HashMap怎么解决hash冲突，jdk1.8后对HashMap的改进2、讲讲ConcurrentHashMap，ConcurrentHashMap怎么保证线程安全，HashTable怎么保证线程安全3、HashSet的底层实现，是不是线程安全的4、ArrayList和LinkedList的区别，是不是线程安全的5、讲讲设计模式，最常用哪种设计模式，单例模式的实现方式6、进程和线程，Java实现多线程的方式，什么是线程安全，怎么保证多线程线程安全7、可重入锁的可重入性是什么意思，哪些是可重入锁8、为什么要用线程池，线程池的好处9、JVM垃圾处理方法，对象什么时候进入老年代，什么时候进行FullGC10、Java堆溢出问题怎么处理，内存泄漏和内存溢出的区别11、智力题：50个红球和50个黑球往两个桶里放，然后自己去抽，怎么样才能使抽到红球的概率最高二面：1、讲讲数据库存储引擎2、介绍一下索引，索引设置的规则，聚簇索引和非聚簇索引的区别，索引的最左前缀原则3、用过redis吗，redis支持哪些数据类型，redis与mysql的区别4、了解垃圾收集器吗，分别介绍介绍5、jvm调优做过没，-Xms和-Xmx分别指什么6、算法题：输入两个字符串，输出它们合并排序后的结果三面：1、讲讲数据库的范式2、Linux进程通信和线程通信3、线程池的参数4、什么是内部类，什么是匿名内部类5、设计题：一个市有9个消防站，现在要新增3个消防站，这3个消防站应该放在哪里 spark shufflemap端的Shuffle简述: 1)input, 根据split输入数据，运行map任务; 2)patition, 每个map task都有一个内存缓冲区，存储着map的输出结果; 3)spill, 当缓冲区快满的时候需要将缓冲区的数据以临时文件的方式存放到磁盘; 4)merge, 当整个map task结束后再对磁盘中这个map task产生的所有临时文件做合并，生成最终的正式输出文件，然后等待reduce task来拉数据。 reduce 端的Shuffle简述: reduce task在执行之前的工作就是不断地拉取当前job里每个map task的最终结果，然后对从不同地方拉取过来的数据不断地做merge，也最终形成一个文件作为reduce task的输入文件。 1) Copy过程，拉取数据。 2)Merge阶段，合并拉取来的小文件 3)Reducer计算 4)Output输出计算结果 Xms Xmx 启动的内存 最大的内存 无论是idea，还是tomcat等都有这个东西]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2Fblog4%2F2019%2F11%2F03%2Fhive%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Education]]></title>
    <url>%2Fblog4%2F2019%2F11%2F02%2FEducation%2F</url>
    <content type="text"><![CDATA[1 数据分层1.1 为什么要分层1.2 数仓的命名规范2 环境准备 3 用户注册模块的需求 4 用户做题模块需求 5 总结和调优 6 DataX]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>项目实战</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql]]></title>
    <url>%2Fblog4%2F2019%2F10%2F31%2Fsql%2F</url>
    <content type="text"><![CDATA[上次面试，感觉自己败在了sql上面 因为平常没有怎么在意，没有深入的学习 决定从今天开始，深入的学习sql sql总结1 触发器的作用 发触发器是一中特殊的存储过程，主要是通过事件来触发而被执行的。它可以强化约束，来维护数据的完整性和一致性，可以跟踪数据库内的操作从而不允许未经许可的更新和变化。可以联级运算。如，某表上的触发器上包含对另一个表的数据操作，而该操作又会导致该表触发器被触发。 2 什么是存储过程？ 用什么来调用存储过程是一个预编译的sql语句，优点是允许模块化的设计，就是说只创建一次，在以后的存储过程可以调用多次。如果某次操作需要多次执行sql，使用存储过程比单纯sql语要快，可以使用一个命令对象来多次调用存储过程。 3 什么是内存泄漏我们说的内存泄漏一般是指内存的泄漏，堆内存是从对中为其分配的，大小是任意的，使用完成之后要显示释放内存，当应用程序使用关键字new等创建对象的时候，就从堆中为它分配一块内存，使用完成后调用free或delete释放内存，否则就说该内存不能被我们使用，也就是说内存泄漏了。 4 索引的作用，它的优点和缺点是什么 索引就一种特殊的查询表，数据库的搜索引擎可以利用它加速对数据的检索。它很类似与现实生活中书的目录，不需要查询整本书内容就可以找到想要的数据。索引可以是唯一的，创建索引允许指定单个列或者是多个列。缺点是它减慢了数据录入的速度，同时也增加了数据库的尺寸大小。 5 什么是事务 什么是锁锁： 事务就是被绑定在一起作为一个逻辑工作单元的SQL语句分组，如果任何一个语句操作失败那么整个操作就失败，以后操作就会回滚到操作前的状态，或者是上有个节点，那么为了确保要么执行，要么不执行，就可以使用事务 ，要将有组语句作为事务考虑，就需要通过ACID测试，即原子性，一致性，隔离性和持久性， 锁：在所有的DBMS中，锁是实现事务的关键，锁可以保证事务的完整性和并发性。与现实生活中的锁一样，它可以使用某些数据的拥有者，在某段时间内不能使用某些数据或者数据结构，当然锁时分级别的。 6 什么叫视图，游标是什么视图是一种虚拟的表，具有和物理表相同的功能，可以对视图进行增删改操作，视图通常是有多个表的行或列的子集，对是的修改不影响基本表，它可以使我们获取数据更容易，相对于多表查询 游标是对查询出来的结果集作为一个单元的有效的处理。 游标可以定在该单元的特定的行，从结果集的当前行检索一行或者多行 ，可以对结果集当前行做修改。一般使用游标，但是需要逐条处理数据的时候，游标显的非常的重要 7 sql的链接？right join left join inner join outer join cross join full join HIveSQL试题热身 12345select store ,count(uid)from visit group by store 解决问题的方法 select from where 先写框架 添加东西 12345678selectstage_someone,count(distinct uid)from LifeStage lateral view explode(split(stage,',')) LifeStage_tempas stage_someone group by stage_someone 这一道题考察了行转列我们需要了解sql语句的执行顺序 先from 后 where 最后groupby 随后是hvaing lateral view 水平视图 select UID,concat_ws(‘,’,collect_set(stage)) as stages from LifeStage group by UID; 考点：行转列 explode与lateral view在关系型数据库中本身是不该出现的，因为他的出现本身就是在操作不满足第一范式的数据（每个属性都不可再分），本身已经违背了数据库的设计原理（不论是业务系统还是数据仓库系统），不过大数据技术普及后，很多类似pv，uv的数据，在业务系统中是存贮在非关系型数据库中，用json存储的概率比较大，直接导入hive为基础的数仓系统中，就需要经过ETL过程解析这类数据，explode与lateral view在这种场景下大显身手。 explode作用是处理map结构的字段，使用案例如下（hive自带map，struct，array字段类型，但是需要先定义好泛型，所以在此案例不使用）：建表语句：drop table explode_lateral_view;create table explode_lateral_view(area string,goods_id string,sale_info string)ROW FORMAT DELIMITEDFIELDS TERMINATED BY ‘|’STORED AS textfile;.0 数据倾斜问题 数据倾斜是进行大数据计算时最经常遇到的问题之一。当我们在执行HiveQL或者运行MapReduce作业时候，如果遇到一直卡在map100%,reduce99%一般就是遇到了数据倾斜的问题。数据倾斜其实是进行分布式计算的时候，某些节点的计算能力比较强或者需要计算的数据比较少，早早执行完了，某些节点计算的能力较差或者由于此节点需要计算的数据比较多，导致出现其他节点的reduce阶段任务执行完成，但是这种节点的数据处理任务还没有执行完成。 在hive中产生数据倾斜的原因和解决方法： 1）group by,我使用Hive对数据做一些类型统计的时候遇到过某种类型的数据量特别多，而其他类型数据的数据量特别少。当按照类型进行group by的时候，会将相同的group by字段的reduce任务需要的数据拉取到同一个节点进行聚合，而当其中每一组的数据量过大时，会出现其他组的计算已经完成而这里还没计算完成，其他节点的一直等待这个节点的任务执行完成，所以会看到一直map 100% reduce 99%的情况。 解决方法：set hive.map.aggr=true set hive.groupby.skewindata=true 原理：hive.map.aggr=true 这个配置项代表是否在map端进行聚合 hive.groupby.skwindata=true 当选项设定为 true，生成的查询计划会有两个 MR Job。第一个 MR Job 中，Map 的输出结果集合会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的；第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作。 2）map和reduce优化。 1.当出现小文件过多，需要合并小文件。可以通过set hive.merge.mapfiles=true来解决。 2.单个文件大小稍稍大于配置的block块的大写，此时需要适当增加map的个数。解决方法：set mapred.map.tasks个数 3.文件大小适中，但map端计算量非常大，如select id,count(*),sum(case when…),sum(case when…)…需要增加map个数。解决方法：set mapred.map.tasks个数，set mapred.reduce.tasks个数 3）当HiveQL中包含count（distinct）时 ​ 如果数据量非常大，执行如select a,count(distinct b) from t group by a;类型的SQL时，会出现数据倾斜的问题。 ​ 解决方法：使用sum…group by代替。如select a,sum(1) from (select a, b from t group by a,b) group by a; 4）当遇到一个大表和一个小表进行join操作时。 解决方法：使用mapjoin 将小表加载到内存中。 如：select /*+ MAPJOIN(a) */ a.c1, b.c1 ,b.c2 from a join b where a.c1 = b.c1; 5）遇到需要进行join的但是关联字段有数据为空，如表一的id需要和表二的id进行关联 解决方法1：id为空的不参与关联 比如：select * from log a join users b on a.id is not null and a.id = b.id union all select * from log a where a.id is null; 解决方法2：给空值分配随机的key值 如：select * from log a left outer join users b on case when a.user_id is null then concat(‘hive’,rand() ) else a.user_id end = b.user_id; Hive之转列列转行vim person.txt 12345孙悟空 白羊座 A大海 射手座 A宋宋 白羊座 B猪八戒 白羊座 A凤姐 射手座 A 1234567891011121314151617181920212223242526create table person_info(name string, constellation string, blood_type string)row format delimited fields terminated by "\t";load data local inpath '/opt/module/data/person_info.txt' into table person_info;selectconcat(constellation,',',blood_type) as constellation_blood_type,concat_ws('|',collect_set(name)) as namesfrom person_infogroup by concat(constellation,',',blood_type)函数说明：concat(string A/col, string B/col…):返回输入字符串连接后的结果，支持任意个输入字符串;concat_ws(separator, str1, str2,...):它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符,分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间;collect_set(col):函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段。]]></content>
  </entry>
  <entry>
    <title><![CDATA[oracle]]></title>
    <url>%2Fblog4%2F2019%2F10%2F30%2Foracle%2F</url>
    <content type="text"><![CDATA[1 Oracle的学习四天 学习步骤 第一天: Oracle的概念和安装 基本查询 条件查询 Oracle中的函数 第二天： 夺标查询 子查询 第三天： 表空间的概念 用户 视图 索引 序列 同义词 PL\SQL编程 第四天： PLSQL编程 游标 存储过程 存储函数 触发器 Oracle的OCA认证 1Oracle的介绍ORACLE数据库系统是美国ORACLE公司（甲骨文）提供的以分布式数据库为核心的一组软件产品，是目前最流行的客户/服务器(CLIENT/SERVER)或B/S体系结构的数据库之一。比如SilverStream就是基于数据库的一种中间件。ORACLE数据库是目前世界上使用最为广泛的数据库管理系统，作为一个通用的数据库系统，它具有完整的数据管理功能；作为一个关系数据库，它是一个完备关系的产品；作为分布式数据库它实现了分布式处理功能。但它的所有知识，只要在一种机型上学习了ORACLE知识，便能在各种类型的机器上使用它。 2 Oracle 的安装自行百度 教程可用 3 Oracle的体系架构 1 数据库Oracle数据库是数据的物理存储。这就包括（数据文件ORA或者DBF、控制文件、联机日志、参数文件）。其实Oracle数据库的概念和其它数据库不一样，这里的数据库是一个操作系统只有一个库。可以看作是Oracle就只有一个大数据库。 2 实例一个Oracle实例（Oracle Instance）有一系列的后台进程（Backguound Processes)和内存结构（Memory Structures)组成。一个数据库可以有n个实例。 3 用户用户是在实例下建立的。不同实例可以建相同名字的用户。 4 表空间表空间是Oracle对物理数据库上相关数据文件（ORA或者DBF文件）的逻辑映射。一个数据库在逻辑上被划分成一到若干个表空间，每个表空间包含了在逻辑上相关联的一组结构。每个数据库至少有一个表空间(称之为system表空间)。 每个表空间由同一磁盘上的一个或多个文件组成，这些文件叫数据文件(datafile)。一个数据文件只能属于一个表空间。 5 数据文件（dbf,ora）数据文件是数据库的物理存储单位。数据库的数据是存储在表空间中的，真正是在某一个或者多个数据文件中。而一个表空间可以由一个或多个数据文件组成，一个数据文件只能属于一个表空间。一旦数据文件被加入到某个表空间后，就不能删除这个文件，如果要删除某个数据文件，只能删除其所属于的表空间才行。注： 表的数据，是有用户放入某一个表空间的，而这个表空间会随机把这些表数据放到一个或者多个数据文件中。由于oracle的数据库不是普通的概念，oracle是有用户和表空间对数据进行管理和存放的。但是表不是有表空间去查询的，而是由用户去查的。因为不同用户可以在同一个表空间建立同一个表空间建立同一个名字的表 4 创建表空间5 Oracle的数据类型Varchar varchar2（常用的字符类型） char()长度是固定的 NUMBER(m) NUMBER(m,n) m总长度 n小数点之后的位数 DATE 日期类型 CLOB 读书馆的所有的书 文本类型 BLOB 视频 6 表的创建7 修改表结构8 创建序列9 scott用户介绍新手接触的用户]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[suanfa]]></title>
    <url>%2Fblog4%2F2019%2F10%2F30%2Fsuanfa%2F</url>
    <content type="text"><![CDATA[1 10大排序算法1 .1 冒泡排序对于冒泡排序而言，它的思想就是不断的交换两个元素的位置，然后两两进行比较。 冒泡排序是一种简单的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.mlz.suanfa.compare;/* * @创建人: MaLingZhao * @创建时间: 2019/10/30 * @描述： */import java.util.Arrays;public class BubbleSort &#123; public static void main(String[] args) &#123; int[] arr=new int[] &#123;5,7,2,9,4,1,0,5,7&#125;; System.out.println("冒泡排序之前的数据:"); System.out.println(Arrays.toString(arr)); bubbleSort(arr); System.out.println("冒泡排序之后的数据"); System.out.println(Arrays.toString(arr)); &#125; private static void bubbleSort(int[] arr) &#123; //控制共比较多少轮 for(int i=0;i&lt;arr.length-1;i++) &#123; //控制比较的次数 和后面的元素进行比较 for(int j=0;j&lt;arr.length-1-i;j++) &#123; if(arr[j]&gt;arr[j+1]) &#123; int temp=arr[j]; arr[j]=arr[j+1]; arr[j+1]=temp; &#125; &#125; &#125; &#125;&#125; 1.2 选择排序表现最稳定的排序算法之一，因为无论什么数据进去都是O(n2)的时间复杂度，所以用到它的时候，数据规模越小越好。唯一的好处可能就是不占用额外的内存空间了吧。理论上讲，选择排序可能也是平时排序一般人想到的最多的排序方法了吧。 选择排序(Selection-sort)是一种简单直观的排序算法。它的工作原理：首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。 代码示例: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package com.mlz.suanfa.compare;/* * @创建人: MaLingZhao * @创建时间: 2019/10/30 * @描述： */import java.util.Arrays;public class SelectSort &#123; public static void main(String[] args) &#123; int[] arr = new int[] &#123;3,4,5,7,1,2,0,3,6,8&#125;; System.out.println("选择排序之前的数据："); System.out.println(Arrays.toString(arr)); selectSort(arr); System.out.println("选择排序之后的数据:"); System.out.println(Arrays.toString(arr)); &#125;/** 选择排序* 核心思想：* 1 遍历所有的数* 2 选取第i个数作为最小的索引 然后依次与后面的数进行比较 选出最小的索引* 3. 如果选出的最小数的下标和最小的下标的那个数不一样的话 就进行交换** */ private static void selectSort(int[] arr) &#123; //遍历所有的数字 for (int i = 0; i &lt;arr.length ; i++) &#123; //纪录最小元素的索引 int minIndex=i; //把当前遍历的数和后面的数依次进行比较 for(int j=i+1;j&lt;arr.length;j++)&#123; //如果后面的数比记录的数小的话 if(arr[minIndex]&gt;arr[j]) &#123; //记录最小的数的那个下标 minIndex=j; &#125; &#125; //如果当前的最小的数和遍历的下标不一致的话 if(i!=minIndex) &#123; int temp=arr[i]; arr[i]=arr[minIndex]; arr[minIndex]=temp; &#125; &#125; &#125;&#125; 1.3 插入排序 插入排序（Insertion-Sort）的算法描述是一种简单直观的排序算法。它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。插入排序在实现上，通常采用in-place排序（即只需用到O(1)的额外空间的排序），因而在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。 代码示例: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package com.mlz.suanfa.compare;/* * @创建人: MaLingZhao * @创建时间: 2019/10/30 * @描述： */import java.util.Arrays;public class InsertSort &#123; public static void main(String[] args) &#123; int[] arr = new int[] &#123;5,3,2,8,5,9,1,0&#125;; System.out.println("选择排序之前的顺序："); insertSort(arr); System.out.println("选择排序之后的顺序"); System.out.println(Arrays.toString(arr)); &#125; /* * 插入排序 * 1、 遍历所有的数字 * 2、 如果后一个数字比前一个数字小的话 我们就把这个数字保存起来 * 3、 把当前的保存数字依次和前面的数字进行比较直到找到这个元素合适的位置 * 4、把保存的变量的值赋给这个元素的合适的位置 * */ private static void insertSort(int[] arr) &#123; //遍历所有的数字 for (int i = 0; i &lt;arr.length ; i++) &#123; //如果当前的数字比前一个数字小的 if(arr[i]&lt;arr[i-1]) &#123; //把当前的数字保存起来 int temp=arr[i]; int j; //遍历当前数字前面的所有的数字 for( j=i-1; j&gt;=0&amp;&amp;temp&lt;arr[j];j--) &#123; //把前面的一个数字的值赋给后一个数字 也就是说数字按照次序向后移动 给小的那个数留出位置 arr[j+1]=arr[j]; &#125; //不满足上述条件的话就按照次序向后移动，现在满足条件了 那么我们就将temp的变量的值赋给arr[j+1] arr[j+1]=temp; &#125; &#125; &#125;&#125; 1.4 希尔排序希尔排序是希尔（Donald Shell）于1959年提出的一种排序算法。希尔排序也是一种插入排序，它是简单插入排序经过改进之后的一个更高效的版本，也称为缩小增量排序，同时该算法是冲破O(n2）的第一批算法之一。它与插入排序的不同之处在于，它会优先比较距离较远的元素。希尔排序又叫缩小增量排序。 希尔排序是把记录按下表的一定增量分组，对每组使用直接插入排序算法排序；随着增量逐渐减少，每组包含的关键词越来越多，当增量减至1时，整个文件恰被分成一组，算法便终止。 代码示例 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package com.mlz.suanfa.compare;/* * @创建人: MaLingZhao * @创建时间: 2019/10/30 * @描述： */import java.util.Arrays;public class ShellSort &#123; public static void main(String[] args) &#123; int[] arr = new int[]&#123;3, 5, 2, 7, 8, 1, 2, 0, 4, 7, 4, 3, 8&#125;; System.out.println("希尔排序之前的数据:"); System.out.println(Arrays.toString(arr)); shellSort(arr); System.out.println("希尔排序之后的数据"); System.out.println(Arrays.toString(arr)); &#125; /* * 希尔排序: * 和插入排序的原理基本一样 * 只是加上了步长 * 从而在一定的程度上加快了查询的速度 * 当然这只是相对而言的 * 1、 先迭代步长 * 2、 然后遍历所有元素 如果加上步长之后的那个元素比没加之前的元素小的话,就将后面的元素向前移动知道找到合适的位置 * * */ private static void shellSort(int[] arr) &#123; int k = 1; //遍历所有的步长 for (int d = arr.length / 2; d &gt; 0; d /= 2) &#123; //遍历所有的元素 for (int i = d; i &lt; arr.length; i++) &#123; //如果当前的元素大于加上步长之后的那个元素的话 if (arr[i] &lt; arr[i - d]) &#123; int temp = arr[i]; int j; for (j = i - d; j &gt;= 0 &amp;&amp; temp &lt; arr[j]; j-=d) &#123; arr[j + d] = arr[j]; &#125; arr[j + d] = temp; &#125; &#125; &#125; &#125;&#125; 1.5 归并排序和选择排序一样，归并排序的性能不受输入数据的影响，但表现比选择排序好的多，因为始终都是O(n log n）的时间复杂度。代价是需要额外的内存空间。 归并排序是建立在归并操作上的一种有效的排序算法。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。归并排序是一种稳定的排序方法。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为2-路归并。 代码示例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101package com.mlz.suanfa.compare;/* * @创建人: MaLingZhao * @创建时间: 2019/10/30 * @描述： *//* * 归并排序的思路： * 二路归并 递归思想 * 1、 计算左边，计算右边，左边和右边进行合并 注意边界条件 * 如何计算左边和右边的合并 * 1. 创建临时数组保存 * 2. 比较左右两边的数组 * 3. 依次写入到临时数组之中 * 4. 处理多余数据 * 5. 将临时数组中的数据写回到原来的数组之中 * * * 所有的都要注意边界问题 存在性问题 * */import java.util.Arrays;public class MergeSort &#123; public static void main(String[] args) &#123; int[] arr = new int[]&#123;1, 3, 5, 2, 4, 6, 8, 10&#125;; System.out.println("归并排序之前的数据："); System.out.println(Arrays.toString(arr)); mergeSort(arr, 0, arr.length - 1); System.out.println("归并排序之后的数据："); System.out.println(Arrays.toString(arr)); &#125; /* * 归并排序 * */ private static void mergeSort(int[] arr, int low, int high) &#123; int middle = (high + low) / 2; if (low &lt; high) &#123; //处理左边 mergeSort(arr, low, middle); //处理右边 mergeSort(arr, middle + 1, high); //归并 merge(arr, low, middle, high); &#125; &#125; private static void merge(int[] arr, int low, int middle, int high) &#123; //用于存储归并后的临时数组 int[] temp = new int[high - low + 1]; //记录第一个数组中需要遍历的下标 int i = low; //记录第二个数组中需要遍历的下标 int j = middle + 1; //用于记录临时数组需要存放的下标 int index = 0; while (i &lt;= middle &amp;&amp; j &lt;= high) &#123; //第一个数组的数据更小 if (arr[i] &lt; arr[j]) &#123; //把小的数据放到临时数组中 temp[index] = arr[i]; //让下标向后移动一位 i++; &#125; else &#123; temp[index] = arr[j]; j++; &#125; index++; &#125; //处理多余的数据 while (j &lt;= high) &#123; temp[index] = arr[j]; j++; index++; &#125; while (i &lt;= middle) &#123; temp[index] = arr[i]; i++; index++; &#125; //把临时数组中的数据重新写回到数组中 for (int k = 0; k &lt; temp.length; k++) &#123; arr[k + low] = temp[k]; &#125; &#125;&#125; 当然了这是递归形式的实现归并排序我们还可以借助其他的方法进行实现 1.6 快速排序快速排序的基本思想：通过一趟排序将待排记录分隔成独立的两部分，其中一部分记录的关键字均比另一部分的关键字小，则可分别对这两部分记录继续进行排序，以达到整个序列有序。 快速排序使用分治法来把一个串（list）分为两个子串（sub-lists）。具体算法描述如下： 从数列中挑出一个元素，称为 “基准”（pivot）； 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作； 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。` 代码案例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package com.mlz.suanfa.compare;/* * @创建人: MaLingZhao * @创建时间: 2019/10/30 * @描述： *//** * 快速排序的思路* 1. 找出基准点* 2. 比基准点大的数据在右边， 否则数据的赋值给左边，右边的数据同理* 3. 最后重新选择基准点 arr[low]是基准点 然后左边的数据做同样的处理 右边的数据做同样的处理* 4. 依次递归 * */import java.util.Arrays;public class QuickSort &#123; public static void main(String[] args) &#123; int[] arr = new int[] &#123;3,4,6,7,2,7,2,8,0,9,1&#125;; System.out.println("快速排序之前的数据："); System.out.println(Arrays.toString(arr)); quickSort(arr,0,arr.length-1); System.out.println("快速排序之后的数据"); System.out.println(Arrays.toString(arr)); &#125; private static void quickSort(int[] arr, int start, int end) &#123; if(start&lt;end) &#123; //将数组中的第0个数字作为标准数 int stard = arr[start]; //记录需要排序的下标 int low = start; int high = end; //循环找出比标准数大的数和比标准数小的数 while(low&lt;high) &#123; while(low&lt;high&amp;&amp; arr[high]&gt;=stard) &#123; high--; &#125; arr[low]=arr[high]; while(low&lt;high &amp;&amp; arr[low]&lt;=stard) &#123; low++; &#125; arr[high]=arr[low]; &#125; //标准数一开始是start 那么low++ 到最后就成了分界点 arr[low]=stard; //左边 quickSort(arr,start,low); //右边 quickSort(arr,low+1,end); &#125; &#125;&#125; 1.7 堆排序 堆排序(Heapsort)是指利用堆积树（堆）这种数据结构所设计的一种排序算法，它是选择排序的一种。可以利用数组的特点快速定位指定索引的元素。堆分为大根堆和小根堆，是完全二叉树。 什么是完全二叉树呢 这样我们的兴趣再次燃起 什么是满二叉树呢 简单来说：堆排序是将数据看成是完全二叉树、根据完全二叉树的特性来进行排序的一种算法 最大堆要求节点的元素都要不小于其孩子，最小堆要求节点元素都不大于其左右孩子 那么处于最大堆的根节点的元素一定是这个堆中的最大值 完全二叉树有个特性 左边子节点位置 = 当前父节点的两倍 + 1，右边子节点位置 = 当前父节点的两倍 + 2` int leftNode=2*index+1 int rightNode =2 * index+2 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package com.mlz.suanfa.compare;/* * @创建人: MaLingZhao * @创建时间: 2019/10/30 * @描述： *//** 堆排序的算法流程** 构建大顶堆从最后一个非叶子节点开始* 然后交换第0个数和最后一个数的位置 每次进行一次调整 最后一个非叶子节点的数减1 依次进行调整 那么每次调整完成的大顶堆的序号都是最后一个** */import java.util.Arrays;import java.util.concurrent.TimeUnit;public class HeapSort &#123; public static void main(String[] args) &#123; int[] arr = new int[]&#123;9, 6, 8, 7, 0, 1, 10, 4, 2&#125;; System.out.println("堆排序之前的数据："); System.out.println(Arrays.toString(arr)); heapSort(arr); System.out.println("堆排序之后的数据："); System.out.println(Arrays.toString(arr)); &#125; private static void heapSort(int[] arr) &#123; //开始位置是最后一个非叶子节点 int start = (arr.length - 1) / 2; //调整为大顶堆 for (int i = start; i &gt;= 0; i--) &#123; maxHeap(arr, arr.length - 1, i); &#125; //先把数组中的第0个和最后一个数交换位置，再把前面的调整成大顶堆 for (int i = arr.length - 1; i &gt; 0; i--) &#123; int temp = arr[0]; arr[0] = arr[i]; arr[i] = temp; maxHeap(arr, i, 0); &#125; &#125; private static void maxHeap(int[] arr, int size, int index) &#123; //左子节点 int leftNode = 2 * index + 1; //右子节点 int rightNode = 2 * index + 2; int max = index; //和两个子节分别对比，找出最大的节点 if (leftNode &lt; size &amp;&amp; arr[leftNode] &gt; max) &#123; max = leftNode; &#125; if (rightNode &lt; size &amp;&amp; arr[rightNode] &gt; arr[max]) &#123; max = rightNode; &#125; //交换位置 if (max != index) &#123; int temp = arr[index]; arr[index] = arr[max]; arr[max] = temp; //交换位置吧以后可能会破坏之前排好的堆 所以需要重新进行调整 maxHeap(arr, size, max); &#125; &#125;&#125; 最佳情况：T(n) = O(nlogn) 最差情况：T(n) = O(nlogn) 平均情况：T(n) = O(nlogn) 1.8 基数排序基数排序是一种可以按优先级排序的排序方法，例如扑克牌或者麻将的排序，先按照花色排序，再按照点数排序。 描述： 1、取数组最大值的位数； 2、从数据最低位开始排序，这里是从个位开始排，得到新的数组； 3、再按次低位排序，位数不够就按0计算； 4、重复以上过程直到排到最高位。 这个过程有点抽象 不太容易想象的出来 但是如果配合图示的话，就变的相对而言简单了很多 https://www.jianshu.com/p/14d06151a84d 这个链接里面有对应的文档结合代码看的话，我想应该会有非常好的效果 能够充分理解 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990package com.mlz.suanfa.compare;/* * @创建人: MaLingZhao * @创建时间: 2019/10/30 * @描述： *//* * * 基数排序的思路 * 先计算个位的顺序 然后计算十位 然后计算百位 * 这样 最后将数据依次取出自然就是有顺序的 * */import java.util.Arrays;public class RadixSort &#123; public static void main(String[] args) &#123; int[] arr = new int[]&#123;23, 6, 189, 45, 9, 287, 56, 1, 798, 34, 65, 652, 5&#125;; System.out.println("基数排序之前的数据"); System.out.println(Arrays.toString(arr)); radixSort(arr); System.out.println("基数排序之后的数据"); System.out.println(Arrays.toString(arr)); &#125; private static void radixSort(int[] arr) &#123; //存储数组中最大的数字 int max = Integer.MIN_VALUE; for (int i = 0; i &lt; arr.length; i++) &#123; if (arr[i] &gt; max) &#123; max = arr[i]; &#125; &#125; //计算最大的数字是几位数 int maxLength = (max + "").length(); //用于临时存储数据的数组 [0,1,2,3,4,5,6,7,8,9] * [arr.length] int[][] temp = new int[10][arr.length]; //用于存储在每一个数字[0,1,2,3,4,5,6,7,8,9]对应的下标中的数字的个数 存放位置的下标 int[] counts = new int[10]; // ys 0 12 3 4 5 6 7 8 9 //根据最大长度决定比较的次数 先计算一位数 再计算二位数 最后计算三位数 for (int i = 0, n = 1; i &lt; maxLength; i++, n *= 10) &#123; //把每一个数字分别计算余数 for (int j = 0; j &lt; arr.length; j++) &#123; //计算余数----&gt;&gt;&gt;得到的是 int ys = arr[j] / n % 10; //23, 6, 189, 45, 9, 287, 56, 1, 798, 34, 65, 652, 5 //把当前遍历的数据放入到指定的数组中 temp[ys][counts[ys]] = arr[j]; //记录数量 counts[ys]++; &#125; //记录取的元素要放的位置 int index = 0; //把数字取出来 for (int k = 0; k &lt; counts.length; k++) &#123; //记录数组中的当前余数记录的数量不为0 if (counts[k] != 0) &#123; //循环取出元素 for (int l = 0; l &lt; counts[k]; l++) &#123; //取出元素 arr[index] = temp[k][l]; //记录下一个位置 index++; &#125; //数量重置为0 下一次使用 counts[k] = 0; &#125; &#125; &#125; &#125;&#125; 2.1 二叉树的建立 2 算法面试 展现正确的思维的方式 沟通本身很重要 ，它暗示着你思考问题的方式 、 考虑很多的问题 算法问题的例子国内公司面试问题参考了leetcode的模型 在leetcode实现以下算法时间长了新的内容算法面试问题的各个分类 过去的经历形成的思维方式 形式上考察你 评估你 衡量你 对于每一名技术人员过去参与的项目 项目经历： 已经工作的人来说 对于大多数的研究生来说 也是会参加项目的计算机的本科生来说 毕业设计 非常好的项目 相应的课程设计 相对的大一些的项目 本科 新的方法想办法将算法封装的更好 非科班 计算机是一个很大的领域 找到相应的项目 实习的方式 参与实战课程的学习 在校教育的机构 理论知识 完成一个拿得出手的项目 算法思想 了解自己想要的是什么分清主次的去看对算法非常感兴趣的话 对算法有了一定的了解之后追求一步到位 很多书 阅读 理解 很多遍 学习切记完美主义 对于算法面试来说 高级的数据结构 高级的算法红黑树计算结合B-TreeFFT斐波那契数列数论 高级的数据结构和算法提及的额概率是非常低的远远达不到信息学竞赛的水平 本科的时候参加信息学的竞赛对算法面试时有很大帮助的 算法面试的准备的范围 不要轻视基础的算法和数据结构 ，而只关注‘有意思’的问题 真正的去进行算法面试的时候都是基础的面试的数据结构的算法的实现 链表 栈 队列 哈希表 图 trie 并查集 排序算法 堆 二叉树 图 算法与数据结构 注意题目中的条件 给定一个有序数组。。。。。。 设计一个O(nlogn)的算法nlogn 排序的时间复杂度 无需考虑额外的空间 开辟额外的空间 数据规模大概是10000 设置O(n^2)的算法 当真正没有思路的时候 给自己几个简单的测试用例 试验一下不要忽视暴力揭发，暴力解法通常是思考的起点 展示暴力解法 在一个字符串中找没有重复的最长字串输入 “abcabcbb” 结果为”abc”输入”bbbbbb” 结果为”b” 什么是暴力法呢 最简单的解法遍历所有的字符串 s[i……j] 使用O(n^2)的算法遍历i,j 可以的二到所有的字串s[i,j] 不仅仅给出代码 算法面试优秀不意味着技术面试的优秀 项目经历和项目中遇到的实际问题 你印象中最深的bug是什么 面向对象 设计模式 网络相关 内存相关 并发 系统设计 ： scakability 技术面试不是考察你的知识水平 技术面试上形式上考察你的能力 这是衡量你的一种方式 通过过去参与的项目 你的过去的经历 项目 3 如何准备算法面试准备面试和准备算法面试的区别 算法面试 啃完一本算法导论？ 计算机领域的缺点 算法导论理论的推导和证明 面试官不能完全掌握理论的推导和证明 算法证明 真的有兴趣 前两编有选择的去阅读 有选择的去看 抓大放小 对算法有一定的实践 学习的时候不能最求一步到位 算法面试不需要达到信息学竞赛的水平 不要轻视基础算法与数据结构 而只关注有意思的题目 采用堆排序 对应二叉树的中序遍历 阿里巴巴面试 在线判题 系统 online judge leetcode 真实的面试问题 全球公认的用于面试的网站 第二个 HackerRank 这个网站对问题的分类比较详细 人工智能 机器学习非常 学习和实践做题 要掌握平衡 解决算法面试问题的整体的思路 注意题目中的条件 给定一个有序数组 数据规模10000 设计一个O(nlogn)的算法 当没有思路的时的简单测试用例 不要忽视暴力解法 不要忽视暴力解法 优化算法 在瓶颈处寻找答案 O(n^3)的算法 字符串中有没有重复的字符 实际的代码的编写问题 在编写代码的时候 极端条件的判断 数组为空 字符串 为空 数量为0 指针为null 变量名 模块化和复用性 陌生问题的思考 对于基本问题，白板编程 实现一个基本的最大堆 花一些时间准备即可对于基本的编程能做到白板编程 C++ JAVA 语言实现 4面试中的时间复杂度的分析5.1 -到底什么是大O教科书中的数学定义 真正的面试中不会让你用数学的方法来证明 对于一个算法莱索 n 代表数据规模 O(n) 表示运行算法所需要的指令数 什么是BigO a b c d 都是常数 数据规模大的情况下，后面的主要影响是n 算法A： O(n) 10000*n 算法B： O(n^2) 10*n^2 在学术界 严格的讲O(f(n))表示算法执行的上界 在归并排序算法的时间复杂度是O(nlogn)同时也是O(n^2) O表示的是时间的上界 cnlogn&lt;O(n^2) 了解即可 但是真的做学术研究的话，要谨慎的说 我们 要说归并排序的时间复杂度是O(nlogn) O 上界的概念 O(nlogn+n)=O(logn)随着数据规模的增大O(nlogn)起到了主导的作用 O(AlogA+B) O(AlogA+B^2) 对邻接表实现的图进行遍历 时间复杂度 为为什么是V+E呢 因为V是图的所有的顶点 E是图的所有的边 一个时间复杂度的问题 错误的将字符串的长度 数组的长度 数组中有多少个字符串 字符串的长度 nlogn比较的数组 两个整数的比较O(1) n和s两回事 算法的复杂度适合用例相关的 插入排序 最好 O(n) 最坏： O(n^2) 平均情况： O(n^2) 快速排序算法 最好：O(nlogn) 最差情况：O(n^2) 平均: O(nlogn) 排序算法 5.2 数据规模的概念 空间复杂度 多开一个辅助的数组： O(n) 多开一个二维数组： O(n^2) 多开常数空间O(1) 递归调用是具有空间代价的 5.3 常见的时间复杂度的分析O(1)在这里没有数据规模的变化 O(n) 存在一个循环 循环是n相关的 执行的是cn次 将字符串翻转 abc cba 扫描前面的一半 和后面的一半 进行调换 O(n^2)级别的算法 选择排序 我们看到了双层的循环 严格的讲O(n^2)执行的指令数是和O(n^2)有一定的关联的 双重循环 每一个班级有30个同学 虽然是双重循环 但是最后的打印的操作是30次 30n的基本操作 对于第二层来说 30是固定的 我们看一下 二分查找的代码 每一次查找 我们都可以扔掉一半的元素 n —&gt;&gt;&gt; n/2—&gt;&gt;n/4………..-&gt;&gt;&gt; 1 整型转换成字符串 整型转换成字符串 nlogn级别 具体问题具体分析O(sqrt(n))的算法 5.4 复杂度实验我们以为写出了一个O(nlogn)的算法 但是实际上是O(n^2)的算法 复杂度实验 5.5 递归算法时间复杂度分析归并排序 快速排序都是递归的排序算法 不是有递归的函数就一定是O(logn) 二分查找法 计算和 幂运算的复杂度 时间复杂度 递归深度 logn 时间复杂度 O(logn) 分析递归调用的次数 归并排序 时间复杂度 处理的数据规模主键缩小的 5.6 均摊复杂度分析动态数组(Vector) 假设数组的容量为n 每一次添加O(1) 面试总结leetcode是一个非常好的锻炼算法能力的平台 国内的公司大多都参考leetcode来的 题目 算法思考如何进行优化 活着的课程]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bigdta]]></title>
    <url>%2Fblog4%2F2019%2F10%2F28%2Fbigdata%2F</url>
    <content type="text"><![CDATA[1 Linux的常用的命令1.1 常用的命令1.1 top查看内存 1.2 df -h 1.3 iotop io 1.5 netstat -tnunlp|grep 端口号 1.6 uptime 1.7 ps aux 1.2 shell的常用的工具awk 文本分析工具 生成报表 支持函数 print split substr sub gsub if while do/while break for continue sed cut sort 2 hadoop的总结2.1 hadoop常用的端口号50070 50075 http服务的端口号 50020 ipc的端口号datanode url web ui rpc 远程过程调用 hadoop的ipc 2.2 配置文件8 env site core-site yarn-site mapred-site hdfs-site slaves 2.3 hdfs的读流程和写流程2.4 MapReduce的shufflemap方法之前Reduce方法之后的叫shuffle map –》》 分区方法–》》 标记分区–》》 环形缓冲区 –》》100M —》》 80%进行溢写–》》一溢写之前对数据排序 —》》 产生大量的一些文件 —》》 溢写文件进行归并排序 —》》 对溢写的文件也可以进行Combiner操作 Reduce拉取map福安对应的数据 拉去完成之后将数据保存到内存，内存不够了再写到磁盘 采用归并排序将磁盘中的数据和内存中的数据都进行归并排序]]></content>
  </entry>
  <entry>
    <title><![CDATA[各个项目的总结]]></title>
    <url>%2Fblog4%2F2019%2F10%2F27%2Fsummarize%2F</url>
    <content type="text"><![CDATA[1 电商推荐系统]]></content>
  </entry>
  <entry>
    <title><![CDATA[ElectricityWarehouse]]></title>
    <url>%2Fblog4%2F2019%2F10%2F25%2FElectricityWarehouse%2F</url>
    <content type="text"><![CDATA[1 项目需求分析及架构设计1.1 项目需求数据采集平台搭建 实现用户行为数据仓库的分层搭建 实现用户行为数据的分层搭建 针对数据仓库中的数据进行留存，转化率，GMV，复购率，活跃等报表分析 1.2项目的框架技术选型 数据采集运输 flume kafka sqoop Logstash Data 数据存储： Mysql HDFS HBase Redis MongoDb 数据计算： Hive Tez Spark Flink Storm 数据查询：Presto druid Impala Kylin 1.3 框架版本apache 开源的 1.4 服务器的选型物理机 128G 内存 20核cpu 8T机械2T 固态 价格4万左右 运维： 有专业的运维人员 1.5集群的资源规划1.5.1 资源按照数据计算 每日的活跃用户100万 每个人平均每天100条数据 最后选择10台 1.5.2 集群的资源规划‘）测试集群服务器规划 服务名称 子服务 服务器hadoop102 服务器hadoop103 服务器hadoop104 HDFS NameNode √ DataNode √ √ √ SecondaryNameNode √ Yarn NodeManager √ √ √ Resourcemanager √ Zookeeper Zookeeper Server √ √ √ Flume(采集日志) Flume √ √ Kafka Kafka √ √ √ Flume（消费Kafka） Flume √ Hive Hive √ MySQL MySQL √ Sqoop Sqoop √ Presto Coordinator √ Worker √ √ Azkaban AzkabanWebServer √ AzkabanExecutorServer √ Druid Druid √ √ √ 服务数总计 13 8 9 项目的架构图]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>项目实战</tag>
        <tag>数仓</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow]]></title>
    <url>%2Fblog4%2F2019%2F10%2F25%2Ftensorflow%2F</url>
    <content type="text"><![CDATA[1 介绍机器翻译 理论部分 Seq2seq模型 Attensoin + Seq2Seq Transformer模型 实战部分 原始的seq2seq模型 encoder 输入给循环神经网络 循环神经网络一直有一个隐含状态，一直在传 最后输出一个隐含状态 这个隐含状态会变成decoder这个神经网络的初始状态， 给decoder这个神经网络输入一个空的标识符 生成第一个单词 第一个单词输入给decoder生成第二个单词 第二个单词传递给第三个单词，这样decoder就能做句子的生成了 这是15年到16年比较流行的一个机器翻译框架 基于-Attension的seq2seq encoder的每一步输出都会传到decoder中去 这个变动如何进行计算 采取加权求和 EO ： encoder的各个位置的输出 H： decoder某一步的隐含状态 FC： 全局连接层 X: decoder的每一步的输入 source=FC(tanh(FC(EO)+FC(H))) H是每一步的 EO是decoder的多步 激活函数 tanh source =EO * W *H W权重矩阵 attension_weights=softmax(score,axis=1) 二 实战]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Personas]]></title>
    <url>%2Fblog4%2F2019%2F10%2F23%2FPersonas%2F</url>
    <content type="text"><![CDATA[创建 项目 业务流程 hadoop的开发环境搭建 hbase的环境搭建]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>项目实战</tag>
        <tag>用户画像</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FlinkPicture]]></title>
    <url>%2Fblog4%2F2019%2F10%2F23%2FFlinkPicture%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[dubbo]]></title>
    <url>%2Fblog4%2F2019%2F10%2F22%2Fdubbo%2F</url>
    <content type="text"><![CDATA[1基础知识1.1 分布式理论1 什么是分布式系统《分布式系统原理与范型》定义： “分布式系统是若干独立计算机的集合，这些计算机对于用户来说就像单个相关系统” 分布式系统（distributed system）是建立在网络之上的软件系统。 随着互联网的发展，网站应用的规模不断扩大，常规的垂直应用架构已无法应对，分布式服务架构以及流动计算架构势在必行，亟需\一个治理系统**确保架构有条不紊的演进。 2 发展演变 1 单一应用架构当网站流量很小时，只需一个应用，将所有功能都部署在一起，以减少部署节点和成本。此时，用于简化增删改查工作量的数据访问框架(ORM)是关键。 适用于小型网站，小型管理系统，将所有功能都部署到一个功能里，简单易用。 缺点： 1、性能扩展比较难 ​ 2、协同开发问题 ​ 3、不利于升级维护 2 垂直应用架构当访问量逐渐增大，单一应用增加机器带来的加速度越来越小，将应用拆成互不相干的几个应用，以提升效率。此时，用于加速前端页面开发的Web框架(MVC)是关键。 通过切分业务来实现各个模块独立部署，降低了维护和部署的难度，团队各司其职更易管理，性能扩展也更方便，更有针对性。 缺点： 公用模块无法重复利用，开发性的浪费 3 分布式服务框架当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，使前端应用能更快速的响应多变的市场需求。此时，用于提高业务复用及整合的分布式服务框架RPC是关键。 4 流动计算架构当服务越来越多，容量的评估，小服务资源的浪费等问题逐渐显现，此时需增加一个调度中心基于访问压力实时管理集群容量，提高集群利用率。此时，用于提高机器利用率的资源调度和治理中心 SOA（Service Oriented Architecture）是关键 3 RPC1 什么叫rpcRPC【Remote Procedure Call】是指远程过程调用，是一种进程间通信方式，他是一种技术的思想，而不是规范。它允许程序调用另一个地址空间（通常是共享网络的另一台机器上）的过程或函数，而不用程序员显式编码这个远程调用的细节。即程序员无论是调用本地的还是远程的函数，本质上编写的调用代码基本相同。 2 RPC的基本原理 1.2 dubbo的核心概念1 简介Apache Dubbo (incubating) |ˈdʌbəʊ| 是一款高性能、轻量级的开源Java RPC框架，它提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务自动注册和发现。 官网： http://dubbo.apache.org/ 2 基本概念 1.3 dubbo的环境搭建 1.4 dubbo-helloworld 1 服务的提供者（provider）： 暴露服务的服务提供方，服务者启动的时候想服务中心注册自己的服务 2 服务消费者（Consumer）：调用远程服务的服务消费方，服务消费者在启动时，向注册中心订阅自己所需的服务，服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 3 注册中心（Registry）注册中心返回服务提供者的地址列表给消费者，如果有变更，服务中心将基于长连接将变更数据给消费者 4 监控中心（Monitor）服务者和提供者，在内存中 累计调用次数和调用时间，定时每一分钟发送一次统计数据给注册中心 调用关系说明 服务器负责启动，加载运行服务提供者 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中翻新订阅自己需要的服务 注册中西返回服务提供者的列表给消费者，l 如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，l 提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 服务提供者和服务提供者，在内存中累计调用次数和调用时间，定时每一分钟发送一次数据到监控中心 1.3 dubbo的环境搭建1 windows下安装zookeeper1 zookeeper网址 https://archive.apache.org/dist/zookeeper/zookeeper-3.4.13/ 2 解压运行zkServer.md 发现报错，原因是没有zoo.cfg 配置文件 3 修改zoo.cfg配置文件将conf下的zoo_sample.cfg复制一份改名为zoo.cfg即可。 注意几个重要位置： dataDir=./ 临时数据存储的目录（可写相对路径） clientPort=2181 zookeeper的端口号 因为java版本的问题可能不会正常启动 不要安装java8_2 及以上 java8_1_xx支持 修改完成后再次启动zookeeper 4 测试zkCli.cmd 进入客户端 s /：列出zookeeper根下保存的所有节点 create –e /mage 123：创建一个mage节点，值为123 get /mage：获取/mage节点的值s /：列出zookeeper根下保存的所有节点 2 windows下安装dubbo-admindubbo本身并不是一个服务软件。它其实就是一个jar包能够帮你的java程序连接到zookeeper，并利用zookeeper消费、提供服务。所以你不用在Linux上启动什么dubbo服务。 但是为了让用户更好的管理监控众多的dubbo服务，官方提供了一个可视化的监控程序，不过这个监控即使不装也不影响使用。 1、下载dubbo-adminhttps://github.com/apache/incubator-dubbo-ops https://github.com/apache/dubbo https://github.com/apache/dubbo-admin 2、进入目录，修改dubbo-admin配置修改 src\main\resources\application.properties 指定zookeeper地址 3、打包dubbo-admin执行这一条命令的时候我们要把target目录清空 不然会报找不到 正确的target路径的错苏 mvn clean package -Dmaven.test.skip=true 失败 没有删除的时候 删除之后 4、运行dubbo-adminjava -jar dubbo-admin-0.0.1-SNAPSHOT.jar 和zoookeeper建立连接 我们在dubbo的配置文件里看到 我们知道dubbo在7001 端口启动 访问 localhost:7001 想我一样装了不少插件的浏览器的话可能会出现一些问题 换个浏览器就好使 一定要注意访问的端口号 3 linux安装zookeeper安装jdk 就不说了 太简单了 安装zookeeper的话 也是解压配置dataDir的路 径 那我们搞测试每次启动zookeeper太麻烦了 给出启动脚本 12345678910111213#!/bin/bash#chkconfig:2345 20 90#description:zookeeper#processname:zookeeperZK_PATH=/home/atguigu/cluster/zookeeper-3.4.10export JAVA_HOME=/opt/module/jdk1.8.0_144case $1 in start) sh $ZK_PATH/bin/zkServer.sh start;; stop) sh $ZK_PATH/bin/zkServer.sh stop;; status) sh $ZK_PATH/bin/zkServer.sh status;; restart) sh $ZK_PATH/bin/zkServer.sh restart;; *) echo "require start|stop|status|restart" ;;esac ZK_PATH 和JAVA_HOME找自己的即可 脚本注册成service服务 添加执行权限 service zookeeper start 我此时用的不是root用户会报权限的问题 所以登录用root用户才会显示服务 jps zookeeper 搭建完了 3.时我们chkconfig –add zookeeper 添加开启启动到服务器 4 linux安装dubbo-admin控制台现在配置dubbo的开启启动 首先上传jar包 1java -jar dubbo-admin-0.0.1-SNAPSHOT.jar 访问192.168.2.101:7001 root/ root登录 1234567891011121314#!/bin/bash#chkconfig:2345 20 90#description:dubbo#processname:dubboexport JAVA_HOME=/opt/module/jdk1.8.0_144HOME=rootAPPNAME=dubbo-admin-0.0.1-SNAPSHOT.jarcase $1 in start) $JAVA_HOME/bin/java -jar /$HOME/$APPNAME &gt;/dev/null &amp; ;; stop) ps -ef|grep $APPNAME |grep -v grep|awk '&#123;print $2&#125;'|xargs kill &gt;/dev/null ;; esac 此时我们像zookeeper 我们要理解脚本的含义 grep -v grep 是过滤掉含有grep掉的指令 | 是管道 而xarg kill 则是杀死进程 上面的&amp; 代表着我们的程序后台运行 然后 chkconfig –add dubbo service dubbo start 然后我们完成了dubbo的开机启动 1.4 dubbo的HelloWorld1 提出需求某个电商系统，订单服务需要调用用户服务获取某个用户的所有地址； 我们现在 需要创建两个服务模块进行测试 模块 功能 订单服务web模块 创建订单等 用户服务service模块 查询用户地址等 测试预期结果： ​ 订单服务web模块在A服务器，用户服务模块在B服务器，A可以远程调用B的功能。某个电商系统，订单服务需要调用用户服务获取某个用户的所有地址； 我们现在 需要创建两个服务模块进行测试 模块 功能 订单服务web模块 创建订单等 用户服务service模块 查询用户地址等 测试预期结果： ​ 订单服务web模块在A服务器，用户服务模块在B服务器，A可以远程调用B的功能。 4.2）、工程架构根据 dubbo《服务化最佳实践》 1、分包建议将服务接口，服务模型，服务异常等均放在 API 包中，因为服务模型及异常也是 API 的一部分，同时，这样做也符合分包原则：重用发布等价原则(REP)，共同重用原则(CRP)。 如果需要，也可以考虑在 API 包中放置一份 spring 的引用配置，这样使用方，只需在 spring 加载过程中引用此配置即可，配置建议放在模块的包目录下，以免冲突，如：com/alibaba/china/xxx/dubbo-reference.xml。 2、粒度服务接口尽可能大粒度，每个服务方法应代表一个功能，而不是某功能的一个步骤，否则将面临分布式事务问题，Dubbo 暂未提供分布式事务支持。 服务接口建议以业务场景为单位划分，并对相近业务做抽象，防止接口数量爆炸。 不建议使用过于抽象的通用接口，如：Map query(Map)，这样的接口没有明确语义，会给后期维护带来不便。 1.4 dubbo的hello World模块 1.5 监控中心安装监控中心 名称空间的配置 procotol address 注册中心 1.6 整合springboot 2 dubbo的配置2.1配置的原则2.2 重试次数2.3 超时时间2.4 版本号3 高可用3.1 zookeeper宕机与dubbo智联3.2 集群下的dubbo的负载均衡配置3.3 整合hystrix，服务熔断与降级处理4 dubbo原理4.1 rpc原理 4.2 netty原理Netty是一个异步事件驱动的网络应用程序框架， 用于快速开发可维护的高性能协议服务器和客户端。它极大地简化并简化了TCP和UDP套接字服务器等网络编程。 NIO(Non-Blocking IO) Selector :选择器 翻译：多路复用器 Connect（连接就绪）、Accept（接受就绪）、Read（读就绪）、Write（写就绪） Netty基本原理： 1 框架设计 l config 配置层：对外配置接口，以 ServiceConfig, ReferenceConfig 为中心，可以直接初始化配置类，也可以通过 spring 解析配置生成配置类 l proxy 服务代理层：服务接口透明代理，生成服务的客户端 Stub 和服务器端 Skeleton, 以 ServiceProxy 为中心，扩展接口为 ProxyFactory l registry 注册中心层：封装服务地址的注册与发现，以服务 URL 为中心，扩展接口为 RegistryFactory, Registry, RegistryService l cluster 路由层：封装多个提供者的路由及负载均衡，并桥接注册中心，以 Invoker 为中心，扩展接口为 Cluster, Directory, Router, LoadBalance l monitor 监控层：RPC 调用次数和调用时间监控，以 Statistics 为中心，扩展接口为 MonitorFactory, Monitor, MonitorService l protocol 远程调用层：封装 RPC 调用，以 Invocation, Result 为中心，扩展接口为 Protocol, Invoker, Exporter l exchange 信息交换层：封装请求响应模式，同步转异步，以 Request, Response 为中心，扩展接口为 Exchanger, ExchangeChannel, ExchangeClient, ExchangeServer l transport 网络传输层：抽象 mina 和 netty 为统一接口，以 Message 为中心，扩展接口为 Channel, Transporter, Client, Server, Codec l serialize 数据序列化层：可复用的一些工具，扩展接口为 Serialization, ObjectInput, ObjectOutput, ThreadPool 2 dubbo原理 暴露服务 3、dubbo原理 -服务引用 4、dubbo原理 -服务调用]]></content>
      <categories>
        <category>javaEEE</category>
      </categories>
      <tags>
        <tag>dubbo</tag>
        <tag>微服务框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flink]]></title>
    <url>%2Fblog4%2F2019%2F10%2F22%2Fflink%2F</url>
    <content type="text"><![CDATA[1 Flink框架flink的官网链接 https://flink.apache.org/ flink1.7的文档连接 https://ci.apache.org/projects/flink/flink-docs-release-1.7/ 1.1 Flink是什么Apache Flink是一个框架和分布式处理引擎，用于对无界和有界数据流进行有状态计算。Flink被设计在所有常见的集群环境中运行，以内存执行速度和任意规模来执行计算。 1.2 Flink的历史Flink起源于Stratosphere项目，Stratosphere是在2010~2014年由3所地处柏林的大学和欧洲的一些其他的大学共同进行的研究项目，2014年4月Stratosphere的代码被复制并捐赠给了Apache软件基金会，参加这个孵化项目的初始成员是Stratosphere系统的核心开发人员，2014年12月，Flink一跃成为Apache软件基金会的顶级项目。 Flink虽然诞生的早(2010年)，但是其实是起大早赶晚集，直到2015年才开始突然爆发热度。 在Flink被apache提升为顶级项目之后，阿里实时计算团队决定在阿里内部建立一个 Flink 分支 Blink，并对 Flink 进行大量的修改和完善，让其适应阿里巴巴这种超大规模的业务场景。 Blink由2016年上线，服务于阿里集团内部搜索、推荐、广告和蚂蚁等大量核心实时业务。与2019年1月Blink正式开源，目前阿里70%的技术部门都有使用该版本。 Blink比起Flink的优势就是对SQL语法的更完善的支持以及执行SQL的性能提升。 1.3 为什么用Flink 1 流数据更真实的反映了我们的生活方式 2 传统的数据架构是基于有限数据集的 3 我们的目标是要低延迟，高吞吐，并能保证准确性和良好的容错性 1.4 flink的特点1 事件驱动型事件驱动型应用是一类具有状态的应用，它从一个或多个事件流提取数据，并根据到来的事件触发计算、状态更新或其他外部动作。比较典型的就是以kafka为代表的消息队列几乎都是事件驱动型应用。 与之不同的就是SparkStreaming微批次，如图： 事件驱动型： 事件驱动型 2 流与批的世界观 批处理的特点是有界、持久、大量，非常适合需要访问全套记录才能完成的计算工作，一般用于离线统计。 流处理的特点是无界、实时, 无需针对整个数据集执行操作，而是对通过系统传输的每个数据项执行操作，一般用于实时统计。 在spark的世界观中,数据是由一个一个无限的小批次组成的。 而在flink世界观中，一切都是由流组成的，离线数据是有界限的流，实时数据是一个没有界限的流，这就是所谓的有界流和无界流。 无界数据流：无界数据流有一个开始但是没有结束，它们不会在生成时终止并提供数据，必须连续处理无界流，也就是说必须在获取后立即处理event。对于无界数据流我们无法等待所有数据都到达，因为输入是无界的，并且在任何时间点都不会完成。处理无界数据通常要求以特定顺序（例如事件发生的顺序）获取event，以便能够推断结果完整性。 有界数据流有明确定义的开始和结束，，可以在执行任何计算之前通过获取所有数据来处理有界流，处理有界流不需要有序获取，因为可以始终对有界数据集进行排序，有界流的处理也称为批处理。 这种以流为世界观的架构，获得的最大好处就是具有极低的延迟。 3 分层api 最底层级的抽象仅仅提供了有状态流，它将通过过程函数（Process Function）被嵌入到DataStream API中。底层过程函数（Process Function） 与 DataStream API 相集成，使其可以对某些特定的操作进行底层的抽象，它允许用户可以自由地处理来自一个或多个数据流的事件，并使用一致的容错的状态。除此之外，用户可以注册事件时间并处理时间回调，从而使程序可以处理复杂的计算。 实际上，大多数应用并不需要上述的底层抽象，而是针对核心API（Core APIs） 进行编程，比如DataStream API（有界或无界流数据）以及DataSet API（有界数据集）。这些API为数据处理提供了通用的构建模块，比如由用户定义的多种形式的转换（transformations），连接（joins），聚合（aggregations），窗口操作（windows）等等。DataSet API 为有界数据集提供了额外的支持，例如循环与迭代。这些API处理的数据类型以类（classes）的形式由各自的编程语言所表示。 Table API 是以表为中心的声明式编程，其中表可能会动态变化（在表达流数据时）。Table API遵循（扩展的）关系模型：表有二维数据结构（schema）（类似于关系数据库中的表），同时API提供可比较的操作，例如select、project、join、group-by、aggregate等。Table API程序声明式地定义了什么逻辑操作应该执行，而不是准确地确定这些操作代码的看上去如何 。 尽管Table API可以通过多种类型的用户自定义函数（UDF）进行扩展，其仍不如核心API更具表达能力，但是使用起来却更加简洁（代码量更少）。除此之外，Table API程序在执行之前会经过内置优化器进行优化。 你可以在表与 DataStream/DataSet 之间无缝切换，以允许程序将 Table API 与 DataStream 以及 DataSet 混合使用。 Flink提供的最高层级的抽象是 SQL 。这一层抽象在语法与表达能力上与 Table API 类似，但是是以SQL查询表达式的形式表现程序。SQL抽象与Table API交互密切，同时SQL查询可以直接在Table API定义的表上执行。 4 支持有状态计算Flink在1.4版本中实现了状态管理，所谓状态管理就是在流失计算过程中将算子的中间结果保存在内存或者文件系统中，等下一个事件进入算子后可以让当前事件的值与历史值进行汇总累计。 5 支持exactly-once语义在分布式系统中，组成系统的各个计算机是独立的。这些计算机有可能fail。 一个sender发送一条message到receiver。根据receiver出现fail时sender如何处理fail，可以将message delivery分为三种语义: At Most once: 对于一条message,receiver最多收到一次(0次或1次). 可以达成At Most Once的策略: sender把message发送给receiver.无论receiver是否收到message,sender都不再重发message. 2支持事件时间 At Least once: 对于一条message,receiver最少收到一次(1次及以上). 可以达成At Least Once的策略: sender把message发送给receiver.当receiver在规定时间内没有回复ACK或回复了error信息,那么sender重发这条message给receiver,直到sender收到receiver的ACK. Exactly once: 对于一条message,receiver确保只收到一次 目前大多数框架时间窗口计算，都是采用当前系统时间，以时间为单位进行的聚合计算只能反应数据到达计算引擎的时间，而并不是实际业务时间 2 Flink的小案例2.1 批处理 wordcount maven项目 flinkDemo pom文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.mlz&lt;/groupId&gt; &lt;artifactId&gt;flinkDemo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-scala_2.11&lt;/artifactId&gt; &lt;version&gt;1.7.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-streaming-scala --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-scala_2.11&lt;/artifactId&gt; &lt;version&gt;1.7.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;!-- 该插件用于将Scala代码编译成class文件 --&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.3.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;!-- 声明绑定到maven的compile阶段 --&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 这里我们要注意这个scala-maven-plugin插件的版本适当的调整 可能会出现一些问题 另外scala的安装版本必须高于 （不能高的太多 要求2.11 你不能装个2.12或2.13 ）或者等于要求的scala版本 否则会编译失败 添加scala的版本 添加scala框架 或者 创建文档hello.txt 12345678he magedage nihao manihapo xiongdimage mage sparkhe 12345678910111213141516171819202122232425262728293031323334353637383940414243package com.mlz.flinkimport org.apache.flink.api.java.DataSetimport org.apache.flink.api.scalaimport org.apache.flink.api.scala.&#123;AggregateDataSet, ExecutionEnvironment&#125;/* * @创建人: MaLingZhao * @创建时间: 2019/5/22 * @描述： *//*普处理wordcouhnt程序* */object BatchWordCount &#123; def main(args: Array[String]): Unit = &#123; //构造执行环境 val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment //读取文件 val input = "file:///d:/temp/hello.txt" val ds: scala.DataSet[String] = env.readTextFile(input) // 其中flatMap 和Map 中 需要引入隐式转换 import org.apache.flink.api.scala.createTypeInformation //经过groupby进行分组，sum进行聚合 val aggDs: AggregateDataSet[(String, Int)] = ds.flatMap(_.split(" ")). map((_, 1)). groupBy(0). sum(1) // 打印 aggDs.print() &#125;&#125; 2.2 流处理 wordcount我们使用的是socket数据 这样的话 我们需要开一个端口 那么我们打开我的一台虚拟机 发现没有nc 安装 1yum install -y nc (root ) 启动程序 没有反应 输入数据 随着数据的输入 我们发现 数据成流式出现 程序 为什么会出现10 吗 没有设置程序的并行度 设置并行度为2 我们发现出现的是1和2 当前电脑的运行的核数 看到我的电脑出现了10 因为我的电脑是12core的 3 flink部署在生产环境下 我们不会将程序发布到 3.1 standalone 模式1 安装 修改 conf/flink-conf.yaml文件 2 修改 /conf/slaves文件 3 发送给另外的两台机器 执行shell脚本 1for i in &#123;3,4&#125;; do scp -r /flink atguigu@hadoop10$i:/opt/module/ ; done; 或者编写集群分发脚本 此时我发现我的xshell连接速度慢 莫慌 vi /etc/ssh/sshd_config 把 #UseDNS yes 修改为 UseDNS no 问题解决 启动集群 ./start-cluster.sh 192.168.2.102:2181 准备数据 hello.txt 1234hello sparkhello hadoophellothank you 修改程序 进行打包 StreamWordCount 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.mlz.flinkimport org.apache.flink.api.java.utils.ParameterToolimport org.apache.flink.streaming.api.scala.&#123;DataStream, StreamExecutionEnvironment&#125;/* * @创建人: MaLingZhao * @创建时间: 2019/10/22 * @描述： */object StreamWordCount &#123; def main(args: Array[String]): Unit = &#123; val params=ParameterTool.fromArgs(args) val host: String = params.get("host") val port :Int=params.getInt("port") //创建流处理环境 val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment //接收socket文本流 // val textDstream: DataStream[String] = env.socketTextStream("192.168.2.102", 7777) val textDstream: DataStream[String] = env.socketTextStream(host, port) // flatMap和Map需要引用的隐式转换 import org.apache.flink.api.scala._ //处理 分组并且sum聚合 val wordStream: DataStream[(String, Int)] = textDstream.flatMap(_.split(" ")) .filter(_.nonEmpty) //要求非空 .map((_, 1)) //分成一个 .keyBy(0) //通过第一个聚合 .sum(1) //求和 //打印 wordStream.print().setParallelism(2) //启动 env.execute() &#125;&#125; 打包可能会出现一些问题 可能是mavan打包的版本不一致造成的 也可能是maven仓库访问的速度太慢了 图形化界面提交jar包 点击submit 此时我是失败的因为端口没开 nc -lk 7088 此时 成功了 命令行提交 12../flink/bin/flink -c com.mlz.flink.StreamWordCount -p 2 /opt/software/flinkDemo-1.0-SNAPSHOT-jar-with-dependencies.jar --host 192.168.2.102:7077 --port 7077 -p 并行度是2 -c 指定class 我们一直输入数据 刷新浏览器 发现数据是不断变化的 命令行的测试是成功的 3.3 Yarn模式部署1 启动hadoop集群 2 启动yarn-session 1./yarn-session.sh -n 2 -s 2 -jm 1024 -tm 1024 -nm test -d 12../flink/bin/flink run -m yarn-cluster -c com.mlz.flink.StreamWordCount /opt/software/flinkDemo-1.0-SNAPSHOT-jar-with-dependencies.jar --host 192.168.2.102 --port 7077 -n(–container)：TaskManager的数量。 -s(–slots)： 每个TaskManager的slot数量，默认一个slot一个core，默认每个taskmanager的slot的个数为1，有时可以多一些taskmanager，做冗余。 -jm：JobManager的内存（单位MB)。 -tm：每个taskmanager的内存（单位MB)。 -nm：yarn 的appName(现在yarn的ui上的名字)。 -d：后台执行。 3.4 Kubernetes部署这里不做介绍 尝试一下 4 flink的运行架构4. 1 任务的提交流程（Yarn架构） 图 Yarn模式任务提交流程 Flink任务提交后，Client向HDFS上传Flink的Jar包和配置，之后向Yarn ResourceManager提交任务，ResourceManager分配Container资源并通知对应的NodeManager启动ApplicationMaster，ApplicationMaster启动后加载Flink的Jar包和配置构建环境，然后启动JobManager，之后ApplicationMaster向ResourceManager申请资源启动TaskManager，ResourceManager分配Container资源后，由ApplicationMaster通知资源所在节点的NodeManager启动TaskManager，NodeManager加载Flink的Jar包和配置构建环境并启动TaskManager，TaskManager启动后向JobManager发送心跳包，并等待JobManager向其分配任务。 4.2任务的调度管理 图 任务调度原理 客户端不是运行时和程序执行的一部分，但它用于准备并发送dataflow(JobGraph)给Master(JobManager)，然后，客户端断开连接或者维持连接以等待接收计算结果。 当 Flink 集群启动后，首先会启动一个 JobManger 和一个或多个的 TaskManager。由 Client 提交任务给 JobManager，JobManager 再调度任务到各个 TaskManager 去执行，然后 TaskManager 将心跳和统计信息汇报给 JobManager。TaskManager 之间以流的形式进行数据的传输。上述三者均为独立的 JVM 进程。 Client行在任何机器上（与 JobManager 环境连通即可）。提交 Job 后，Client 可以结束进程（Streaming的任务），也可以不结束并等待结果返回。 JobManager 主要负责调度 Job 并协调 Task 做 checkpoint，职责上很像 Storm 的 Nimbus。从 Client 处接收到 Job 和 JAR 包等资源后，会生成优化后的执行计划，并以 Task 的单元调度到各个 TaskManager 去执行。 TaskManager 在启动的时候就设置好了槽位数（Slot），每个 slot 能启动一个 Task，Task 为线程。从 JobManager 处接收需要部署的 Task，部署启动后，与自己的上游建立 Netty 连接，接收数据并处理。 关于执行图 四层：StreamGraph -&gt; JobGraph -&gt; ExecutionGraph -&gt; \物理执行图**。 \StreamGraph**：是根据用户通过 Stream API 编写的代码生成的最初的图。用来表示程序的拓扑结构。 \JobGraph**：StreamGraph经过优化后生成了 JobGraph，提交给 JobManager 的数据结构。主要的优化为，将多个符合条件的节点 chain 在一起作为一个节点，这样可以减少数据在节点之间流动所需要的序列化/反序列化/传输消耗。 \ExecutionGraph**：JobManager 根据 JobGraph 生成ExecutionGraph。ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构。 \物理执行图**：JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构。 Worker与Solts 它可能会在独立的线程上执行一个或多个subtask****。为了控制一个worker能接收多少个task，worker通过task slot来进行控制（一个worker至少有一个task slot）。· 每个task slot表示TaskManager拥有资源的一个固定大小的子集。假如一个TaskManager有三个slot，那么它会将其管理的内存分成三份给各个slot。资源slot化意味着一个subtask将不需要跟来自其他job的subtask竞争被管理的内存，取而代之的是它将拥有一定数量的内存储备。需要注意的是，这里不会涉及到CPU的隔离，slot目前仅仅用来隔离task的受管理的内存。 通过调整task slot的数量，允许用户定义subtask之间如何互相隔离。如果一个TaskManager一个slot，那将意味着每个task group运行在独立的JVM中（该JVM可能是通过一个特定的容器启动的），而一个TaskManager多个slot意味着更多的subtask可以共享同一个JVM。而在同一个JVM进程中的task将共享TCP连接（基于多路复用）和心跳消息。它们也可能共享数据集和数据结构，因此这减少了每个task的负载。 图 TaskManager与Slot TaskSolt是静态的概念，是指TaskManager具有的并发执行能力*，可以通过参数taskmanager.numberOfTaskSlots进行配置，而**并行度parallelism是动态概念，即TaskManager运行程序时实际使用的并发能力****，可以通过参数parallelism.default进行配置。 也就是说，假设一共有3个TaskManager，每一个TaskManager中的分配3个TaskSlot，也就是每个TaskManager可以接收3个task，一共9个TaskSlot，如果我们设置parallelism.default=1，即运行程序默认的并行度为1，9个TaskSlot只用了1个，有8个空闲，因此，设置合适的并行度才能提高效率。 4 .4并行数据流Flink程序的执行具有并行、分布式的特性。在执行过程中，一个 stream 包含一个或多个 stream partition ，而每一个 operator 包含一个或多个 operator subtask，这些operator subtasks在不同的线程、不同的物理机或不同的容器中彼此互不依赖得执行。 一个特定operator的subtask的个数被称之为其parallelism(并行度\。一个stream的并行度总是等同于其producing operator的并行度。一个程序中，不同的operator可能具有不同的并行度。 图 并行数据流 Stream在operator之间传输数据的形式可以是one-to-one(forwarding)的模式也可以是redistributing的模式，具体是哪一种形式，取决于operator的种类。 \One-to-one**：\stream(比如在source和map operator之间)维护着分区以及元素的顺序**。那意味着map operator的subtask看到的元素的个数以及顺序跟source operator的subtask生产的元素的个数、顺序相同，map、fliter、flatMap等算子都是one-to-one的对应关系。 Ø \类似于s******park****中的窄依赖**** \Redistributing**：\stream(map()跟keyBy/window之间或者keyBy/window跟sink之间)的分区会发生改变**。每一个operator subtask依据所选择的transformation发送数据到不同的目标subtask。例如，keyBy() 基于hashCode重分区、broadcast和rebalance会随机重新分区，这些算子都会引起redistribute过程，而redistribute过程就类似于Spark中的shuffle过程。 Ø \类似于s******park****中的宽依赖**** 4.5 tsak 与operation chains\相同并行度的******one to one****操作****，Flink这样相连的operator** **链接在一起形成一个task，原来的operator成为里面的s****ubtask*。将operators链接成task是非常有效的优化：*它能减少线程之间的切换和基于缓存区的数据交换，在减少时延的同时提升吞吐量****。链接的行为可以在编程API中进行指定。 图 task与operator chains OperatorChain的优点 Ø 减少线程切换 Ø 减少序列化与反序列化 Ø 减少延迟并且提高吞吐能力 OperatorChain 组成条件（重要） Ø 上下游算子并行度一致 Ø 上下游算子之间没有数据shuffle 5 spark的流处理API5.1 EnvironmentgetExecutionEnvironment1val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment 如果没有设置并行度，会以flink-conf.yaml中的配置为准，默认是1 创建一个执行环境，表示当前执行程序的上下文。 如果程序是独立调用的，则此方法返回本地执行环境；如果从命令行客户端调用程序以提交到集群，则此方法返回此集群的执行环境，也就是说，getExecutionEnvironment会根据查询运行的方式决定返回什么样的运行环境，是最常用的一种创建执行环境的方式。 createLocalEnvironment 返回本地执行环境，需要在调用时指定默认的并行度。 1val env = StreamExecutionEnvironment.createLocalEnvironment(1) createRemoteEnvironment返回集群执行环境，将Jar提交到远程服务器。需要在调用时指定JobManager的IP和端口号，并指定要在集群中运行的Jar包。 1val env = ExecutionEnvironment.createRemoteEnvironment("jobmanager-hostname", 6123,"C://jar//flink//wordcount.jar") val env = ExecutionEnvironment.createRemoteEnvironment(“jobmanager-hostname”, 6123,”C://jar//flink//wordcount.jar”)]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[头条推荐系统]]></title>
    <url>%2Fblog4%2F2019%2F10%2F22%2FTouTiaoXiangMu%2F</url>
    <content type="text"><![CDATA[项目介绍1.1 头条推荐业务架构介绍简介： 系统建立在头条的APP推荐系统建立在用户与海量文章之上， ，使用lambda大数据实时和离线计算整体架构，利用黑马头条用户在APP上的点击行为、浏览行为、收藏行为等建立用户与文章之间的画像关系，通过机器学习推荐算法进行智能推荐。增加热门文章和新文章的推荐占比，达到千人千面的用户推荐效果。 头条 主要推荐场景 首页频道推荐 文章相似结果 1.2 架构与业务流 1.3 开发环境搭建1 安装centos7 64位 2 搭建hadoop 集群 3 搭建spark集群 4 安装Hive 5 安装anaconda 6 安装python的执行环境 2.2 数据的迁移的需求 业务数据：111 112 113 114 推荐系统 137 138 139 导入数据 * 本地137 业务数据 docker exec -it mysql bash 数据库的表 2.3 sqoop迁移数据sqoop list-database –connect jdbs:mysql:// 迁移脚本的定期执行 crontab -e 便器contrab文件 Linux的shell输入命令 用户行为收集 埋点业务数据 项目实战 头条推荐系统 flink项目实战]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>实战</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vue]]></title>
    <url>%2Fblog4%2F2019%2F10%2F21%2Fvue%2F</url>
    <content type="text"><![CDATA[1 介绍基础学习 打包工具webpack 项目驱动的学习 1.1 什么是vue.js前端框架 最流行 最火 React是最流行的前端框架 React 开发网站 开发手机APp Vue语法也能用于进行手机app的开发 1.2 流行框架 1 . 提高开发效率 js —》》 jQuery 之类的的类库 —》》 前端模板引擎 —》 Angular.j/Vue.js 帮助我们减少不必要的操作（操作DOM） vue的核心概念 让用户更多的关注代码的逻辑 不用关心dom的操作 1.3 框架和库的区别框架 完整的解决方案 对项目的侵入性比较大 node中的express jquery –》》 Zeptno EJS art-template 1.4 MVC和WVVM的区别MVC是都断分层的概念 MVVM 是前端视图层的概念 前端分成了3部分 Model View ViewModel 2 配置Vscode的调试 安装插件open in browser和view in browser 2.2 配置h5的模板 搜索html 最后 改成 1234567891011121314151617181920212223242526272829303132333435363738394041424344&#123; // Place your snippets for html here. Each snippet is defined under a snippet name and has a prefix, body and // description. The prefix is what is used to trigger the snippet and the body will be expanded and inserted. Possible variables are: // $1, $2 for tab stops, $0 for the final cursor position, and $&#123;1:label&#125;, $&#123;2:another&#125; for placeholders. Placeholders with the // same ids are connected. // Example: // "Print to console": &#123; // "prefix": "log", // "body": [ // "console.log('$1');", // "$2" // ], // "description": "Log output to console" // &#125; "h5 template": &#123; "prefix": "vh", // 对应的是使用这个模板的快捷键 "body": [ "&lt;!DOCTYPE html&gt;", "&lt;html lang=\"en\"&gt;", "&lt;head&gt;", "\t&lt;meta charset=\"UTF-8\"&gt;", "\t&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;", "\t&lt;meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\"&gt;", "\t&lt;title&gt;Document&lt;/title&gt;", "\t&lt;script src=\"./lib/vue-2.4.0.js\"&gt;&lt;/script&gt;", "&lt;/head&gt;\n", "&lt;body&gt;", "\t&lt;div id =\"app\"&gt; &lt;/div&gt;\n", "\t&lt;script&gt;", "\t //创建Vue实例,得到 ViewModel", "\t var vm = new Vue(&#123;", "\t\tel: '#app',", "\t\tdata: &#123;&#125;,", "\t\tmethods: &#123;&#125;", "\t &#125;);", "\t&lt;/script&gt;", "&lt;/body&gt;\n", "&lt;/html&gt;" ], "description": "HT-H5" // 模板的描述 &#125; &#125; vh键出现代码段 2.3 设置vscode的自动保存否则每次使用都要保存 用户设置 搜索自动保存 2.4 配置vue插件vuetur 3 指令v-on 事件的修饰符 设置vscode v-model 实现双向数据绑定去他 最近发现vscode 的使用需花费一定的事件去学习和配置 算了 还是用idea 这个工具比vscode更加智能 解决idea写vue代码红色波浪线的问题 快速开发开始 习惯性模式 idea配置html模板 File-&gt;settings，进入Editor，找到File and Code Templates，配置完后确认就行 添加html的模板 简易计算器案例 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt; &lt;meta http-equiv="X-UA-Compatible" content="ie=edge"&gt; &lt;title&gt;Document&lt;/title&gt; &lt;script src="./lib/vue-2.4.0.js"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div id="app"&gt;&lt;input type="text" v-model="n1"&gt; &lt;select v-model="opt"&gt; &lt;option value="+"&gt;+&lt;/option&gt; &lt;option value="-"&gt;-&lt;/option&gt; &lt;option value="*"&gt;*&lt;/option&gt; &lt;option value="/"&gt;/&lt;/option&gt; &lt;/select&gt; &lt;input type="text" v-model="n2"&gt; &lt;input type="button" value="=" @click="calc"&gt; &lt;input type="text" v-model="result"&gt;&lt;/div&gt;&lt;script&gt; // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123; n1:0, n2:0, result:0, opt: '+' &#125;, methods: &#123; calc() &#123; //计算的方法 //逻辑 /* switch(this.opt) &#123; case '+' : this.result=parseInt(this.n1)+parseInt(this.n2) break; case '-' : this.result=parseInt(this.n1)-parseInt(this.n2) break; case '*' : this.result=parseInt(this.n1)*parseInt(this.n2) break; case '/' : this.result=parseInt(this.n1)/parseInt(this.n2) break; &#125; &#125; &#125; */ //在项目中不要经常用 投机取巧的方式 /* * 正式的开发中尽量少用 * */ var codeStr='parseInt(this.n1)' + this.opt+ 'parseInt(this.n2)' this.result=eval(codeStr) &#125; &#125; &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 3.1 解决idea中html中v-model @click没有提示的问题 1、File-&gt;setting-&gt;Inspections，右侧找到Html-&gt;Unknown HTML tag attribute，在右边Options-&gt;Custom HTML tag attributes里面添加如下代码 12@tap,@tap.stop,@tap.prevent,@tap.once,@click,@click.stop,@click.prevent,@click.once,@change,@change.lazy,@change.number,@change.trim,v-model,v-for,v-text,v-html,v-if,v-else-if,v-else,v-pre,v-once,v-bind,scoped,@keyup.enter,:class,:style,v-show,:key,@keyup,@submit,@submit.prevent,@mouseenter,@mouseleave,@mouseout,@mouseover 2、File-&gt;setting-&gt;File Types里面找到HTML,在Registered Patterns里面添加*.vue即可，如图所示 配置完成 3.5 idea的html的常用的快捷键123456789101112131415161718192021222324252627282930311、link:css 引入css文件 &lt;link rel="stylesheet" href=""&gt; 2、script:src 引入js文件 &lt;script src=""&gt;&lt;/script&gt; 3、ul+ ul以及一个li 4、script html中插入js 5、a:link &lt;a href=”http://”&gt;&lt;/a&gt; 6、html:5 h5结构 7、div.class1 class:class1 &lt;div class="class1"&gt;&lt;/div&gt; 8、div#id1 id:id1 &lt;div id="id1"&gt;&lt;/div&gt; 9、div.class1.class2 class:class1,class2 &lt;div class="class1 class2"&gt;&lt;/div&gt; 10、a.mail &lt;a href=''mailto:''&gt;&lt;/a&gt; 11、form:get get表单 12、input:hidden hidden输入框 13、head&gt;link:css head+link 14、p+P 两个p 15、p*3 3个p 16、ul&gt;li.item$*5 创建ul下有个li同时class分别为item1，item2...5个 17、pos position，组合可用pos:s|a|r|f 18、t top，组合a auto 19、r right，组合a 20、l left 21、b bottom 22、z z-index 23、fl float 24、cl clear 25、d display 组合可用n|b|i|ib 26、ov overflow 27、v visibility 28、zoo zoom:1 29、cp clip 30、bx box-sizing 31、bxsh:w -webkit-box-shadow:0 0 0 #000; 3.6 idea自定义html快捷键File —》》 Settings 搜索 Live Templates 定义即可 注意$start 3.9 搞定vscode代码段的格式安装如下插件 ESLint Prettier - Code formatter Vetur 设置v 3.10安装bootsreap插件强大的代码提示功能bootstrap 3和4插件 强大的代码提示功能 4 vue的指令介绍4.1 v-on v-text v-html指令直接使用v-bind 简化指令: 绑定的时候 拼接绑定内容 `:title=”btnTitle + ‘, 这是追加的内容’” v-on:click html页面 4.2 v-on123456789&lt;div id=&quot;app&quot;&gt; &lt;p&gt;&#123;&#123;info&#125;&#125;&lt;/p&gt; &lt;input type=&quot;button&quot; value=&quot;开启&quot; v-on:click=&quot;go&quot;&gt; &lt;input type=&quot;button&quot; value=&quot;停止&quot; v-on:click=&quot;stop&quot;&gt; &lt;/div&gt; 创建vue的实例 123456789101112131415161718192021222324252627282930313233343536373839404142434445// 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: &apos;#app&apos;, data: &#123; info: &apos;猥琐发育，别浪~！&apos;, intervalId: null &#125;, methods: &#123; go() &#123; // 如果当前有定时器在运行，则直接return if (this.intervalId != null) &#123; return; &#125; // 开始定时器 this.intervalId = setInterval(() =&gt; &#123; this.info = this.info.substring(1) + this.info.substring(0, 1); &#125;, 500); &#125;, stop() &#123; clearInterval(this.intervalId); &#125; &#125; &#125;); v-on 的缩写 @click.stop 组织冒泡 .prevent 阻止默认事件 .capture添加事件监听器使用事捕获模式 .self 只当事件在元素本身（不是子元素） 触发的时候回调 .once 事件只触发了一次 4.3 Vue指令 v-model和数据的双向绑定4.4 v-for指令和key属性迭代数组 迭代对象中的元素 迭代 数字 4.5 vue指令之v-if和v-showv-if 有更高的切换消耗，v-show有更高的初始渲染消耗，因此需要频繁切换v-show比较好，如果再运行的条件不大可能改变v-if 比较好 4.6 Vue的调试工具vue-tools的安装https://github.com/vuejs/vue-devtools 按照指定的操作即可 本人不能翻墙 翻墙的话直接在google商店安装即可， 但是就算是在google商店安装的也可能会出现一些问题 以下是我的安装的步骤 1git clone https://github.com/vuejs/vue-devtools.git npm的版本6+或者3+ 123456$ cd vue-devtools/npm installnpm run dev 查看localhost:8100 火狐访问 5 过滤器的使用5.1全局过滤器123Vue.filetr(xxxx()&#123;&#125;) 5.2私有过滤器私有过滤器只能在当前VM对象控制的View区域进行使用 在vue实例中添加filters选项 1filters:&#123;&#125; 注意：当有局部和全局两个名称相同的过滤器时候，会以就近原则进行调用，即：局部过滤器优先于全局过滤器被调用！ 6vue组件全局组件的定义的3种方式 使用 Vue.extend 配合 Vue.component 方法： 1234var login = Vue.extend(&#123; template: &apos;&lt;h1&gt;登录&lt;/h1&gt;&apos; &#125;); Vue.component(&apos;login&apos;, login); 直接使用 Vue.component 方法： 123Vue.component(&apos;register&apos;, &#123; template: &apos;&lt;h1&gt;注册&lt;/h1&gt;&apos; &#125;); 将模板字符串，定义到script标签种： 123&lt;script id=&quot;tmpl&quot; type=&quot;x-template&quot;&gt; &lt;div&gt;&lt;a href=&quot;#&quot;&gt;登录&lt;/a&gt; | &lt;a href=&quot;#&quot;&gt;注册&lt;/a&gt;&lt;/div&gt; &lt;/script&gt; 同时，需要使用 Vue.component 来定义组件： 123Vue.component(&apos;account&apos;, &#123; template: &apos;#tmpl&apos; &#125;); 注意： 组件中的DOM结构，有且只能有唯一的根元素（Root Element）来进行包裹！ 组件中展示数据和响应事件 使用 Vue.extend 配合 Vue.component 方法： 1234var login = Vue.extend(&#123; template: &apos;&lt;h1&gt;登录&lt;/h1&gt;&apos; &#125;); Vue.component(&apos;login&apos;, login); 直接使用 Vue.component 方法： 123Vue.component(&apos;register&apos;, &#123; template: &apos;&lt;h1&gt;注册&lt;/h1&gt;&apos; &#125;); 将模板字符串，定义到script标签种： 123&lt;script id=&quot;tmpl&quot; type=&quot;x-template&quot;&gt; &lt;div&gt;&lt;a href=&quot;#&quot;&gt;登录&lt;/a&gt; | &lt;a href=&quot;#&quot;&gt;注册&lt;/a&gt;&lt;/div&gt; &lt;/script&gt; 同时，需要使用 Vue.component 来定义组件： 123Vue.component(&apos;account&apos;, &#123; template: &apos;#tmpl&apos; &#125;); 注意： 组件中的DOM结构，有且只能有唯一的根元素（Root Element）来进行包裹！ 组件中展示数据和响应事件 在组件中，data需要被定义为一个方法，例如： 12345678910111213Vue.component(&apos;account&apos;, &#123; template: &apos;#tmpl&apos;, data() &#123; return &#123; msg: &apos;大家好！&apos; &#125; &#125;, methods:&#123; login()&#123; alert(&apos;点击了登录按钮&apos;); &#125; &#125; &#125;); 在子组件中，如果将模板字符串，定义到了script标签中，那么，要访问子组件身上的data属性中的值，需要使用this来访问； 【重点】为什么组件中的data属性必须定]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>前端</tag>
        <tag>vue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试突击]]></title>
    <url>%2Fblog4%2F2019%2F10%2F20%2FjavaInterview2%2F</url>
    <content type="text"><![CDATA[了解自己的技术水平===================================================================（1）技术广度技术广度 为什么要考察一个人的技术广度呢假设 团队有一个系统 Dubbo作为服务框架 MQ（RocketMQ） kafka，SpringCloud缓存Redis 搜索（ElasticSearch） 在互联网行业 ，非互联网的公司，本省是有主流的技术栈的 假设，业务发挥在那特别的额迅猛需要团队扩招5个人，跟猎头合作找一些合适的人出来 最最起码的你应该从那个角度考察候选人 职位的最低要求工作经验在3到5年有一定的社会工作经验的 技术广度进行考察 招过来一个有几年经验的人，这样就不用再去培养了 候选人的整个技术栈是比较陪我们团队的技术展的从光度上吧各种技术都考察一下，尤其我们团队涉及到的技术 Dubbo你熟悉吗 之前你们公司也是 从服务的注册与发现他是怎么运行呢你们当时的服务的注册中心用的什么技术跟dubbo搭配起来的 看你的简历上写了RocketMQ 先说说你们公司为什么用rocketMQ集群架构 高可用是如何保证的rocketMQ核心的架构原理 工作原理当时有没有考虑过发送到rocketMQ的消息会丢失 ？ 缓存（Redis）整个集群怎么部署的 Redis的高可用 Redis为什么是单线程 高并发 聊聊ES, 你们的ES是如何优化的 到此为止，确定，然进来的话like就可以上手熟悉你们的架构，系统和代码，技术上不用特殊的培养，很快就可以上手干活，给予你们现有的架构，现有的技术栈，上手就可以开发各种业务功能模块 常见的技术方案涉及 常见的一些问题可以自己处理 常见的一些优化可以自己做20k差不多 jvm数据库和高并发 必考 我们希望你对jvm的基本原理都有一定的理解没如果你们的团队出现了内存溢出GC的问题希望你能独立的分析和解决 数据库 mysql 包括事务的问题常见的SQL 并发，java编程语言最基本的一个功能，并发本身有一定的深度和广度写出高效率的并发程序有一定的难度 （2） 项目经验 你平常用的各项技术 如何对业务进行落地 架构优化 生产实践是怎么来做的 分库分表 简历用了springdata -jdbc首先跟我说说你们的系统有那些库哪些表对应的是哪些业务呢新增的数据量有多少 单标百万级 千万级 你们是什么时候分表什么时候分的库 为什么在没有分表之前 sql的性能大该如何 分表之后sql的性能大概如何分库之前服务器上放多少GB的数据 ，一台服务器可以放所少数据？分库之后拆分到几台服务器上去？每台服务器方所少GB的数据 很多同学出去面试学了很多的技术，学习了很多的技术无理论是视频金科城 培训课程 买了一些书积累了很多的知识，Sharding_jdbc，分库分表的原理，常见的分库分表的技术方案，旺旺是为了面试去准备的技术，从来没有在项目中实践过，从你的项目落地的各种细节是如何实现的 面试的时候，旺旺被面试官问项目的各个细节，然后直接就死了 （3）生产经验 分布式 微服务 模块 你说你用网关 调研了哪几种技术，说说你们的的优缺点？ 最后你们是如何技术选型，你们系统的每天的访问呢量多高，你们的我那个管矿多高的GPS 网关是如何部署的 部署了几台机器 每台机器的饿配置如何 几个核 几个cpu 你的服务器增加了一个新的接口，你不能每次手动在网关里配置一些新的接口和服务的对应关系，网关的动态路由是怎么做的 每次上线服务或者断点额接口 跟你的网关动态路由是如何搭配起来的 线上机器 在生产环境下 访问压力下 平常的高峰期的CPU负载 如何有没有考虑过网观点额扩容 有没有进行请求路由的性能如何 一般请求一个w网关的路由的开销量有多少 现在的话，假设说，弯管方式在线上部署有没有遇到过什么问题，比如并发问题，新能问题？如果要对你们生产环境的网关进行高并发高性能的优化 你们是怎么做的呢 如果要做 从哪些角度入手做总结： 项目经验 + 生产经验 28k 30k 32k(5-8年)希望您能带一个小组，当一个小小的team leader 肯定希望你能将整个 项目经验 技术在 项目中如何落地带几个小弟，对那你负责的项目进行所有细节的把控 结合业务和项目的而细节考虑技术如何落地 生产经验：能够把控住项目部署之后的生产环境的情况对各种开发环境做出优化的手段全面负责自己的项目 很多的大厂3到5年经验也回来考察你的项目经验 越是大厂越希望你的技术能力强能够独挡一面 （4）技术的深度 你有没有读过哪些开源项目的源码 你能说一下rocketMQ的源码如果你精通技术的源码的话 为什么特别的有竞争力 技术深度 决定了技术的工地功底 你的声场环境随时肯可能会遇到异常kafka ES Dubbo RocketMQ 队列时可能会报错 RocketMQ异常无法写入消息ES突然巨慢 一次查询需要10几秒的时间 必须需要哪些金共同源码的同学，现场根据异常去分析技术的源码，从源码级别定位问题的所在，然后解决问题 大厂，很可能考察你的技术的深度，如果你没有那个技术的深度，那么可能你没有太大的技术优势 （5）系统设计简单了说，让你设计一个秒杀系统设计一个12306的火车票购票系统支撑10亿用户买火车票你会如何设计设计一个微信的红包系统越是大厂越是对你的要求高，独立的去负责一块东西能够把技术落地的更加的合理 大厂，独立的设计一块系统 独立设计一个小的架构 此时就会要求你有一定的独立的系统设计的能力30k,40k都会考察 30k,40k 50k更高薪资的 技术专家架构师要求本来在你们公司负责一大块系统的加厚生产经验 项目架构 技术广度 技术深度 系统架构 都很丰富 发布过《《互联网java工程师面试突击（第一季）》》spring springMVC Lucence Activity OA系统 财务系统 CRM系统 工厂管理类似这样的一些东西 国内的主流技术栈 MQ 消息第十 消息重复 高可用的部署 原理 缓存 数据库和缓存双鞋如何保证一致性分布式锁实现原理 分布式事务的常见方案 Dubbo SpringCloud 定位 面试突击第一季常见的主流的技术方案分析一下 扫盲 避免出去面试一问三不知 数据库的而一些原理优化 jvm的原理优化 并发的原理和优化 已经帮助了数以千计的同学，不完全统计看过面试第一季的同学至少5000人以上，快速扫盲，积累很多的互联网主流的技术栈，很多人发感谢信 收到反馈，面试突击第一季，但是还是有很多的问题，项目经验，生产经验技术的深度 面试突击第一季 常见的计数原理 常见的技术方案 项目经验 分库分表 死磕项目经验 (1) 项目经验 分库分表 死磕项目经验(2) 生产经验 Zuul网关在生产环如何优化 分布式锁会不会导致并发能力降低，如何优化分布式事务会不会导致TPS降低，如何优化，服务注册中心如果发现过慢如何解决怎么部署的需要多少台机器 （3）技术深度 kafka基本原理 rocketMQ基本原理 技术深度的话，kafka分底层的技术架构 推还是拉 生产者底层的网络通信机制 （4）面试现场给系统设计问题，让我结合哪些场景设计什么 github地址https://github.com/shihan100/Java-Interview-advanced 6季 分布式 微服务 海量数据 高性能 高并发高可用 每一集把对应的技术主题中的相关技术早项目中落地的细节 生产经验 架构经验技术深度 项目设计 每一季持续21天 ，每一周的周一和周五回更新课程 每天更新四讲 每天的额内容量在一小时左右总的课程时长大概在1000分钟左右 分析面试的要求 差距 第二天 分布式的Dubbo和SpringCloud的常见面试题简单的快速搭建 1 Dubbo和SpirngCloud 技术深度 2 服务注册中心与服务网关世间 网关 动态路由 超高并发 经典 典型 生产环境遇到的问题 画一画整个系统的架构图 接口 防重幂等性 分布式事务 具体业务落地 选型 原理 抗高并发 分布式锁 Redis 锁 zookeeper锁 你的分布式锁 淘宝京东库存是如何设计的 面试的常用的额项目经验 生产经验 技术深度 面试常见的技术问题 尽可能把学到的 东西落地到公司的项目中去 作业分析一下自己的差距和大厂的差距有多大结合自身的情自己况来分析一下 结合面试突击第一季的课程 面试突击第一季 技术广度的一些 ，各种技术（1）自己在技术广度上做的如何 对主流的技术栈哪些技术有一定的了解 包括核心原理和解决方案（2） 自己在项目经验生产经验上做的如何？你会的这些技术自己字啊项目中到底用到过多少用的到底有多复杂，用的时候考虑了哪些项目细节和生产细节 (3) 你现在对那些技术是除了核心原理以及基础知识之外，对一些技术的底层概念和院里有一定的了解 （4）系统设计，你目前自己独立负责过射及的消退给你何家沟有多复杂，如果让你来独立设计秒杀胸膛，红包系统，12306系统火者 其他的大型的架构 你会怎么去设计呢 能力 差距在哪里，第一周是预售周，除了四讲是看，其他的是不更新的，从第一周预售结束之后，可以把面试突击第一季巩固一下对自己的能力模型做一个详细的梳理自己现在的能力有多强 差距在哪里 技术深度 你对那些技术是除了核心原理和基础知识之外，对一些技术和概念有一定的理解 系统性分析大厂对工程师的要求，数据结构和算法 ，软素质，工程素养，履历北京，真正在找人的时候，会考虑很多的方面，技术深度，技术广度，项目经验，系统设计，技术上的要求，带团队管理 ======================================================================= 针对 面试突击第一季做的总结 面试第一季40到50讲 技术专题 回顾 为什么把系统拆分成哪个分布式的，为啥用dubbo？分布式的架构 拆分成了哪些架构 dubbo的工作原理是啥？注册中心挂了还可以继续通信吗 dubbo都支持哪些通信协议和序列化协议 dubbo支持哪些负载均衡、高可用以及动态代理的策略？ spm是啥意思？dubbo的spm机制是怎么玩的 基于dubbo如何做服务治理，服务降级以及容错 这些问题都是非常简单的问题 合格的工程师 大系统拆分成了多少子系统 多少服务框架多个服务 肯定用到服务框架 Dubbo GRPC SpringCloud Thirft服务的注册与发现负载均衡算法负载机制 负载策略通信协议请求超时 请求重试 分布式系统接口的幂等性该如何保证，比如不能重复扣款？ 分布式系统的结构调用如何保证顺序性 接口幂等性 服务的接口的 如果不保证 是否会会发生重复下单 重复加载之类的问题 如何设计一个类似dubbo的rpc框架，架构上该如何考虑自己看过一些 dubbo，springclodu 对一款服务框架有一定的了解和认识，此时如果说 它希望深入的考察你一下，看看你的水平，这个时候就有可能会问你这个问题 10 .说说zookeeper一般有哪些使用场景 分布式锁是啥？对比redis和zk两种分布式锁的优劣 拆分成了很多子系统，就说明有很多子系统在同时再运行， 如果两个子系统都需要对某个数据资源进行一系列复杂的操作在复杂的操作期间，不能让数据被其他任何人来改变，分布式锁，技术实现原理 说说你们的分布式session方案是啥？怎么做的 前后端分离 一般是前端来care session之类的问题 后端比较少了 了解分布式事务方案吗？你们咋做的？有啥坑？ 服务框架 服务注册中心 网关 最基本的分布式系统技术的调研和选型 落地如何做 分布式出现问题 ============================================================= SpringCloud基础分布式围绕dubbo来讨论的 最流行的是springcloud dubbo和springCloud 正在融合，SpringCloudAlibaba 只不过现在用的公司没有这么多作为合格的工程师 行业的主流的技术栈 Dubbo和SpringCLoud有的用SpirngCloud 不用Dubbo 用Dubbo不用SpringCloudjava工程师，dubbo和springCloud的基础原理都有一定的了解 大白话+现场画图 博客资料 书 第一天用非常通俗的语言 把一个系统如果用springcloud的分布式架构的额话，那么他需要用到SpringCLou的哪些组件 为什么 书 博客 demo 描述的不是很清楚 应该先讲解核心架构原理 Dubbo和SpringCLoud做两个最基本的工程 电商的工程搭建几个服务端的 SpringCLoud的核心架构原理我们一个电商系统 用户需要下单购买一些东西 订单系统 库存系统 仓储系统 、积分系统 不太可能说用单块的架构，电商消退给你支撑多少用户量 日活100用户还行 购买 注册的用户多的话 百万级用户 十万日货量 就不合适了 背后几十个人在开发此时单块系统是不合适的 重新梳理和明确一个概念 电商系统 拆分成了多个子系统 一次订单的请求 分布式系统 SpringCLoud的核心架构原理我们一个电商系统 用户需要下单购买一些东西订单系统 库存系统 仓储系统 、积分系统 不太可能说用单块的架构，电商消退给你支撑多少用户量 日活100用户购买 注册的用户多的话 百万级用户 十万日货量 就不合适了 背后几十个人在开发此时单块系统是不合适的 重新梳理和明确一个概念 电商系统 拆分成了多个子系统 一次订单的请求 思考问题 SpringCloud的核心技术 四个最基本的组件 Eureka 服务注册中心 Feign 服务调用 在底层将请求 转换成http的请求 Ribboon：负载均衡 Zuul/SpringCloud GateWay 网关 这么多的系统 电商系统包好了20个子系统 每个子系统里面有20个核心接口一共电商系统400个接口 直接对外暴露，前后端分离的架构，难道你让前端同学必须记住你的20个系统部署的机器的接口，让他们去负载均衡网关 Hystrix 链路追踪 stream 很多组件 Hystrix 高可用的环节 一个普通的系统 没有用好的话 反而会出问题 必须设计对应一整套的方案 限流方案，熔断方案 配合降级机制 微服务 网关 灰度发布 统一限流 统一授权认证网关将所有的接口配置在网关里 前端不用管库存，xxxx，yyyyy部署到什么地方 dubbo的基础架构 动态代理 Proxy 负载均衡：Cluster，负载均衡，故障转移 注册中心：Registry 通信协议：Protocol，filter机制，http、rmi、dubbo等协议 http、rmi、dubbo 网络通信：Transport，netty、mina 序列化：封装好的请求如何序列化成二进制数组，通过netty/mina发送出去 网络通信：Transport，基于netty/mina实现的Server 信息交换：Exchange，Response 通信协议：Protocol，filter机制 动态代理：Proxy 网络通信的一些东西，是如何通过NIO的方式，多线程的方式，让一个服务提供者被多个服务消费者去并发的调用和请求 从整体架构原理的角度，说了一下如何进行扩展的 Dubbo一次服务请求调用，牵扯到了哪些组件，负载均衡组件、注册中心、协议层、转换层、网络层（netty开发）、动态代理，服务提供者也是类似的 你对dubbo真的熟悉吗dubbo 负载均衡策略和集群容错策略都有哪些？动态代理策略呢？ dubbo 工作原理：服务注册、注册中心、消费者、代理通信、负载均衡； 网络通信、序列化：dubbo 协议、长连接、NIO、hessian 序列化协议； 负载均衡策略、集群容错策略、动态代理策略：dubbo 跑起来的时候一些功能是如何运转的？怎么做负载均衡？怎么做集群容错？怎么生成动态代理？ dubbo SPI 机制：你了解不了解 dubbo 的 SPI 机制？如何基于 SPI 机制对 dubbo 进行扩展？ dubbo 负载均衡策略1 random loadbalance默认情况下，dubbo 是 random load balance ，即随机调用实现负载均衡，可以对 provider 不同实例设置不同的权重，会按照权重来负载均衡，权重越大分配流量越高，一般就用这个默认的就可以了。 roundrobin loadbalance 这个的话默认就是均匀地将流量打到各个机器上去，但是如果各个机器的性能不一样，容易导致性能差的机器负载过高。所以此时需要调整权重，让性能差的机器承载权重小一些，流量少一些。 跟运维同学申请机器，有的时候，我们运气好，正好公司资源比较充足，刚刚有一批热气腾腾、刚刚做好的虚拟机新鲜出炉，配置都比较高：8 核 + 16G 机器，申请到 2 台。过了一段时间，我们感觉 2 台机器有点不太够，我就去找运维同学说，“哥儿们，你能不能再给我一台机器”，但是这时只剩下一台 4 核 + 8G 的机器。我要还是得要。 这个时候，可以给两台 8 核 16G 的机器设置权重 4，给剩余 1 台 4 核 8G 的机器设置权重 2。 leastactive loadbalance这个就是自动感知一下，如果某个机器性能越差，那么接收的请求越少，越不活跃，此时就会给不活跃的性能差的机器更少的请求。 consistanthash loadbalance一致性 Hash 算法，相同参数的请求一定分发到一个 provider 上去，provider 挂掉的时候，会基于虚拟节点均匀分配剩余的流量，抖动不会太大。如果你需要的不是随机负载均衡，是要一类请求都到一个节点，那就走这个一致性 Hash 策略。 dubbo 集群容错策略failover cluster 模式失败自动切换，自动重试其他机器，默认就是这个，常见于读操作。（失败重试其它机器） 可以通过以下几种方式配置重试次数： 1&lt;dubbo:service retries="2" /&gt; 或者 1&lt;dubbo:reference retries="2" /&gt; 或者 123&lt;dubbo:reference&gt; &lt;dubbo:method name="findFoo" retries="2" /&gt;&lt;/dubbo:reference&gt; failfast cluster 模式一次调用失败就立即失败，常见于非幂等性的写操作，比如新增一条记录（调用失败就立即失败） failsafe cluster 模式并行调用多个 provider，只要一个成功就立即返回。常用于实时性要求比较高的读操作，但是会浪费更多的服务资源，可通过 forks=&quot;2&quot; 来设置最大并行数。 broadcacst cluster逐个调用所有的 provider。任何一个 provider 出错则报错（从2.1.0 版本开始支持）。通常用于通知所有提供者更新缓存或日志等本地资源信息。 dubbo动态代理策略默认使用 javassist 动态字节码生成，创建代理类。但是可以通过 spi 扩展机制配置自己的动态代理策略。 幂等性接口幂等性实现起来非常的简单 （1）数据库唯一索引（2）基于Redis实现一套幂等性防重框架 对于插入类的操作建议数据库表中设计一些唯一索引 你如果有一个订单被支付了，此时就要通知wms创建一个对应发货单，也是数据库里的一个表，仓库里的人会看到这个发货单，此时他就会根据发货单的信息从仓库里进行拣货，打包，封装，交给物流公司 发货单 id order_id 订单金额 发货地址 xxxx 对order_id就可以建立一个唯一索引，你插入发货单的时候，同一个order_id最多只能对应一个发货单，不可能说同样的一个order_id对应了多个发货单 订单服务 -&gt; wms服务，出现了重试，导致第二次请求再次让人家创建这个订单的发货单，create语句，order_id触发了唯一索引约束 扣减库存、累加积分，更新，很难通过数据库唯一索引来保证 基于Redis实现一套接口的防重框架 你得做一个类似spring mvc里的拦截器这样的东西，在这个拦截器里，他会拦截所有的请求，对所有的请求都会提取请求对应的参数，GET请求、POST请求、PUT请求，有些参数是跟在URL地址里的，?xx=xx&amp;xx=xx POST、PUT，可能是请求体里的，可能是一个JSON格式 把参数拼接在一起，作为key去redis中判断一下，是否存在这个key，之前附加这些参数的请求是否发起过，如果没有的话，此时就可以把这些参数+接口名称，作为一个key，存储到redis中去 然后呢，把请求放行，去执行这个请求 如果说人家重试再次发起一个这个请求，此时就可以判断出来，参数组成的key在redis中已经存在了，此时就不让执行这个请求了，认为是重复调用了 考虑很多问题，幂等不幂等，通用框架，需要一个公司所有的接口都按照指定的参数来传递，还有很多业务语义的问题 第一次发起一个请求，直接把请求key放入redis，但是他执行的过程中失败了，而且还阻塞了一段时间，此时人家再次重试发起第二次请求，这个时候按照上述的框架逻辑，就会把请求拦截下来了 到底是不是要对所有接口都开启这么一个东西呢？ 每个接口如果执行成功了之后，我可以设置一个每个接口调用的时候执行成功之后，做一个后拦截器，如果成功了，就把请求对应的参数拼接为key放入redis中 有没有可能是第一次请求发送过来，在执行过程中，时间长了，比如需要1.3秒才执行完毕；此时人家发现超过1s了，直接重试，第二次请求过来了，也在正常的执行 第一次请求1.3秒之后执行成功了，第二次请求也执行成功了 只要一个服务希望对自己的接口开启幂等性防重功能，就把你开发好的拦截器对应的jar包，通过maven引入一个依赖就可以了 中大型互联网公司里也没做一个统一的防重幂等框架，其实一般都是各个服务对自己核心的接口，如果要保证幂等性的话，每个服务根据自己的业务逻辑来实现，而且仅仅是对少数核心接口做幂等性保障 核心接口，库存服务，扣减库存接口 定制化的去针对接口开发幂等性的机制，比如说一旦库存扣减成功之后，就立马要写一条数据到redis里去，order_id_11356_stock_deduct，写入redis中，如果写入成功，就说明之前这个订单的库存扣减，没人执行过 但是如果此时有一些重试的请求过来了，调用了你的库存扣减接口，他同时也进行了库存的扣减，但是他用同样的一个key，order_id_11356_stock_deduct，写入redis中，此时会发现已经有人写过key，key已经存在了 此时你就应该直接对刚才的库存扣减逻辑做一个反向的回滚逻辑，update product_stock set stock = stock - 100，update product_stock set stock = stock + 100，反向逻辑，回滚掉，自己避免说重复扣减库存 核心接口，幂等性都是自己保证的，人家可能会重试调用你的接口，对于create类的操作，用唯一索引来保证；对update类的操作，建议在核心接口里基于自己的业务逻辑，配合上redis，来保证幂等性 Dubbo，Spring Cloud，服务注册中心，你们当时是怎么选型和调研的，你们最终是选择了哪块技术呢？你选择这块技术的原因和理由是什么呢？(1)服务注册发现的原理 集群模式 Eureka，peer-to-peer，部署一个集群，但是集群里每个机器的地位是对等的，各个服务可以向任何一个Eureka实例服务注册和服务发现，集群里任何一个Euerka实例接收到写请求之后，会自动同步给其他所有的Eureka实例 ZooKeeper，服务注册和发现的原理，Leader + Follower两种角色，只有Leader可以负责写也就是服务注册，他可以把数据同步给Follower，读的时候leader/follower都可以读 (2)一致性保障：CP or AP **CAP，C是一致性，A是可用性，P是分区容 CP，AP ZooKeeper是有一个leader节点会接收数据， 然后同步写其他节点，一旦leader挂了，要重新选举leader，这个过程里为了保证C，就牺牲了A，不可用一段时间，但是一个leader选举好了，那么就可以继续写数据了，保证一致性 (4)容量 zk，不适合大规模的服务实例，因为服务上下线的时候，需要瞬间推送数据通知到所有的其他服务实例，所以一旦服务规模太大，到了几千个服务实例的时候，会导致网络带宽被大量占用 eureka，也很难支撑大规模的服务实例，因为每个eureka实例都要接受所有的请求，实例多了压力太大，扛不住，也很难到几千服务实例 之前dubbo技术体系都是用zk当注册中心，spring cloud技术体系都是用eureka当注册中心这两种是运用最广泛的，但是现在很多中小型公司以spring cloud居多，所以后面基于eureka说一下服务注册中心的生产优化 Eureka是peer模式，可能还没同步数据过去，结果自己就死了，此时还是可以继续从别的机器上拉取注册表，但是看到的就不是最新的数据了，但是保证了可用性，强一致，最终一致性 3）服务注册发现的时效性 zk，时效性更好，注册或者是挂了，一般秒级就能感知到 eureka，默认配置非常糟糕，服务发现感知要到几十秒，甚至分钟级别，上线一个新的服务实例，到其他人可以发现他，极端情况下，可能要1分钟的时间，ribbon去获取每个服务上缓存的eureka的注册表进行负载均衡 服务故障，隔60秒才去检查心跳，发现这个服务上一次心跳是在60秒之前，隔60秒去检查心跳，超过90秒没有心跳，才会认为他死了，2分钟都过去 30秒，才会更新缓存，30秒，其他服务才会来拉取最新的注册表 三分钟都过去了，如果你的服务实例挂掉了，此时别人感知到，可能要两三分钟的时间，一两分钟的时间，很漫长 4)容量 zk，不适合大规模的服务实例，因为服务上下线的时候，需要瞬间推送数据通知到所有的其他服务实例，所以一旦服务规模太大，到了几千个服务实例的时候，会导致网络带宽被大量占用 eureka，也很难支撑大规模的服务实例，因为每个eureka实例都要接受所有的请求，实例多了压力太大，扛不住，也很难到几千服务实例 之前dubbo技术体系都是用zk当注册中心，spring cloud技术体系都是用eureka当注册中心这两种是运用最广泛的，但是现在很多中小型公司以spring cloud居多，所以后面基于eureka说一下服务注册中心的生产优化 （5）多机房、多数据中心、健康检查 eureka：peer-to-peer，每台机器都是高并发请求，有瓶颈 zookeeper：服务上下线，全量通知其他服务，网络带宽被打满，有瓶颈 分布式服务注册中心，分片存储服务注册表，横向扩容，每台机器均摊高并发请求，各个服务主动拉取，避免反向通知网卡被打满 RocketKMQ 核心交易链路，分布式事务框架 有些服务之间的调用是走异步的，下成功了订单之后，你会通知一个wms服务去发货，这个过程可以是异步的，可以是走一个MQ的，发送一个消息到MQ里去，由wms服务去从MQ里消费消息 MQ，消息中间件，面试突击第一季，刚开头我就讲过消息中间件的面试连环炮 可靠消息最终一致性方案，参考面试突击第一季 落地，RocketMQ来实现可靠消息最终一致性事务方案 Producer向RocketMQ发送一个half message RocketMQ返回一个half message success的响应给Producer，这个时候就形成了一个half message了，此时这个message是不能被消费的 注意，这个步骤可能会因为网络等原因失败，可能你没收到RocketMQ返回的响应，那么就需要重试发送half message，直到一个half message成功建立为止 接着Producer本地执行数据库操作 Producer根据本地数据库操作的结果发送commit/rollback给RocketMQ，如果本地数据库执行成功，那么就发送一个commit给RocketMQ，让他把消息变为可以被消费的；如果本地数据库执行失败，那么就发送一个rollback给RocketMQ，废弃之前的message 注意，这个步骤可能失败，就是Producer可能因为网络原因没成功发送commit/rollback给RocketMQ，此时RocketMQ自己过一段时间发现一直没收到message的commit/rollback，就回调你服务提供的一个接口 此时在这个接口里，你需要自己去检查之前执行的本地数据库操作是否成功了，然后返回commit/rollback给RocketMQ 只要message被commit了，此时下游的服务就可以消费到这个消息，此时还需要结合ack机制，下游消费必须是消费成功了返回ack给RocketMQ，才可以认为是成功了，否则一旦失败没有ack，则必须让RocketMQ重新投递message给其他consumer TCC框架，bytetcc，seata seata-server bytetcc，大家就是基于mysql里面创建一些表，基于表中的数据进行状态的更新 核心链路中的各个服务都需要跟TC这个角色进行频繁的网络通信，频繁的网络通信其实就会带来性能的开销，本来一次请求不引入分布式事务只需要100ms，此时引入了分布式事务之后可能需要耗费200ms 网络请求可能还挺耗时的，上报一些分支事务的状态给TC，seata-server，选择基于哪种存储来放这些分布式事务日志或者状态的，file，磁盘文件，MySQL，数据库来存放对应的一些状态 高并发场景下，会不会有问题，seata-server，你也需要支持扩容，也需要部署多台机器，用一个数据库来存放分布式事务的日志和状态的话，假设并发量每秒上万，分库分表，对TC背后的数据库也会有同样的压力 这个时候对TC背后的db也得进行分库分表，抗更高的并发压力 为什么要进行系统拆分？如何进行系统拆分？拆分后不用 dubbo 可以吗？从这个问题开始就进行分布式系统环节了，现在出去面试分布式都成标配了，没有哪个公司不问问你分布式的事儿。你要是不会分布式的东西，简直这简历没法看，没人会让你去面试。 其实为啥会这样呢？这就是因为整个大行业技术发展的原因。 早些年，印象中在 2010 年初的时候，整个 IT 行业，很少有人谈分布式，更不用说微服务，虽然很多 BAT 等大型公司，因为系统的复杂性，很早就是分布式架构，大量的服务，只不过微服务大多基于自己搞的一套框架来实现而已。 但是确实，那个年代，大家很重视 ssh2，很多中小型公司几乎大部分都是玩儿 struts2、spring、hibernate，稍晚一些，才进入了 spring mvc、spring、mybatis 的组合。那个时候整个行业的技术水平就是那样，当年 oracle 很火，oracle 管理员很吃香，oracle 性能优化啥的都是 IT 男的大杀招啊。连大数据都没人提，当年 OCP、OCM 等认证培训机构，火的不行。 但是确实随着时代的发展，慢慢的，很多公司开始接受分布式系统架构了，这里面尤为对行业有至关重要影响的，是阿里的 dubbo，某种程度上而言，阿里在这里推动了行业技术的前进。 正是因为有阿里的 dubbo，很多中小型公司才可以基于 dubbo，来把系统拆分成很多的服务，每个人负责一个服务，大家的代码都没有冲突，服务可以自治，自己选用什么技术都可以，每次发布如果就改动一个服务那就上线一个服务好了，不用所有人一起联调，每次发布都是几十万行代码，甚至几百万行代码了。 直到今日，很高兴看到分布式系统都成行业面试标配了，任何一个普通的程序员都该掌握这个东西，其实这是行业的进步，也是所有 IT 码农的技术进步。所以既然分布式都成标配了，那么面试官当然会问了，因为很多公司现在都是分布式、微服务的架构，那面试官当然得考察考察你了。 为什么要将系统进行拆分？是不拆分，一个大系统几十万行代码，20 个人维护一份代码，简直是悲剧啊。代码经常改着改着就冲突了，各种代码冲突和合并要处理，非常耗费时间；经常我改动了我的代码，你调用了我的，导致你的代码也得重新测试，麻烦的要死；然后每次发布都是几十万行代码的系统一起发布，大家得一起提心吊胆准备上线，几十万行代码的上线，可能每次上线都要做很多的检查，很多异常问题的处理，简直是又麻烦又痛苦；而且如果我现在打算把技术升级到最新的 spring 版本，还不行，因为这可能导致你的代码报错，我不敢随意乱改技术。 假设一个系统是 20 万行代码，其中 A 在里面改了 1000 行代码，但是此时发布的时候是这个 20 万行代码的大系统一块儿发布。就意味着 20 万上代码在线上就可能出现各种变化，20 个人，每个人都要紧张地等在电脑面前，上线之后，检查日志，看自己负责的那一块儿有没有什么问题。 A 就检查了自己负责的 1 万行代码对应的功能，确保 ok 就闪人了；结果不巧的是，A 上线的时候不小心修改了线上机器的某个配置，导致另外 B 和 C 负责的 2 万行代码对应的一些功能，出错了。 几十个人负责维护一个几十万行代码的单块应用，每次上线，准备几个礼拜，上线 -&gt; 部署 -&gt; 检查自己负责的功能。 拆分了以后，整个世界清爽了，几十万行代码的系统，拆分成 20 个服务，平均每个服务就 1~2 万行代码，每个服务部署到单独的机器上。20 个工程，20 个 git 代码仓库，20 个开发人员，每个人维护自己的那个服务就可以了，是自己独立的代码，跟别人没关系。再也没有代码冲突了，爽。每次就测试我自己的代码就可以了，爽。每次就发布我自己的一个小服务就可以了，爽。技术上想怎么升级就怎么升级，保持接口不变就可以了，真爽。 所以简单来说，一句话总结，如果是那种代码量多达几十万行的中大型项目，团队里有几十个人，那么如果不拆分系统，开发效率极其低下，问题很多。但是拆分系统之后，每个人就负责自己的一小部分就好了，可以随便玩儿随便弄。分布式系统拆分之后，可以大幅度提升复杂系统大型团队的开发效率。 但是同时，也要提醒的一点是，系统拆分成分布式系统之后，大量的分布式系统面临的问题也是接踵而来，所以后面的问题都是在围绕分布式系统带来的复杂技术挑战在说。 如何进行系统拆分？这个问题说大可以很大，可以扯到领域驱动模型设计上去，说小了也很小，我不太想给大家太过于学术的说法，因为你也不可能背这个答案，过去了直接说吧。还是说的简单一点，大家自己到时候知道怎么回答就行了。 系统拆分为分布式系统，拆成多个服务，拆成微服务的架构，是需要拆很多轮的。并不是说上来一个架构师一次就给拆好了，而以后都不用拆。 第一轮；团队继续扩大，拆好的某个服务，刚开始是 1 个人维护 1 万行 如果是多人维护一个服务，最理想的情况下，几十个人，1 个人负责 1 个或 2~3 个服务；某个服务工作量变大了，代码量越来越多，某个同学，负责一个服务，代码量变成了 10 万行了，他自己不堪重负，他现在一个人拆开，5 个服务，1 个人顶着，负责 5 个人，接着招人，2 个人，给那个同学带着，3 个人负责 5 个服务，其中 2 个人每个人负责 2 个服务，1 个人负责 1 个服务。 个人建议，一个服务的代码不要太多，1 万行左右，两三万撑死了吧 大部分的系统，是要进行多轮拆分的，第一次拆分，可能就是将以前的多个模块该拆分开来了，比如说将电商系统拆分成订单系统、商品系统、采购系统、仓储系统、用户系统，等等吧。 但是后面可能每个系统又变得越来越复杂了，比如说采购系统里面又分成了供应商管理系统、采购单管理系统，订单系统又拆分成了购物车系统、价格系统、订单管理系统。 扯深了实在很深，所以这里先给大家举个例子，你自己感受一下，核心意思就是根据情况，先拆分一轮，后面如果系统更复杂了，可以继续分拆。你根据自己负责系统的例子，来考虑一下就好了。 拆分后不用 dubbo 可以吗？当然可以了，大不了最次，就是各个系统之间，直接基于 spring mvc，就纯 http 接口互相通信呗，还能咋样。但是这个肯定是有问题的，因为 http 接口通信维护起来成本很高，你要考虑超时重试、负载均衡等等各种乱七八糟的问题，比如说你的订单系统调用商品系统，商品系统部署了 5 台机器，你怎么把请求均匀地甩给那 5 台机器？这不就是负载均衡？你要是都自己搞那是可以的，但是确实很痛苦。 所以 dubbo 说白了，是一种 rpc 框架，就是说本地就是进行接口调用，但是 dubbo 会代理这个调用请求，跟远程机器网络通信，给你处理掉负载均衡、服务实例上下线自动感知、超时重试等等乱七八糟的问题。那你就不用自己做了，用 dubbo 就可以了。 如果有问题，结合你的业务，如何基于唯一索引、redis定制化防重机制 自己哪个业务可以用分布式锁？用什么框架？有什么生产问题？ 部署，机器配置，大概能抗多少并发；流量、QPS、性能，metrics；压测，借助一些小工具；扩容方案，横向加机器，还是说纵向提升机器的配置 zookeeper 都有哪些使用场景？分布式锁这个东西，很常用的，你做 Java 系统开发，分布式系统，可能会有一些场景会用到。最常用的分布式锁就是基于 zookeeper 来实现的。 其实说实话，问这个问题，一般就是看看你是否了解 zookeeper，因为 zookeeper 是分布式系统中很常见的一个基础系统。而且问的话常问的就是说 zookeeper 的使用场景是什么？看你知道不知道一些基本的使用场景。但是其实 zookeeper 挖深了自然是可以问的很深很深的。 大致来说，zookeeper 的使用场景如下，我就举几个简单的，大家能说几个就好了： 分布式协调 分布式锁 元数据/配置信息管理 HA高可用性 分布式协调 这个其实是 zookeeper 很经典的一个用法，简单来说，就好比，你 A 系统发送个请求到 mq，然后 B 系统消息消费之后处理了。那 A 系统如何知道 B 系统的处理结果？用 zookeeper 就可以实现分布式系统之间的协调工作。A 系统发送请求之后可以在 zookeeper 上对某个节点的值注册个监听器，一旦 B 系统处理完了就修改 zookeeper 那个节点的值，A 系统立马就可以收到通知，完美解决。 分布式锁 对某一个数据连续发出两个修改操作，两台机器同时收到了请求，但是只能一台机器先执行完另外一个机器再执行。那么此时就可以使用 zookeeper 分布式锁，一个机器接收到了请求之后先获取 zookeeper 上的一把分布式锁，就是可以去创建一个 znode，接着执行操作；然后另外一个机器也尝试去创建那个 znode，结果发现自己创建不了，因为被别人创建了，那只能等着，等第一个机器执行完了自己再执行。 元数据/配置信息管理 zookeeper 可以用作很多系统的配置信息的管理，比如 kafka、storm 等等很多分布式系统都会选用 zookeeper 来做一些元数据、配置信息的管理，包括 dubbo 注册中心不也支持 zookeeper 么？ HA高可用性 这个应该是很常见的，比如 hadoop、hdfs、yarn 等很多大数据系统，都选择基于 zookeeper 来开发 HA 高可用机制，就是一个重要进程一般会做主备两个，主进程挂了立马通过 zookeeper 感知到切换到备用进程。 分布式锁的原理 羊群效应 如果几十个客户端同时争抢一个锁，此时会导致任何一个客户端释放锁的时候，zk反向通知几十个客户端，几十个客户端又要发送请求到zk去尝试创建锁，所以大家会发现，几十个人要加锁，大家乱糟糟的，无序的 羊群效应 造成很多没必要的请求和网络开销，会加重网络的负载 Redis和ZooKeeper，哪种分布式锁更好？ 从分布式系统协调语义而言，是ZooKeeper做分布式锁更好一些，因为Redis本身其实是缓存，但是Redis能抗高并发，高并发场景下更好一些 zookeeper本身不适合部署大规模集群，他本身适用的场景就是部署三五台机器，不是承载高并发请求的，仅仅是用作分布式系统的协调的 Redis？ZooKeeper？ 有redis集群，没有zookeeper集群，那你当然就选择redis了；如果你们公司两个都有，用哪种分布式锁都可以，高并发场景，redis 分布式锁脑裂，重复加锁 分布式系统，主控节点有一个Master，此时因为网络故障，导致其他人以为这个Master不可用了，其他节点出现了别的Master，导致集群里有2个Master同时在运行 curator框架源码，加一些协调机制 高可用电商网站的商品详情页系统架构小型电商网站的商品详情页系统架构小型电商网站的页面展示采用页面全量静态化的思想。数据库中存放了所有的商品信息，页面静态化系统，将数据填充进静态模板中，形成静态化页面，推入 Nginx 服务器。用户浏览网站页面时，取用一个已经静态化好的 html 页面，直接返回回去，不涉及任何的业务逻辑处理。 下面是页面模板的简单 Demo 。 1234567&lt;html&gt; &lt;body&gt; 商品名称：#&#123;productName&#125;&lt;br&gt; 商品价格：#&#123;productPrice&#125;&lt;br&gt; 商品描述：#&#123;productDesc&#125; &lt;/body&gt;&lt;/html&gt; 这样做，好处在于，用户每次浏览一个页面，不需要进行任何的跟数据库的交互逻辑，也不需要执行任何的代码，直接返回一个 html 页面就可以了，速度和性能非常高。 对于小网站，页面很少，很实用，非常简单，Java 中可以使用 velocity、freemarker、thymeleaf 等等，然后做个 cms 页面内容管理系统，模板变更的时候，点击按钮或者系统自动化重新进行全量渲染。 坏处在于，仅仅适用于一些小型的网站，比如页面的规模在几十到几万不等。对于一些大型的电商网站，亿级数量的页面，你说你每次页面模板修改了，都需要将这么多页面全量静态化，靠谱吗？每次渲染花个好几天时间，那你整个网站就废掉了。 大型电商网站的商品详情页系统架构大型电商网站商品详情页的系统设计中，当商品数据发生变更时，会将变更消息压入 MQ 消息队列中。缓存服务从消息队列中消费这条消息时，感知到有数据发生变更，便通过调用数据服务接口，获取变更后的数据，然后将整合好的数据推送至 redis 中。Nginx 本地缓存的数据是有一定的时间期限的，比如说 10 分钟，当数据过期之后，它就会从 redis 获取到最新的缓存数据，并且缓存到自己本地。 用户浏览网页时，动态将 Nginx 本地数据渲染到本地 html 模板并返回给用户。 虽然没有直接返回 html 页面那么快，但是因为数据在本地缓存，所以也很快，其实耗费的也就是动态渲染一个 html 页面的性能。如果 html 模板发生了变更，不需要将所有的页面重新静态化，也不需要发送请求，没有网络请求的开销，直接将数据渲染进最新的 html 页面模板后响应即可。 在这种架构下，我们需要保证系统的高可用性。 如果系统访问量很高，Nginx 本地缓存过期失效了，redis 中的缓存也被 LRU 算法给清理掉了，那么会有较高的访问量，从缓存服务调用商品服务。但如果此时商品服务的接口发生故障，调用出现了延时，缓存服务全部的线程都被这个调用商品服务接口给耗尽了，每个线程去调用商品服务接口的时候，都会卡住很长时间，后面大量的请求过来都会卡在那儿，此时缓存服务没有足够的线程去调用其它一些服务的接口，从而导致整个大量的商品详情页无法正常显示。 这其实就是一个商品接口服务故障导致缓存服务资源耗尽的现象。 深入 Hystrix 断路器执行原理RequestVolumeThreshold12HystrixCommandProperties.Setter() .withCircuitBreakerRequestVolumeThreshold(int) 表示在滑动窗口中，至少有多少个请求，才可能触发断路。 Hystrix 经过断路器的流量超过了一定的阈值，才有可能触发断路。比如说，要求在 10s 内经过断路器的流量必须达到 20 个，而实际经过断路器的流量才 10 个，那么根本不会去判断要不要断路。 ErrorThresholdPercentage12HystrixCommandProperties.Setter() .withCircuitBreakerErrorThresholdPercentage(int) 表示异常比例达到多少，才会触发断路，默认值是 50(%)。 如果断路器统计到的异常调用的占比超过了一定的阈值，比如说在 10s 内，经过断路器的流量达到了 30 个，同时其中异常访问的数量也达到了一定的比例，比如 60% 的请求都是异常（报错 / 超时 / reject），就会开启断路。 SleepWindowInMilliseconds12HystrixCommandProperties.Setter() .withCircuitBreakerSleepWindowInMilliseconds(int) 断路开启，也就是由 close 转换到 open 状态（close -&gt; open）。那么之后在 SleepWindowInMilliseconds 时间内，所有经过该断路器的请求全部都会被断路，不调用后端服务，直接走 fallback 降级机制。 而在该参数时间过后，断路器会变为 half-open 半开闭状态，尝试让一条请求经过断路器，看能不能正常调用。如果调用成功了，那么就自动恢复，断路器转为 close 状态。 Enabled12HystrixCommandProperties.Setter() .withCircuitBreakerEnabled(boolean) 控制是否允许断路器工作，包括跟踪依赖服务调用的健康状况，以及对异常情况过多时是否允许触发断路。默认值是 true。 ForceOpen12HystrixCommandProperties.Setter() .withCircuitBreakerForceOpen(boolean) 如果设置为 true 的话，直接强迫打开断路器，相当于是手动断路了，手动降级，默认值是 false。 ForceClosed12HystrixCommandProperties.Setter() .withCircuitBreakerForceClosed(boolean) 如果设置为 true，直接强迫关闭断路器，相当于手动停止断路了，手动升级，默认值是 false。 实例 DemoHystrixCommand 配置参数在 GetProductInfoCommand 中配置 Setter 断路器相关参数。 滑动窗口中，最少 20 个请求，才可能触发断路。 异常比例达到 40% 时，才触发断路。 断路后 3000ms 内，所有请求都被 reject，直接走 fallback 降级，不会调用 run() 方法。3000ms 过后，变为 half-open 状态。 run() 方法中，我们判断一下 productId 是否为 -1，是的话，直接抛出异常。这么写，我们之后测试的时候就可以传入 productId=-1，模拟服务执行异常了。 在降级逻辑中，我们直接给它返回降级商品就好了。 1234567891011121314151617181920212223242526272829303132333435363738394041public class GetProductInfoCommand extends HystrixCommand&lt;ProductInfo&gt; &#123; private Long productId; private static final HystrixCommandKey KEY = HystrixCommandKey.Factory.asKey("GetProductInfoCommand"); public GetProductInfoCommand(Long productId) &#123; super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey("ProductInfoService")) .andCommandKey(KEY) .andCommandPropertiesDefaults(HystrixCommandProperties.Setter() // 是否允许断路器工作 .withCircuitBreakerEnabled(true) // 滑动窗口中，最少有多少个请求，才可能触发断路 .withCircuitBreakerRequestVolumeThreshold(20) // 异常比例达到多少，才触发断路，默认50% .withCircuitBreakerErrorThresholdPercentage(40) // 断路后多少时间内直接reject请求，之后进入half-open状态，默认5000ms .withCircuitBreakerSleepWindowInMilliseconds(3000))); this.productId = productId; &#125; @Override protected ProductInfo run() throws Exception &#123; System.out.println("调用接口查询商品数据，productId=" + productId); if (productId == -1L) &#123; throw new Exception(); &#125; String url = "http://localhost:8081/getProductInfo?productId=" + productId; String response = HttpClientUtils.sendGetRequest(url); return JSONObject.parseObject(response, ProductInfo.class); &#125; @Override protected ProductInfo getFallback() &#123; ProductInfo productInfo = new ProductInfo(); productInfo.setName("降级商品"); return productInfo; &#125;&#125; 断路测试类我们在测试类中，前 30 次请求，传入 productId=-1，然后休眠 3s，之后 70 次请求，传入 productId=1。 12345678910111213141516171819202122@SpringBootTest@RunWith(SpringRunner.class)public class CircuitBreakerTest &#123; @Test public void testCircuitBreaker() &#123; String baseURL = "http://localhost:8080/getProductInfo?productId="; for (int i = 0; i &lt; 30; ++i) &#123; // 传入-1，会抛出异常，然后走降级逻辑 HttpClientUtils.sendGetRequest(baseURL + "-1"); &#125; TimeUtils.sleep(3); System.out.println("After sleeping..."); for (int i = 31; i &lt; 100; ++i) &#123; // 传入1，走服务正常调用 HttpClientUtils.sendGetRequest(baseURL + "1"); &#125; &#125;&#125; 测试结果测试结果，我们可以明显看出系统断路与恢复的整个过程。 12345678910111213141516调用接口查询商品数据，productId=-1ProductInfo(id=null, name=降级商品, price=null, pictureList=null, specification=null, service=null, color=null, size=null, shopId=null, modifiedTime=null, cityId=null, cityName=null, brandId=null, brandName=null)// ...// 这里重复打印了 20 次上面的结果ProductInfo(id=null, name=降级商品, price=null, pictureList=null, specification=null, service=null, color=null, size=null, shopId=null, modifiedTime=null, cityId=null, cityName=null, brandId=null, brandName=null)// ...// 这里重复打印了 8 次上面的结果// 休眠 3s 后调用接口查询商品数据，productId=1ProductInfo(id=1, name=iphone7手机, price=5599.0, pictureList=a.jpg,b.jpg, specification=iphone7的规格, service=iphone7的售后服务, color=红色,白色,黑色, size=5.5, shopId=1, modifiedTime=2017-01-01 12:00:00, cityId=1, cityName=null, brandId=1, brandName=null)// ...// 这里重复打印了 69 次上面的结果 前 30 次请求，我们传入的 productId 为 -1，所以服务执行过程中会抛出异常。我们设置了最少 20 次请求通过断路器并且异常比例超出 40% 就触发断路。因此执行了 21 次接口调用，每次都抛异常并且走降级，21 次过后，断路器就被打开了。 之后的 9 次请求，都不会执行 run() 方法，也就不会打印以下信息。 1调用接口查询商品数据，productId=-1 而是直接走降级逻辑，调用 getFallback() 执行。 休眠了 3s 后，我们在之后的 70 次请求中，都传入 productId 为 1。由于我们前面设置了 3000ms 过后断路器变为 half-open 状态。因此 Hystrix 会尝试执行请求，发现成功了，那么断路器关闭，之后的所有请求也都能正常调用了。 Hystrix 隔离策略细粒度控制Hystrix 实现资源隔离，有两种策略： 线程池隔离 信号量隔离 对资源隔离这一块东西，其实可以做一定细粒度的一些控制。 execution.isolation.strategy指定了 HystrixCommand.run() 的资源隔离策略：THREAD or SEMAPHORE，一种基于线程池，一种基于信号量。 12345// to use thread isolationHystrixCommandProperties.Setter().withExecutionIsolationStrategy(ExecutionIsolationStrategy.THREAD)// to use semaphore isolationHystrixCommandProperties.Setter().withExecutionIsolationStrategy(ExecutionIsolationStrategy.SEMAPHORE) 线程池机制，每个 command 运行在一个线程中，限流是通过线程池的大小来控制的；信号量机制，command 是运行在调用线程中，通过信号量的容量来进行限流。 如何在线程池和信号量之间做选择？ 默认的策略就是线程池。 线程池其实最大的好处就是对于网络访问请求，如果有超时的话，可以避免调用线程阻塞住。 而使用信号量的场景，通常是针对超大并发量的场景下，每个服务实例每秒都几百的 QPS，那么此时你用线程池的话，线程一般不会太多，可能撑不住那么高的并发，如果要撑住，可能要耗费大量的线程资源，那么就是用信号量，来进行限流保护。一般用信号量常见于那种基于纯内存的一些业务逻辑服务，而不涉及到任何网络访问请求。 command key &amp; command group我们使用线程池隔离，要怎么对依赖服务、依赖服务接口、线程池三者做划分呢？ 每一个 command，都可以设置一个自己的名称 command key，同时可以设置一个自己的组 command group。 1234567private static final Setter cachedSetter = Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey("ExampleGroup")) .andCommandKey(HystrixCommandKey.Factory.asKey("HelloWorld")); public CommandHelloWorld(String name) &#123; super(cachedSetter); this.name = name;&#125; command group 是一个非常重要的概念，默认情况下，就是通过 command group 来定义一个线程池的，而且还会通过 command group 来聚合一些监控和报警信息。同一个 command group 中的请求，都会进入同一个线程池中。 command thread poolThreadPoolKey 代表了一个 HystrixThreadPool，用来进行统一监控、统计、缓存。默认的 ThreadPoolKey 就是 command group 的名称。每个 command 都会跟它的 ThreadPoolKey 对应的 ThreadPool 绑定在一起。 如果不想直接用 command group，也可以手动设置 ThreadPool 的名称。 12345678private static final Setter cachedSetter = Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey("ExampleGroup")) .andCommandKey(HystrixCommandKey.Factory.asKey("HelloWorld")) .andThreadPoolKey(HystrixThreadPoolKey.Factory.asKey("HelloWorldPool"));public CommandHelloWorld(String name) &#123; super(cachedSetter); this.name = name;&#125; command key &amp; command group &amp; command thread poolcommand key ，代表了一类 command，一般来说，代表了底层的依赖服务的一个接口。 command group ，代表了某一个底层的依赖服务，这是很合理的，一个依赖服务可能会暴露出来多个接口，每个接口就是一个 command key。command group 在逻辑上去组织起来一堆 command key 的调用、统计信息、成功次数、timeout 超时次数、失败次数等，可以看到某一个服务整体的一些访问情况。一般来说，推荐根据一个服务区划分出一个线程池，command key 默认都是属于同一个线程池的。 比如说你以一个服务为粒度，估算出来这个服务每秒的所有接口加起来的整体 QPS 在 100 左右，你调用这个服务，当前这个服务部署了 10 个服务实例，每个服务实例上，其实用这个 command group 对应这个服务，给一个线程池，量大概在 10 个左右就可以了，你对整个服务的整体的访问 QPS 就大概在每秒 100 左右。 但是，如果说 command group 对应了一个服务，而这个服务暴露出来的几个接口，访问量很不一样，差异非常之大。你可能就希望在这个服务 command group 内部，包含的对应多个接口的 command key，做一些细粒度的资源隔离。就是说，对同一个服务的不同接口，使用不同的线程池。 123command key -&gt; command groupcommand key -&gt; 自己的 thread pool key 逻辑上来说，多个 command key 属于一个command group，在做统计的时候，会放在一起统计。每个 command key 有自己的线程池，每个接口有自己的线程池，去做资源隔离和限流。 说白点，就是说如果你的 command key 要用自己的线程池，可以定义自己的 thread pool key，就 ok 了。 coreSize设置线程池的大小，默认是 10。一般来说，用这个默认的 10 个线程大小就够了。 1HystrixThreadPoolProperties.Setter().withCoreSize(int value); queueSizeRejectionThreshold如果说线程池中的 10 个线程都在工作中，没有空闲的线程来做其它的事情，此时再有请求过来，会先进入队列积压。如果说队列积压满了，再有请求过来，就直接 reject，拒绝请求，执行 fallback 降级的逻辑，快速返回。 控制 queue 满了之后 reject 的 threshold，因为 maxQueueSize 不允许热修改，因此提供这个参数可以热修改，控制队列的最大大小。 1HystrixThreadPoolProperties.Setter().withQueueSizeRejectionThreshold(int value); execution.isolation.semaphore.maxConcurrentRequests设置使用 SEMAPHORE 隔离策略的时候允许访问的最大并发量，超过这个最大并发量，请求直接被 reject。 这个并发量的设置，跟线程池大小的设置，应该是类似的，但是基于信号量的话，性能会好很多，而且 Hystrix 框架本身的开销会小很多。 默认值是 10，尽量设置的小一些，因为一旦设置的太大，而且有延时发生，可能瞬间导致 tomcat 本身的线程资源被占满。 1HystrixCommandProperties.Setter().withExecutionIsolationSemaphoreMaxConcurrentRequests(int value); 基于本地缓存的 fallback 降级机制Hystrix 出现以下四种情况，都会去调用 fallback 降级机制： 断路器处于打开的状态。 资源池已满（线程池+队列 / 信号量）。 Hystrix 调用各种接口，或者访问外部依赖，比如 MySQL、Redis、Zookeeper、Kafka 等等，出现了任何异常的情况。 访问外部依赖的时候，访问时间过长，报了 TimeoutException 异常。 两种最经典的降级机制 纯内存数据在降级逻辑中，你可以在内存中维护一个 ehcache，作为一个纯内存的基于 LRU 自动清理的缓存，让数据放在缓存内。如果说外部依赖有异常，fallback 这里直接尝试从 ehcache 中获取数据。 默认值fallback 降级逻辑中，也可以直接返回一个默认值。 在 HystrixCommand，降级逻辑的书写，是通过实现 getFallback() 接口；而在 HystrixObservableCommand 中，则是实现 resumeWithFallback() 方法。 现在，我们用一个简单的栗子，来演示 fallback 降级是怎么做的。 比如，有这么个场景。我们现在有个包含 brandId 的商品数据，假设正常的逻辑是这样：拿到一个商品数据，根据 brandId 去调用品牌服务的接口，获取品牌的最新名称 brandName。 假如说，品牌服务接口挂掉了，那么我们可以尝试从本地内存中，获取一份稍过期的数据，先凑合着用。 步骤一：本地缓存获取数据本地获取品牌名称的代码大致如下。 12345678910111213141516171819202122/** * 品牌名称本地缓存 * */public class BrandCache &#123; private static Map&lt;Long, String&gt; brandMap = new HashMap&lt;&gt;(); static &#123; brandMap.put(1L, "Nike"); &#125; /** * brandId 获取 brandName * * @param brandId 品牌id * @return 品牌名 */ public static String getBrandName(Long brandId) &#123; return brandMap.get(brandId); &#125; 步骤二：实现 GetBrandNameCommand在 GetBrandNameCommand 中，run() 方法的正常逻辑是去调用品牌服务的接口获取到品牌名称，如果调用失败，报错了，那么就会去调用 fallback 降级机制。 这里，我们直接模拟接口调用报错，给它抛出个异常。 而在 getFallback() 方法中，就是我们的降级逻辑，我们直接从本地的缓存中，获取到品牌名称的数据。 1234567891011121314151617181920212223242526272829303132/** * 获取品牌名称的command * */public class GetBrandNameCommand extends HystrixCommand&lt;String&gt; &#123; private Long brandId; public GetBrandNameCommand(Long brandId) &#123; super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey("BrandService")) .andCommandKey(HystrixCommandKey.Factory.asKey("GetBrandNameCommand")) .andCommandPropertiesDefaults(HystrixCommandProperties.Setter() // 设置降级机制最大并发请求数 .withFallbackIsolationSemaphoreMaxConcurrentRequests(15))); this.brandId = brandId; &#125; @Override protected String run() throws Exception &#123; // 这里正常的逻辑应该是去调用一个品牌服务的接口获取名称 // 如果调用失败，报错了，那么就会去调用fallback降级机制 // 这里我们直接模拟调用报错，抛出异常 throw new Exception(); &#125; @Override protected String getFallback() &#123; return BrandCache.getBrandName(brandId); &#125;&#125; FallbackIsolationSemaphoreMaxConcurrentRequests 用于设置 fallback 最大允许的并发请求量，默认值是 10，是通过 semaphore 信号量的机制去限流的。如果超出了这个最大值，那么直接 reject。 步骤三：CacheController 调用接口在 CacheController 中，我们通过 productInfo 获取 brandId，然后创建 GetBrandNameCommand 并执行，去尝试获取 brandName。这里执行会报错，因为我们在 run() 方法中直接抛出异常，Hystrix 就会去调用 getFallback() 方法走降级逻辑。 123456789101112131415161718192021@Controllerpublic class CacheController &#123; @RequestMapping("/getProductInfo") @ResponseBody public String getProductInfo(Long productId) &#123; HystrixCommand&lt;ProductInfo&gt; getProductInfoCommand = new GetProductInfoCommand(productId); ProductInfo productInfo = getProductInfoCommand.execute(); Long brandId = productInfo.getBrandId(); HystrixCommand&lt;String&gt; getBrandNameCommand = new GetBrandNameCommand(brandId); // 执行会抛异常报错，然后走降级 String brandName = getBrandNameCommand.execute(); productInfo.setBrandName(brandName); System.out.println(productInfo); return "success"; &#125;&#125; 关于降级逻辑的演示，基本上就结束了。 用 Hystrix 构建高可用服务架构参考 Hystrix Home。 Hystrix 是什么？在分布式系统中，每个服务都可能会调用很多其他服务，被调用的那些服务就是依赖服务，有的时候某些依赖服务出现故障也是很正常的。 Hystrix 可以让我们在分布式系统中对服务间的调用进行控制，加入一些调用延迟或者依赖故障的容错机制。 Hystrix 通过将依赖服务进行资源隔离，进而阻止某个依赖服务出现故障时在整个系统所有的依赖服务调用中进行蔓延；同时Hystrix 还提供故障时的 fallback 降级机制。 总而言之，Hystrix 通过这些方法帮助我们提升分布式系统的可用性和稳定性。 Hystrix 的历史Hystrix 是高可用性保障的一个框架。Netflix（可以认为是国外的优酷或者爱奇艺之类的视频网站）的 API 团队从 2011 年开始做一些提升系统可用性和稳定性的工作，Hystrix 就是从那时候开始发展出来的。 在 2012 年的时候，Hystrix 就变得比较成熟和稳定了，Netflix 中，除了 API 团队以外，很多其他的团队都开始使用 Hystrix。 时至今日，Netflix 中每天都有数十亿次的服务间调用，通过 Hystrix 框架在进行，而 Hystrix 也帮助 Netflix 网站提升了整体的可用性和稳定性。 2018 年 11 月，Hystrix 在其 Github 主页宣布，不再开放新功能，推荐开发者使用其他仍然活跃的开源项目。维护模式的转变绝不意味着 Hystrix 不再有价值。相反，Hystrix 激发了很多伟大的想法和项目，我们高可用的这一块知识还是会针对 Hystrix 进行讲解。 Hystrix 的设计原则 对依赖服务调用时出现的调用延迟和调用失败进行控制和容错保护。 在复杂的分布式系统中，阻止某一个依赖服务的故障在整个系统中蔓延。比如某一个服务故障了，导致其它服务也跟着故障。 提供 fail-fast（快速失败）和快速恢复的支持。 提供 fallback 优雅降级的支持。 支持近实时的监控、报警以及运维操作。 有这样一个分布式系统，服务 A 依赖于服务 B，服务 B 依赖于服务 C/D/E。在这样一个成熟的系统内，比如说最多可能只有 100 个线程资源。正常情况下，40 个线程并发调用服务 C，各 30 个线程并发调用 D/E。 调用服务 C，只需要 20ms，现在因为服务 C 故障了，比如延迟，或者挂了，此时线程会 hang 住 2s 左右。40 个线程全部被卡住，由于请求不断涌入，其它的线程也用来调用服务 C，同样也会被卡住。这样导致服务 B 的线程资源被耗尽，无法接收新的请求，甚至可能因为大量线程不断的运转，导致自己宕机。服务 A 也挂。 Hystrix 可以对其进行资源隔离，比如限制服务 B 只有 40 个线程调用服务 C。当此 40 个线程被 hang 住时，其它 60 个线程依然能正常调用工作。从而确保整个系统不会被拖垮。 Hystrix 更加细节的设计原则 阻止任何一个依赖服务耗尽所有的资源，比如 tomcat 中的所有线程资源。 避免请求排队和积压，采用限流和 fail fast 来控制故障。 提供 fallback 降级机制来应对故障。 使用资源隔离技术，比如 bulkhead（舱壁隔离技术）、swimlane（泳道技术）、circuit breaker（断路技术）来限制任何一个依赖服务的故障的影响。 通过近实时的统计/监控/报警功能，来提高故障发现的速度。 通过近实时的属性和配置热修改功能，来提高故障处理和恢复的速度。 保护依赖服务调用的所有故障情况，而不仅仅只是网络故障情况。 深入 Hystrix 执行时内部原理前面我们了解了 Hystrix 最基本的支持高可用的技术：资源隔离 + 限流。 创建 command； 执行这个 command； 配置这个 command 对应的 group 和线程池。 开始执行这个 command，调用了这个 command 的 execute() 方法之后，Hystrix 底层的执行流程和步骤以及原理是什么。 在讲解这个流程的过程中，我会带出来 Hystrix 其他的一些核心以及重要的功能。 这里是整个 8 大步骤的流程图 步骤一：创建 command一个 HystrixCommand 或 HystrixObservableCommand 对象，代表了对某个依赖服务发起的一次请求或者调用。创建的时候，可以在构造函数中传入任何需要的参数。 HystrixCommand 主要用于仅仅会返回一个结果的调用。 HystrixObservableCommand 主要用于可能会返回多条结果的调用。 12345// 创建 HystrixCommandHystrixCommand hystrixCommand = new HystrixCommand(arg1, arg2);// 创建 HystrixObservableCommandHystrixObservableCommand hystrixObservableCommand = new HystrixObservableCommand(arg1, arg2); 步骤二：调用 command 执行方法执行 command，就可以发起一次对依赖服务的调用。 要执行 command，可以在 4 个方法中选择其中的一个：execute()、queue()、observe()、toObservable()。 其中 execute() 和 queue() 方法仅仅对 HystrixCommand 适用。 execute()：调用后直接 block 住，属于同步调用，直到依赖服务返回单条结果，或者抛出异常。 queue()：返回一个 Future，属于异步调用，后面可以通过 Future 获取单条结果。 observe()：订阅一个 Observable 对象，Observable 代表的是依赖服务返回的结果，获取到一个那个代表结果的 Observable 对象的拷贝对象。 toObservable()：返回一个 Observable 对象，如果我们订阅这个对象，就会执行 command 并且获取返回结果。 1234K value = hystrixCommand.execute();Future&lt;K&gt; fValue = hystrixCommand.queue();Observable&lt;K&gt; oValue = hystrixObservableCommand.observe();Observable&lt;K&gt; toOValue = hystrixObservableCommand.toObservable(); execute() 实际上会调用 queue().get() 方法，可以看一下 Hystrix 源码。 1234567public R execute() &#123; try &#123; return queue().get(); &#125; catch (Exception e) &#123; throw Exceptions.sneakyThrow(decomposeException(e)); &#125;&#125; 而在 queue() 方法中，会调用 toObservable().toBlocking().toFuture()。 1final Future&lt;R&gt; delegate = toObservable().toBlocking().toFuture(); 也就是说，先通过 toObservable() 获得 Future 对象，然后调用 Future 的 get() 方法。那么，其实无论是哪种方式执行 command，最终都是依赖于 toObservable() 去执行的。 步骤三：检查是否开启缓存从这一步开始，就进入到 Hystrix 底层运行原理啦，看一下 Hystrix 一些更高级的功能和特性。 如果这个 command 开启了请求缓存 Request Cache，而且这个调用的结果在缓存中存在，那么直接从缓存中返回结果。否则，继续往后的步骤。 步骤四：检查是否开启了断路器检查这个 command 对应的依赖服务是否开启了断路器。如果断路器被打开了，那么 Hystrix 就不会执行这个 command，而是直接去执行 fallback 降级机制，返回降级结果。 步骤五：检查线程池/队列/信号量是否已满如果这个 command 线程池和队列已满，或者 semaphore 信号量已满，那么也不会执行 command，而是直接去调用 fallback 降级机制，同时发送 reject 信息给断路器统计。 步骤六：执行 command调用 HystrixObservableCommand 对象的 construct() 方法，或者 HystrixCommand 的 run() 方法来实际执行这个 command。 HystrixCommand.run() 返回单条结果，或者抛出异常。 12// 通过command执行，获取最新一条商品数据ProductInfo productInfo = getProductInfoCommand.execute(); HystrixObservableCommand.construct() 返回一个 Observable 对象，可以获取多条结果。 123456789101112131415161718192021222324Observable&lt;ProductInfo&gt; observable = getProductInfosCommand.observe();// 订阅获取多条结果observable.subscribe(new Observer&lt;ProductInfo&gt;() &#123; @Override public void onCompleted() &#123; System.out.println("获取完了所有的商品数据"); &#125; @Override public void onError(Throwable e) &#123; e.printStackTrace(); &#125; /** * 获取完一条数据，就回调一次这个方法 * * @param productInfo 商品信息 */ @Override public void onNext(ProductInfo productInfo) &#123; System.out.println(productInfo); &#125;&#125;); 如果是采用线程池方式，并且 HystrixCommand.run() 或者 HystrixObservableCommand.construct() 的执行时间超过了 timeout 时长的话，那么 command 所在的线程会抛出一个 TimeoutException，这时会执行 fallback 降级机制，不会去管 run() 或 construct() 返回的值了。另一种情况，如果 command 执行出错抛出了其它异常，那么也会走 fallback 降级。这两种情况下，Hystrix 都会发送异常事件给断路器统计。 注意，我们是不可能终止掉一个调用严重延迟的依赖服务的线程的，只能说给你抛出来一个 TimeoutException。 如果没有 timeout，也正常执行的话，那么调用线程就会拿到一些调用依赖服务获取到的结果，然后 Hystrix 也会做一些 logging 记录和 metric 度量统计。 步骤七：断路健康检查Hystrix 会把每一个依赖服务的调用成功、失败、Reject、Timeout 等事件发送给 circuit breaker 断路器。断路器就会对这些事件的次数进行统计，根据异常事件发生的比例来决定是否要进行断路（熔断）。如果打开了断路器，那么在接下来一段时间内，会直接断路，返回降级结果。 如果在之后，断路器尝试执行 command，调用没有出错，返回了正常结果，那么 Hystrix 就会把断路器关闭。 步骤八：调用 fallback 降级机制在以下几种情况中，Hystrix 会调用 fallback 降级机制。 断路器处于打开状态； 线程池/队列/semaphore满了； command 执行超时； run() 或者 construct() 抛出异常。 一般在降级机制中，都建议给出一些默认的返回值，比如静态的一些代码逻辑，或者从内存中的缓存中提取一些数据，在这里尽量不要再进行网络请求了。 在降级中，如果一定要进行网络调用的话，也应该将那个调用放在一个 HystrixCommand 中进行隔离。 HystrixCommand 中，实现 getFallback() 方法，可以提供降级机制。 HystrixObservableCommand 中，实现 resumeWithFallback() 方法，返回一个 Observable 对象，可以提供降级结果。 如果没有实现 fallback，或者 fallback 抛出了异常，Hystrix 会返回一个 Observable，但是不会返回任何数据。 不同的 command 执行方式，其 fallback 为空或者异常时的返回结果不同。 对于 execute()，直接抛出异常。 对于 queue()，返回一个 Future，调用 get() 时抛出异常。 对于 observe()，返回一个 Observable 对象，但是调用 subscribe() 方法订阅它时，立即抛出调用者的 onError() 方法。 对于 toObservable()，返回一个 Observable 对象，但是调用 subscribe() 方法订阅它时，立即抛出调用者的 onError() 方法。 不同的执行方式 execute()，获取一个 Future.get()，然后拿到单个结果。 queue()，返回一个 Future。 observe()，立即订阅 Observable，然后启动 8 大执行步骤，返回一个拷贝的 Observable，订阅时立即回调给你结果。 toObservable()，返回一个原始的 Observable，必须手动订阅才会去执行 8 大步骤。 基于 request cache 请求缓存技术优化批量商品数据查询接口Hystrix command 执行时 8 大步骤第三步，就是检查 Request cache 是否有缓存。 首先，有一个概念，叫做 Request Context 请求上下文，一般来说，在一个 web 应用中，如果我们用到了 Hystrix，我们会在一个 filter 里面，对每一个请求都施加一个请求上下文。就是说，每一次请求，就是一次请求上下文。然后在这次请求上下文中，我们会去执行 N 多代码，调用 N 多依赖服务，有的依赖服务可能还会调用好几次。 在一次请求上下文中，如果有多个 command，参数都是一样的，调用的接口也是一样的，而结果可以认为也是一样的。那么这个时候，我们可以让第一个 command 执行返回的结果缓存在内存中，然后这个请求上下文后续的其它对这个依赖的调用全部从内存中取出缓存结果就可以了。 这样的话，好处在于不用在一次请求上下文中反复多次执行一样的 command，避免重复执行网络请求，提升整个请求的性能。 举个栗子。比如说我们在一次请求上下文中，请求获取 productId 为 1 的数据，第一次缓存中没有，那么会从商品服务中获取数据，返回最新数据结果，同时将数据缓存在内存中。后续同一次请求上下文中，如果还有获取 productId 为 1 的数据的请求，直接从缓存中取就好了。 HystrixCommand 和 HystrixObservableCommand 都可以指定一个缓存 key，然后 Hystrix 会自动进行缓存，接着在同一个 request context 内，再次访问的话，就会直接取用缓存。 下面，我们结合一个具体的业务场景，来看一下如何使用 request cache 请求缓存技术。当然，以下代码只作为一个基本的 Demo 而已。 现在，假设我们要做一个批量查询商品数据的接口，在这个里面，我们是用 HystrixCommand 一次性批量查询多个商品 id 的数据。但是这里有个问题，如果说 Nginx 在本地缓存失效了，重新获取一批缓存，传递过来的 productIds 都没有进行去重，比如 productIds=1,1,1,2,2，那么可能说，商品 id 出现了重复，如果按照我们之前的业务逻辑，可能就会重复对 productId=1 的商品查询三次，productId=2 的商品查询两次。 我们对批量查询商品数据的接口，可以用 request cache 做一个优化，就是说一次请求，就是一次 request context，对相同的商品查询只执行一次，其余重复的都走 request cache。 实现 Hystrix 请求上下文过滤器并注册定义 HystrixRequestContextFilter 类，实现 Filter 接口。 123456789101112131415161718192021222324252627/** * Hystrix 请求上下文过滤器 */public class HystrixRequestContextFilter implements Filter &#123; @Override public void init(FilterConfig filterConfig) throws ServletException &#123; &#125; @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) &#123; HystrixRequestContext context = HystrixRequestContext.initializeContext(); try &#123; filterChain.doFilter(servletRequest, servletResponse); &#125; catch (IOException | ServletException e) &#123; e.printStackTrace(); &#125; finally &#123; context.shutdown(); &#125; &#125; @Override public void destroy() &#123; &#125;&#125; 然后将该 filter 对象注册到 SpringBoot Application 中。 1234567891011121314@SpringBootApplicationpublic class EshopApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EshopApplication.class, args); &#125; @Bean public FilterRegistrationBean filterRegistrationBean() &#123; FilterRegistrationBean filterRegistrationBean = new FilterRegistrationBean(new HystrixRequestContextFilter()); filterRegistrationBean.addUrlPatterns("/*"); return filterRegistrationBean; &#125;&#125; command 重写 getCacheKey() 方法在 GetProductInfoCommand 中，重写 getCacheKey() 方法，这样的话，每一次请求的结果，都会放在 Hystrix 请求上下文中。下一次同一个 productId 的数据请求，直接取缓存，无须再调用 run() 方法。 12345678910111213141516171819202122232425262728293031323334353637383940public class GetProductInfoCommand extends HystrixCommand&lt;ProductInfo&gt; &#123; private Long productId; private static final HystrixCommandKey KEY = HystrixCommandKey.Factory.asKey("GetProductInfoCommand"); public GetProductInfoCommand(Long productId) &#123; super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey("ProductInfoService")) .andCommandKey(KEY)); this.productId = productId; &#125; @Override protected ProductInfo run() &#123; String url = "http://localhost:8081/getProductInfo?productId=" + productId; String response = HttpClientUtils.sendGetRequest(url); System.out.println("调用接口查询商品数据，productId=" + productId); return JSONObject.parseObject(response, ProductInfo.class); &#125; /** * 每次请求的结果，都会放在Hystrix绑定的请求上下文上 * * @return cacheKey 缓存key */ @Override public String getCacheKey() &#123; return "product_info_" + productId; &#125; /** * 将某个商品id的缓存清空 * * @param productId 商品id */ public static void flushCache(Long productId) &#123; HystrixRequestCache.getInstance(KEY, HystrixConcurrencyStrategyDefault.getInstance()).clear("product_info_" + productId); &#125;&#125; 这里写了一个 flushCache() 方法，用于我们开发手动删除缓存。 controller 调用 command 查询商品信息在一次 web 请求上下文中，传入商品 id 列表，查询多条商品数据信息。对于每个 productId，都创建一个 command。 如果 id 列表没有去重，那么重复的 id，第二次查询的时候就会直接走缓存。 12345678910111213141516171819202122@Controllerpublic class CacheController &#123; /** * 一次性批量查询多条商品数据的请求 * * @param productIds 以,分隔的商品id列表 * @return 响应状态 */ @RequestMapping("/getProductInfos") @ResponseBody public String getProductInfos(String productIds) &#123; for (String productId : productIds.split(",")) &#123; // 对每个productId，都创建一个command GetProductInfoCommand getProductInfoCommand = new GetProductInfoCommand(Long.valueOf(productId)); ProductInfo productInfo = getProductInfoCommand.execute(); System.out.println("是否是从缓存中取的结果：" + getProductInfoCommand.isResponseFromCache()); &#125; return "success"; &#125;&#125; 发起请求调用接口，查询多个商品的信息。 1http://localhost:8080/getProductInfos?productIds=1,1,1,2,2,5 在控制台，我们可以看到以下结果。 123456789调用接口查询商品数据，productId=1是否是从缓存中取的结果：false是否是从缓存中取的结果：true是否是从缓存中取的结果：true调用接口查询商品数据，productId=2是否是从缓存中取的结果：false是否是从缓存中取的结果：true调用接口查询商品数据，productId=5是否是从缓存中取的结果：false 第一次查询 productId=1 的数据，会调用接口进行查询，不是从缓存中取结果。而随后再出现查询 productId=1 的请求，就直接取缓存了，这样的话，效率明显高很多。 删除缓存我们写一个 UpdateProductInfoCommand，在更新商品信息之后，手动调用之前写的 flushCache()，手动将缓存删除。 12345678910111213141516171819public class UpdateProductInfoCommand extends HystrixCommand&lt;Boolean&gt; &#123; private Long productId; public UpdateProductInfoCommand(Long productId) &#123; super(HystrixCommandGroupKey.Factory.asKey("UpdateProductInfoGroup")); this.productId = productId; &#125; @Override protected Boolean run() throws Exception &#123; // 这里执行一次商品信息的更新 // ... // 然后清空缓存 GetProductInfoCommand.flushCache(productId); return true; &#125;&#125; 这样，以后查询该商品的请求，第一次就会走接口调用去查询最新的商品信息。 基于 Hystrix 线程池技术实现资源隔离上一讲提到，如果从 Nginx 开始，缓存都失效了，Nginx 会直接通过缓存服务调用商品服务获取最新商品数据（我们基于电商项目做个讨论），有可能出现调用延时而把缓存服务资源耗尽的情况。这里，我们就来说说，怎么通过 Hystrix 线程池技术实现资源隔离。 资源隔离，就是说，你如果要把对某一个依赖服务的所有调用请求，全部隔离在同一份资源池内，不会去用其它资源了，这就叫资源隔离。哪怕对这个依赖服务，比如说商品服务，现在同时发起的调用量已经到了 1000，但是线程池内就 10 个线程，最多就只会用这 10 个线程去执行，不会说，对商品服务的请求，因为接口调用延时，将 tomcat 内部所有的线程资源全部耗尽。 Hystrix 进行资源隔离，其实是提供了一个抽象，叫做 command。这也是 Hystrix 最最基本的资源隔离技术。 利用 HystrixCommand 获取单条数据我们通过将调用商品服务的操作封装在 HystrixCommand 中，限定一个 key，比如下面的 GetProductInfoCommandGroup，在这里我们可以简单认为这是一个线程池，每次调用商品服务，就只会用该线程池中的资源，不会再去用其它线程资源了。 1234567891011121314151617public class GetProductInfoCommand extends HystrixCommand&lt;ProductInfo&gt; &#123; private Long productId; public GetProductInfoCommand(Long productId) &#123; super(HystrixCommandGroupKey.Factory.asKey("GetProductInfoCommandGroup")); this.productId = productId; &#125; @Override protected ProductInfo run() &#123; String url = "http://localhost:8081/getProductInfo?productId=" + productId; // 调用商品服务接口 String response = HttpClientUtils.sendGetRequest(url); return JSONObject.parseObject(response, ProductInfo.class); &#125;&#125; 我们在缓存服务接口中，根据 productId 创建 command 并执行，获取到商品数据。 12345678910@RequestMapping("/getProductInfo")@ResponseBodypublic String getProductInfo(Long productId) &#123; HystrixCommand&lt;ProductInfo&gt; getProductInfoCommand = new GetProductInfoCommand(productId); // 通过command执行，获取最新商品数据 ProductInfo productInfo = getProductInfoCommand.execute(); System.out.println(productInfo); return "success";&#125; 上面执行的是 execute() 方法，其实是同步的。也可以对 command 调用 queue() 方法，它仅仅是将 command 放入线程池的一个等待队列，就立即返回，拿到一个 Future 对象，后面可以继续做其它一些事情，然后过一段时间对 Future 调用 get() 方法获取数据。这是异步的。 利用 HystrixObservableCommand 批量获取数据只要是获取商品数据，全部都绑定到同一个线程池里面去，我们通过 HystrixObservableCommand 的一个线程去执行，而在这个线程里面，批量把多个 productId 的 productInfo 拉回来。 1234567891011121314151617181920212223242526public class GetProductInfosCommand extends HystrixObservableCommand&lt;ProductInfo&gt; &#123; private String[] productIds; public GetProductInfosCommand(String[] productIds) &#123; // 还是绑定在同一个线程池 super(HystrixCommandGroupKey.Factory.asKey("GetProductInfoGroup")); this.productIds = productIds; &#125; @Override protected Observable&lt;ProductInfo&gt; construct() &#123; return Observable.unsafeCreate((Observable.OnSubscribe&lt;ProductInfo&gt;) subscriber -&gt; &#123; for (String productId : productIds) &#123; // 批量获取商品数据 String url = "http://localhost:8081/getProductInfo?productId=" + productId; String response = HttpClientUtils.sendGetRequest(url); ProductInfo productInfo = JSONObject.parseObject(response, ProductInfo.class); subscriber.onNext(productInfo); &#125; subscriber.onCompleted(); &#125;).subscribeOn(Schedulers.io()); &#125;&#125; 在缓存服务接口中，根据传来的 id 列表，比如是以 , 分隔的 id 串，通过上面的 HystrixObservableCommand，执行 Hystrix 的一些 API 方法，获取到所有商品数据。 123456789101112131415161718192021222324252627public String getProductInfos(String productIds) &#123; String[] productIdArray = productIds.split(","); HystrixObservableCommand&lt;ProductInfo&gt; getProductInfosCommand = new GetProductInfosCommand(productIdArray); Observable&lt;ProductInfo&gt; observable = getProductInfosCommand.observe(); observable.subscribe(new Observer&lt;ProductInfo&gt;() &#123; @Override public void onCompleted() &#123; System.out.println("获取完了所有的商品数据"); &#125; @Override public void onError(Throwable e) &#123; e.printStackTrace(); &#125; /** * 获取完一条数据，就回调一次这个方法 * @param productInfo */ @Override public void onNext(ProductInfo productInfo) &#123; System.out.println(productInfo); &#125; &#125;); return "success";&#125; 我们回过头来，看看 Hystrix 线程池技术是如何实现资源隔离的。 从 Nginx 开始，缓存都失效了，那么 Nginx 通过缓存服务去调用商品服务。缓存服务默认的线程大小是 10 个，最多就只有 10 个线程去调用商品服务的接口。即使商品服务接口故障了，最多就只有 10 个线程会 hang 死在调用商品服务接口的路上，缓存服务的 tomcat 内其它的线程还是可以用来调用其它的服务，干其它的事情。 基于 timeout 机制为服务接口调用超时提供安全保护一般来说，在调用依赖服务的接口的时候，比较常见的一个问题就是超时。超时是在一个复杂的分布式系统中，导致系统不稳定，或者系统抖动。出现大量超时，线程资源会被 hang 死，从而导致吞吐量大幅度下降，甚至服务崩溃。 你去调用各种各样的依赖服务，特别是在大公司，你甚至都不认识开发一个服务的人，你都不知道那个人的技术水平怎么样，对那个人根本不了解。 Peter Steiner 说过，”On the Internet, nobody knows you’re a dog“，也就是说在互联网的另外一头，你都不知道甚至坐着一条狗。 像特别复杂的分布式系统，特别是在大公司里，多个团队、大型协作，你可能都不知道服务是谁的，很可能说开发服务的那个哥儿们甚至是一个实习生。依赖服务的接口性能可能很不稳定，有时候 2ms，有时候 200ms，甚至 2s，都有可能。 如果你不对各种依赖服务接口的调用做超时控制，来给你的服务提供安全保护措施，那么很可能你的服务就被各种垃圾的依赖服务的性能给拖死了。大量的接口调用很慢，大量的线程被卡死。如果你做了资源的隔离，那么也就是线程池的线程被卡死，但其实我们可以做超时控制，没必要让它们全卡死。 TimeoutMilliseconds在 Hystrix 中，我们可以手动设置 timeout 时长，如果一个 command 运行时间超过了设定的时长，那么就被认为是 timeout，然后 Hystrix command 标识为 timeout，同时执行 fallback 降级逻辑。 TimeoutMilliseconds 默认值是 1000，也就是 1000ms。 12HystrixCommandProperties.Setter() ..withExecutionTimeoutInMilliseconds(int) TimeoutEnabled这个参数用于控制是否要打开 timeout 机制，默认值是 true。 12HystrixCommandProperties.Setter() .withExecutionTimeoutEnabled(boolean) 实例 Demo我们在 command 中，将超时时间设置为 500ms，然后在 run() 方法中，设置休眠时间 1s，这样一个请求过来，直接休眠 1s，结果就会因为超时而执行降级逻辑。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class GetProductInfoCommand extends HystrixCommand&lt;ProductInfo&gt; &#123; private Long productId; private static final HystrixCommandKey KEY = HystrixCommandKey.Factory.asKey("GetProductInfoCommand"); public GetProductInfoCommand(Long productId) &#123; super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey("ProductInfoService")) .andCommandKey(KEY) .andThreadPoolPropertiesDefaults(HystrixThreadPoolProperties.Setter() .withCoreSize(8) .withMaxQueueSize(10) .withQueueSizeRejectionThreshold(8)) .andCommandPropertiesDefaults(HystrixCommandProperties.Setter() .withCircuitBreakerEnabled(true) .withCircuitBreakerRequestVolumeThreshold(20) .withCircuitBreakerErrorThresholdPercentage(40) .withCircuitBreakerSleepWindowInMilliseconds(3000) // 设置是否打开超时，默认是true .withExecutionTimeoutEnabled(true) // 设置超时时间，默认1000(ms) .withExecutionTimeoutInMilliseconds(500) .withFallbackIsolationSemaphoreMaxConcurrentRequests(30))); this.productId = productId; &#125; @Override protected ProductInfo run() throws Exception &#123; System.out.println("调用接口查询商品数据，productId=" + productId); // 休眠1s TimeUtils.sleep(1); String url = "http://localhost:8081/getProductInfo?productId=" + productId; String response = HttpClientUtils.sendGetRequest(url); System.out.println(response); return JSONObject.parseObject(response, ProductInfo.class); &#125; @Override protected ProductInfo getFallback() &#123; ProductInfo productInfo = new ProductInfo(); productInfo.setName("降级商品"); return productInfo; &#125;&#125; 在测试类中，我们直接发起请求。 123456789@SpringBootTest@RunWith(SpringRunner.class)public class TimeoutTest &#123; @Test public void testTimeout() &#123; HttpClientUtils.sendGetRequest("http://localhost:8080/getProductInfo?productId=1"); &#125;&#125; 结果中可以看到，打印出了降级商品相关信息。 12ProductInfo(id=null, name=降级商品, price=null, pictureList=null, specification=null, service=null, color=null, size=null, shopId=null, modifiedTime=null, cityId=null, cityName=null, brandId=null, brandName=null)&#123;"id": 1, "name": "iphone7手机", "price": 5599, "pictureList":"a.jpg,b.jpg", "specification": "iphone7的规格", "service": "iphone7的售后服务", "color": "红色,白色,黑色", "size": "5.5", "shopId": 1, "modifiedTime": "2017-01-01 12:00:00", "cityId": 1, "brandId": 1&#125; 高并发架构面试题为什么要分库分表（设计高并发系统的时候，数据库层面该如何设计）？用过哪些分库分表中间件？不同的分库分表中间件都有什么优点和缺点？你们具体是如何对数据库如何进行垂直拆分或水平拆分的？ 面试官心理分析其实这块肯定是扯到高并发了，因为分库分表一定是为了支撑高并发、数据量大两个问题的。而且现在说实话，尤其是互联网类的公司面试，基本上都会来这么一下，分库分表如此普遍的技术问题，不问实在是不行，而如果你不知道那也实在是说不过去！ 面试题剖析为什么要分库分表？（设计高并发系统的时候，数据库层面该如何设计？）说白了，分库分表是两回事儿，大家可别搞混了，可能是光分库不分表，也可能是光分表不分库，都有可能。 我先给大家抛出来一个场景。 假如我们现在是一个小创业公司（或者是一个 BAT 公司刚兴起的一个新部门），现在注册用户就 20 万，每天活跃用户就 1 万，每天单表数据量就 1000，然后高峰期每秒钟并发请求最多就 10。天，就这种系统，随便找一个有几年工作经验的，然后带几个刚培训出来的，随便干干都可以。 结果没想到我们运气居然这么好，碰上个 CEO 带着我们走上了康庄大道，业务发展迅猛，过了几个月，注册用户数达到了 2000 万！每天活跃用户数 100 万！每天单表数据量 10 万条！高峰期每秒最大请求达到 1000！同时公司还顺带着融资了两轮，进账了几个亿人民币啊！公司估值达到了惊人的几亿美金！这是小独角兽的节奏！ 好吧，没事，现在大家感觉压力已经有点大了，为啥呢？因为每天多 10 万条数据，一个月就多 300 万条数据，现在咱们单表已经几百万数据了，马上就破千万了。但是勉强还能撑着。高峰期请求现在是 1000，咱们线上部署了几台机器，负载均衡搞了一下，数据库撑 1000QPS 也还凑合。但是大家现在开始感觉有点担心了，接下来咋整呢…… 再接下来几个月，我的天，CEO 太牛逼了，公司用户数已经达到 1 亿，公司继续融资几十亿人民币啊！公司估值达到了惊人的几十亿美金，成为了国内今年最牛逼的明星创业公司！天，我们太幸运了。 但是我们同时也是不幸的，因为此时每天活跃用户数上千万，每天单表新增数据多达 50 万，目前一个表总数据量都已经达到了两三千万了！扛不住啊！数据库磁盘容量不断消耗掉！高峰期并发达到惊人的 5000~8000！别开玩笑了，哥。我跟你保证，你的系统支撑不到现在，已经挂掉了！ 好吧，所以你看到这里差不多就理解分库分表是怎么回事儿了，实际上这是跟着你的公司业务发展走的，你公司业务发展越好，用户就越多，数据量越大，请求量越大，那你单个数据库一定扛不住。 分表比如你单表都几千万数据了，你确定你能扛住么？绝对不行，单表数据量太大，会极大影响你的 sql 执行的性能，到了后面你的 sql 可能就跑的很慢了。一般来说，就以我的经验来看，单表到几百万的时候，性能就会相对差一些了，你就得分表了。 分表是啥意思？就是把一个表的数据放到多个表中，然后查询的时候你就查一个表。比如按照用户 id 来分表，将一个用户的数据就放在一个表中。然后操作的时候你对一个用户就操作那个表就好了。这样可以控制每个表的数据量在可控的范围内，比如每个表就固定在 200 万以内。 分库分库是啥意思？就是你一个库一般我们经验而言，最多支撑到并发 2000，一定要扩容了，而且一个健康的单库并发值你最好保持在每秒 1000 左右，不要太大。那么你可以将一个库的数据拆分到多个库中，访问的时候就访问一个库好了。 这就是所谓的分库分表，为啥要分库分表？你明白了吧。 # 分库分表前 分库分表后 并发支撑情况 MySQL 单机部署，扛不住高并发 MySQL从单机到多机，能承受的并发增加了多倍 磁盘使用情况 MySQL 单机磁盘容量几乎撑满 拆分为多个库，数据库服务器磁盘使用率大大降低 SQL 执行性能 单表数据量太大，SQL 越跑越慢 单表数据量减少，SQL 执行效率明显提升 用过哪些分库分表中间件？不同的分库分表中间件都有什么优点和缺点？这个其实就是看看你了解哪些分库分表的中间件，各个中间件的优缺点是啥？然后你用过哪些分库分表的中间件。 比较常见的包括： Cobar TDDL Atlas Sharding-jdbc Mycat Cobar阿里 b2b 团队开发和开源的，属于 proxy 层方案，就是介于应用服务器和数据库服务器之间。应用程序通过 JDBC 驱动访问 Cobar 集群，Cobar 根据 SQL 和分库规则对 SQL 做分解，然后分发到 MySQL 集群不同的数据库实例上执行。早些年还可以用，但是最近几年都没更新了，基本没啥人用，差不多算是被抛弃的状态吧。而且不支持读写分离、存储过程、跨库 join 和分页等操作。 TDDL淘宝团队开发的，属于 client 层方案。支持基本的 crud 语法和读写分离，但不支持 join、多表查询等语法。目前使用的也不多，因为还依赖淘宝的 diamond 配置管理系统。 Atlas360 开源的，属于 proxy 层方案，以前是有一些公司在用的，但是确实有一个很大的问题就是社区最新的维护都在 5 年前了。所以，现在用的公司基本也很少了。 Sharding-jdbc当当开源的，属于 client 层方案，目前已经更名为 ShardingSphere（后文所提到的 Sharding-jdbc，等同于 ShardingSphere）。确实之前用的还比较多一些，因为 SQL 语法支持也比较多，没有太多限制，而且截至 2019.4，已经推出到了 4.0.0-RC1 版本，支持分库分表、读写分离、分布式 id 生成、柔性事务（最大努力送达型事务、TCC 事务）。而且确实之前使用的公司会比较多一些（这个在官网有登记使用的公司，可以看到从 2017 年一直到现在，是有不少公司在用的），目前社区也还一直在开发和维护，还算是比较活跃，个人认为算是一个现在也可以选择的方案。 Mycat基于 Cobar 改造的，属于 proxy 层方案，支持的功能非常完善，而且目前应该是非常火的而且不断流行的数据库中间件，社区很活跃，也有一些公司开始在用了。但是确实相比于 Sharding jdbc 来说，年轻一些，经历的锤炼少一些。 总结综上，现在其实建议考量的，就是 Sharding-jdbc 和 Mycat，这两个都可以去考虑使用。 Sharding-jdbc 这种 client 层方案的优点在于不用部署，运维成本低，不需要代理层的二次转发请求，性能很高，但是如果遇到升级啥的需要各个系统都重新升级版本再发布，各个系统都需要耦合 Sharding-jdbc 的依赖； Mycat 这种 proxy 层方案的缺点在于需要部署，自己运维一套中间件，运维成本高，但是好处在于对于各个项目是透明的，如果遇到升级之类的都是自己中间件那里搞就行了。 通常来说，这两个方案其实都可以选用，但是我个人建议中小型公司选用 Sharding-jdbc，client 层方案轻便，而且维护成本低，不需要额外增派人手，而且中小型公司系统复杂度会低一些，项目也没那么多；但是中大型公司最好还是选用 Mycat 这类 proxy 层方案，因为可能大公司系统和项目非常多，团队很大，人员充足，那么最好是专门弄个人来研究和维护 Mycat，然后大量项目直接透明使用即可。 你们具体是如何对数据库如何进行垂直拆分或水平拆分的？水平拆分的意思，就是把一个表的数据给弄到多个库的多个表里去，但是每个库的表结构都一样，只不过每个库表放的数据是不同的，所有库表的数据加起来就是全部数据。水平拆分的意义，就是将数据均匀放更多的库里，然后用多个库来扛更高的并发，还有就是用多个库的存储容量来进行扩容。 垂直拆分的意思，就是把一个有很多字段的表给拆分成多个表，或者是多个库上去。每个库表的结构都不一样，每个库表都包含部分字段。一般来说，会将较少的访问频率很高的字段放到一个表里去，然后将较多的访问频率很低的字段放到另外一个表里去。因为数据库是有缓存的，你访问频率高的行字段越少，就可以在缓存里缓存更多的行，性能就越好。这个一般在表层面做的较多一些。 这个其实挺常见的，不一定我说，大家很多同学可能自己都做过，把一个大表拆开，订单表、订单支付表、订单商品表。 还有表层面的拆分，就是分表，将一个表变成 N 个表，就是让每个表的数据量控制在一定范围内，保证 SQL 的性能。否则单表数据量越大，SQL 性能就越差。一般是 200 万行左右，不要太多，但是也得看具体你怎么操作，也可能是 500 万，或者是 100 万。你的SQL越复杂，就最好让单表行数越少。 好了，无论分库还是分表，上面说的那些数据库中间件都是可以支持的。就是基本上那些中间件可以做到你分库分表之后，中间件可以根据你指定的某个字段值，比如说 userid，自动路由到对应的库上去，然后再自动路由到对应的表里去。 你就得考虑一下，你的项目里该如何分库分表？一般来说，垂直拆分，你可以在表层面来做，对一些字段特别多的表做一下拆分；水平拆分，你可以说是并发承载不了，或者是数据量太大，容量承载不了，你给拆了，按什么字段来拆，你自己想好；分表，你考虑一下，你如果哪怕是拆到每个库里去，并发和容量都 ok 了，但是每个库的表还是太大了，那么你就分表，将这个表分开，保证每个表的数据量并不是很大。 而且这儿还有两种分库分表的方式： 一种是按照 range 来分，就是每个库一段连续的数据，这个一般是按比如时间范围来的，但是这种一般较少用，因为很容易产生热点问题，大量的流量都打在最新的数据上了。 或者是按照某个字段 hash 一下均匀分散，这个较为常用。 range 来分，好处在于说，扩容的时候很简单，因为你只要预备好，给每个月都准备一个库就可以了，到了一个新的月份的时候，自然而然，就会写新的库了；缺点，但是大部分的请求，都是访问最新的数据。实际生产用 range，要看场景。 hash 分发，好处在于说，可以平均分配每个库的数据量和请求压力；坏处在于说扩容起来比较麻烦，会有一个数据迁移的过程，之前的数据需要重新计算 hash 值重新分配到不同的库或表。 面试题如何设计可以动态扩容缩容的分库分表方案？ 面试官心理分析对于分库分表来说，主要是面对以下问题： 选择一个数据库中间件，调研、学习、测试； 设计你的分库分表的一个方案，你要分成多少个库，每个库分成多少个表，比如 3 个库，每个库 4 个表； 基于选择好的数据库中间件，以及在测试环境建立好的分库分表的环境，然后测试一下能否正常进行分库分表的读写； 完成单库单表到分库分表的迁移，双写方案； 线上系统开始基于分库分表对外提供服务； 扩容了，扩容成 6 个库，每个库需要 12 个表，你怎么来增加更多库和表呢？ 这个是你必须面对的一个事儿，就是你已经弄好分库分表方案了，然后一堆库和表都建好了，基于分库分表中间件的代码开发啥的都好了，测试都 ok 了，数据能均匀分布到各个库和各个表里去，而且接着你还通过双写的方案咔嚓一下上了系统，已经直接基于分库分表方案在搞了。 那么现在问题来了，你现在这些库和表又支撑不住了，要继续扩容咋办？这个可能就是说你的每个库的容量又快满了，或者是你的表数据量又太大了，也可能是你每个库的写并发太高了，你得继续扩容。 这都是玩儿分库分表线上必须经历的事儿。 面试题剖析停机扩容（不推荐）这个方案就跟停机迁移一样，步骤几乎一致，唯一的一点就是那个导数的工具，是把现有库表的数据抽出来慢慢倒入到新的库和表里去。但是最好别这么玩儿，有点不太靠谱，因为既然分库分表就说明数据量实在是太大了，可能多达几亿条，甚至几十亿，你这么玩儿，可能会出问题。 从单库单表迁移到分库分表的时候，数据量并不是很大，单表最大也就两三千万。那么你写个工具，多弄几台机器并行跑，1小时数据就导完了。这没有问题。 如果 3 个库 + 12 个表，跑了一段时间了，数据量都 1~2 亿了。光是导 2 亿数据，都要导个几个小时，6 点，刚刚导完数据，还要搞后续的修改配置，重启系统，测试验证，10 点才可以搞完。所以不能这么搞。 优化后的方案一开始上来就是 32 个库，每个库 32 个表，那么总共是 1024 张表。 我可以告诉各位同学，这个分法，第一，基本上国内的互联网肯定都是够用了，第二，无论是并发支撑还是数据量支撑都没问题。 每个库正常承载的写入并发量是 1000，那么 32 个库就可以承载 32 * 1000 = 32000 的写并发，如果每个库承载 1500 的写并发，32 * 1500 = 48000 的写并发，接近 5 万每秒的写入并发，前面再加一个MQ，削峰，每秒写入 MQ 8 万条数据，每秒消费 5 万条数据。 有些除非是国内排名非常靠前的这些公司，他们的最核心的系统的数据库，可能会出现几百台数据库的这么一个规模，128 个库，256 个库，512 个库。 1024 张表，假设每个表放 500 万数据，在 MySQL 里可以放 50 亿条数据。 每秒 5 万的写并发，总共 50 亿条数据，对于国内大部分的互联网公司来说，其实一般来说都够了。 谈分库分表的扩容，第一次分库分表，就一次性给他分个够，32 个库，1024 张表，可能对大部分的中小型互联网公司来说，已经可以支撑好几年了。 一个实践是利用 32 * 32 来分库分表，即分为 32 个库，每个库里一个表分为 32 张表。一共就是 1024 张表。根据某个 id 先根据 32 取模路由到库，再根据 32 取模路由到库里的表。 orderId id % 32 (库) id / 32 % 32 (表) 259 3 8 1189 5 5 352 0 11 4593 17 15 刚开始的时候，这个库可能就是逻辑库，建在一个数据库上的，就是一个 mysql 服务器可能建了 n 个库，比如 32 个库。后面如果要拆分，就是不断在库和 mysql 服务器之间做迁移就可以了。然后系统配合改一下配置即可。 比如说最多可以扩展到 32 个数据库服务器，每个数据库服务器是一个库。如果还是不够？最多可以扩展到 1024 个数据库服务器，每个数据库服务器上面一个库一个表。因为最多是 1024 个表。 这么搞，是不用自己写代码做数据迁移的，都交给 dba 来搞好了，但是 dba 确实是需要做一些库表迁移的工作，但是总比你自己写代码，然后抽数据导数据来的效率高得多吧。 哪怕是要减少库的数量，也很简单，其实说白了就是按倍数缩容就可以了，然后修改一下路由规则。 这里对步骤做一个总结： 设定好几台数据库服务器，每台服务器上几个库，每个库多少个表，推荐是 32 库 * 32 表，对于大部分公司来说，可能几年都够了。 路由的规则，orderId 模 32 = 库，orderId / 32 模 32 = 表 扩容的时候，申请增加更多的数据库服务器，装好 mysql，呈倍数扩容，4 台服务器，扩到 8 台服务器，再到 16 台服务器。 由 dba 负责将原先数据库服务器的库，迁移到新的数据库服务器上去，库迁移是有一些便捷的工具的。 我们这边就是修改一下配置，调整迁移的库所在数据库服务器的地址。 重新发布系统，上线，原先的路由规则变都不用变，直接可以基于 n 倍的数据库服务器的资源，继续进行线上系统的提供服务。 面试题分库分表之后，id 主键如何处理？ 面试官心理分析其实这是分库分表之后你必然要面对的一个问题，就是 id 咋生成？因为要是分成多个表之后，每个表都是从 1 开始累加，那肯定不对啊，需要一个全局唯一的 id 来支持。所以这都是你实际生产环境中必须考虑的问题。 面试题剖析基于数据库的实现方案数据库自增 id这个就是说你的系统里每次得到一个 id，都是往一个库的一个表里插入一条没什么业务含义的数据，然后获取一个数据库自增的一个 id。拿到这个 id 之后再往对应的分库分表里去写入。 这个方案的好处就是方便简单，谁都会用；缺点就是单库生成自增 id，要是高并发的话，就会有瓶颈的；如果你硬是要改进一下，那么就专门开一个服务出来，这个服务每次就拿到当前 id 最大值，然后自己递增几个 id，一次性返回一批 id，然后再把当前最大 id 值修改成递增几个 id 之后的一个值；但是无论如何都是基于单个数据库。 适合的场景：你分库分表就俩原因，要不就是单库并发太高，要不就是单库数据量太大；除非是你并发不高，但是数据量太大导致的分库分表扩容，你可以用这个方案，因为可能每秒最高并发最多就几百，那么就走单独的一个库和表生成自增主键即可。 设置数据库 sequence 或者表自增字段步长可以通过设置数据库 sequence 或者表的自增字段步长来进行水平伸缩。 比如说，现在有 8 个服务节点，每个服务节点使用一个 sequence 功能来产生 ID，每个 sequence 的起始 ID 不同，并且依次递增，步长都是 8。 适合的场景：在用户防止产生的 ID 重复时，这种方案实现起来比较简单，也能达到性能目标。但是服务节点固定，步长也固定，将来如果还要增加服务节点，就不好搞了。 UUID好处就是本地生成，不要基于数据库来了；不好之处就是，UUID 太长了、占用空间大，作为主键性能太差了；更重要的是，UUID 不具有有序性，会导致 B+ 树索引在写的时候有过多的随机写操作（连续的 ID 可以产生部分顺序写），还有，由于在写的时候不能产生有顺序的 append 操作，而需要进行 insert 操作，将会读取整个 B+ 树节点到内存，在插入这条记录后会将整个节点写回磁盘，这种操作在记录占用空间比较大的情况下，性能下降明显。 适合的场景：如果你是要随机生成个什么文件名、编号之类的，你可以用 UUID，但是作为主键是不能用 UUID 的。 1UUID.randomUUID().toString().replace(“-”, “”) -&gt; sfsdf23423rr234sfdaf 获取系统当前时间这个就是获取当前时间即可，但是问题是，并发很高的时候，比如一秒并发几千，会有重复的情况，这个是肯定不合适的。基本就不用考虑了。 适合的场景：一般如果用这个方案，是将当前时间跟很多其他的业务字段拼接起来，作为一个 id，如果业务上你觉得可以接受，那么也是可以的。你可以将别的业务字段值跟当前时间拼接起来，组成一个全局唯一的编号。 snowflake 算法snowflake 算法是 twitter 开源的分布式 id 生成算法，采用 Scala 语言实现，是把一个 64 位的 long 型的 id，1 个 bit 是不用的，用其中的 41 bit 作为毫秒数，用 10 bit 作为工作机器 id，12 bit 作为序列号。 1 bit：不用，为啥呢？因为二进制里第一个 bit 为如果是 1，那么都是负数，但是我们生成的 id 都是正数，所以第一个 bit 统一都是 0。 41 bit：表示的是时间戳，单位是毫秒。41 bit 可以表示的数字多达 2^41 - 1，也就是可以标识 2^41 - 1 个毫秒值，换算成年就是表示69年的时间。 10 bit：记录工作机器 id，代表的是这个服务最多可以部署在 2^10台机器上哪，也就是1024台机器。但是 10 bit 里 5 个 bit 代表机房 id，5 个 bit 代表机器 id。意思就是最多代表 2^5个机房（32个机房），每个机房里可以代表 2^5 个机器（32台机器）。 12 bit：这个是用来记录同一个毫秒内产生的不同 id，12 bit 可以代表的最大正整数是 2^12 - 1 = 4096，也就是说可以用这个 12 bit 代表的数字来区分同一个毫秒内的 4096 个不同的 id。 10 | 0001100 10100010 10111110 10001001 01011100 00 | 10001 | 1 1001 | 0000 00000000 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110public class IdWorker &#123; private long workerId; private long datacenterId; private long sequence; public IdWorker(long workerId, long datacenterId, long sequence) &#123; // sanity check for workerId // 这儿不就检查了一下，要求就是你传递进来的机房id和机器id不能超过32，不能小于0 if (workerId &gt; maxWorkerId || workerId &lt; 0) &#123; throw new IllegalArgumentException( String.format("worker Id can't be greater than %d or less than 0", maxWorkerId)); &#125; if (datacenterId &gt; maxDatacenterId || datacenterId &lt; 0) &#123; throw new IllegalArgumentException( String.format("datacenter Id can't be greater than %d or less than 0", maxDatacenterId)); &#125; System.out.printf( "worker starting. timestamp left shift %d, datacenter id bits %d, worker id bits %d, sequence bits %d, workerid %d", timestampLeftShift, datacenterIdBits, workerIdBits, sequenceBits, workerId); this.workerId = workerId; this.datacenterId = datacenterId; this.sequence = sequence; &#125; private long twepoch = 1288834974657L; private long workerIdBits = 5L; private long datacenterIdBits = 5L; // 这个是二进制运算，就是 5 bit最多只能有31个数字，也就是说机器id最多只能是32以内 private long maxWorkerId = -1L ^ (-1L &lt;&lt; workerIdBits); // 这个是一个意思，就是 5 bit最多只能有31个数字，机房id最多只能是32以内 private long maxDatacenterId = -1L ^ (-1L &lt;&lt; datacenterIdBits); private long sequenceBits = 12L; private long workerIdShift = sequenceBits; private long datacenterIdShift = sequenceBits + workerIdBits; private long timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits; private long sequenceMask = -1L ^ (-1L &lt;&lt; sequenceBits); private long lastTimestamp = -1L; public long getWorkerId() &#123; return workerId; &#125; public long getDatacenterId() &#123; return datacenterId; &#125; public long getTimestamp() &#123; return System.currentTimeMillis(); &#125; public synchronized long nextId() &#123; // 这儿就是获取当前时间戳，单位是毫秒 long timestamp = timeGen(); if (timestamp &lt; lastTimestamp) &#123; System.err.printf("clock is moving backwards. Rejecting requests until %d.", lastTimestamp); throw new RuntimeException(String.format( "Clock moved backwards. Refusing to generate id for %d milliseconds", lastTimestamp - timestamp)); &#125; if (lastTimestamp == timestamp) &#123; // 这个意思是说一个毫秒内最多只能有4096个数字 // 无论你传递多少进来，这个位运算保证始终就是在4096这个范围内，避免你自己传递个sequence超过了4096这个范围 sequence = (sequence + 1) &amp; sequenceMask; if (sequence == 0) &#123; timestamp = tilNextMillis(lastTimestamp); &#125; &#125; else &#123; sequence = 0; &#125; // 这儿记录一下最近一次生成id的时间戳，单位是毫秒 lastTimestamp = timestamp; // 这儿就是将时间戳左移，放到 41 bit那儿； // 将机房 id左移放到 5 bit那儿； // 将机器id左移放到5 bit那儿；将序号放最后12 bit； // 最后拼接起来成一个 64 bit的二进制数字，转换成 10 进制就是个 long 型 return ((timestamp - twepoch) &lt;&lt; timestampLeftShift) | (datacenterId &lt;&lt; datacenterIdShift) | (workerId &lt;&lt; workerIdShift) | sequence; &#125; private long tilNextMillis(long lastTimestamp) &#123; long timestamp = timeGen(); while (timestamp &lt;= lastTimestamp) &#123; timestamp = timeGen(); &#125; return timestamp; &#125; private long timeGen() &#123; return System.currentTimeMillis(); &#125; // ---------------测试--------------- public static void main(String[] args) &#123; IdWorker worker = new IdWorker(1, 1, 1); for (int i = 0; i &lt; 30; i++) &#123; System.out.println(worker.nextId()); &#125; &#125;&#125; 怎么说呢，大概这个意思吧，就是说 41 bit 是当前毫秒单位的一个时间戳，就这意思；然后 5 bit 是你传递进来的一个机房 id（但是最大只能是 32 以内），另外 5 bit 是你传递进来的机器 id（但是最大只能是 32 以内），剩下的那个 12 bit序列号，就是如果跟你上次生成 id 的时间还在一个毫秒内，那么会把顺序给你累加，最多在 4096 个序号以内。 所以你自己利用这个工具类，自己搞一个服务，然后对每个机房的每个机器都初始化这么一个东西，刚开始这个机房的这个机器的序号就是 0。然后每次接收到一个请求，说这个机房的这个机器要生成一个 id，你就找到对应的 Worker 生成。 利用这个 snowflake 算法，你可以开发自己公司的服务，甚至对于机房 id 和机器 id，反正给你预留了 5 bit + 5 bit，你换成别的有业务含义的东西也可以的。 这个 snowflake 算法相对来说还是比较靠谱的，所以你要真是搞分布式 id 生成，如果是高并发啥的，那么用这个应该性能比较好，一般每秒几万并发的场景，也足够你用了。 面试题现在有一个未分库分表的系统，未来要分库分表，如何设计才可以让系统从未分库分表动态切换到分库分表上？ 面试官心理分析你看看，你现在已经明白为啥要分库分表了，你也知道常用的分库分表中间件了，你也设计好你们如何分库分表的方案了（水平拆分、垂直拆分、分表），那问题来了，你接下来该怎么把你那个单库单表的系统给迁移到分库分表上去？ 所以这都是一环扣一环的，就是看你有没有全流程经历过这个过程。 面试题剖析这个其实从 low 到高大上有好几种方案，我们都玩儿过，我都给你说一下。 停机迁移方案我先给你说一个最 low 的方案，就是很简单，大家伙儿凌晨 12 点开始运维，网站或者 app 挂个公告，说 0 点到早上 6 点进行运维，无法访问。 接着到 0 点停机，系统停掉，没有流量写入了，此时老的单库单表数据库静止了。然后你之前得写好一个导数的一次性工具，此时直接跑起来，然后将单库单表的数据哗哗哗读出来，写到分库分表里面去。 导数完了之后，就 ok 了，修改系统的数据库连接配置啥的，包括可能代码和 SQL 也许有修改，那你就用最新的代码，然后直接启动连到新的分库分表上去。 验证一下，ok了，完美，大家伸个懒腰，看看看凌晨 4 点钟的北京夜景，打个滴滴回家吧。 但是这个方案比较 low，谁都能干，我们来看看高大上一点的方案。 双写迁移方案这个是我们常用的一种迁移方案，比较靠谱一些，不用停机，不用看北京凌晨 4 点的风景。 简单来说，就是在线上系统里面，之前所有写库的地方，增删改操作，除了对老库增删改，都加上对新库的增删改，这就是所谓的双写，同时写俩库，老库和新库。 然后系统部署之后，新库数据差太远，用之前说的导数工具，跑起来读老库数据写新库，写的时候要根据 gmt_modified 这类字段判断这条数据最后修改的时间，除非是读出来的数据在新库里没有，或者是比新库的数据新才会写。简单来说，就是不允许用老数据覆盖新数据。 导完一轮之后，有可能数据还是存在不一致，那么就程序自动做一轮校验，比对新老库每个表的每条数据，接着如果有不一样的，就针对那些不一样的，从老库读数据再次写。反复循环，直到两个库每个表的数据都完全一致为止。 接着当数据完全一致了，就 ok 了，基于仅仅使用分库分表的最新代码，重新部署一次，不就仅仅基于分库分表在操作了么，还没有几个小时的停机时间，很稳。所以现在基本玩儿数据迁移之类的，都是这么干的。 lucene 和 es 的前世今生lucene 是最先进、功能最强大的搜索库。如果直接基于 lucene 开发，非常复杂，即便写一些简单的功能，也要写大量的 Java 代码，需要深入理解原理。 elasticsearch 基于 lucene，隐藏了 lucene 的复杂性，提供了简单易用的 restful api / Java api 接口（另外还有其他语言的 api 接口）。 分布式的文档存储引擎 分布式的搜索引擎和分析引擎 分布式，支持 PB 级数据 es 的核心概念Near Realtime近实时，有两层意思： 从写入数据到数据可以被搜索到有一个小延迟（大概是 1s） 基于 es 执行搜索和分析可以达到秒级 Cluster 集群集群包含多个节点，每个节点属于哪个集群都是通过一个配置来决定的，对于中小型应用来说，刚开始一个集群就一个节点很正常。 Node 节点Node 是集群中的一个节点，节点也有一个名称，默认是随机分配的。默认节点会去加入一个名称为 elasticsearch 的集群。如果直接启动一堆节点，那么它们会自动组成一个 elasticsearch 集群，当然一个节点也可以组成 elasticsearch 集群。 Document &amp; field文档是 es 中最小的数据单元，一个 document 可以是一条客户数据、一条商品分类数据、一条订单数据，通常用 json 数据结构来表示。每个 index 下的 type，都可以存储多条 document。一个 document 里面有多个 field，每个 field 就是一个数据字段。 1234567&#123; "product_id": "1", "product_name": "iPhone X", "product_desc": "苹果手机", "category_id": "2", "category_name": "电子产品"&#125; Index索引包含了一堆有相似结构的文档数据，比如商品索引。一个索引包含很多 document，一个索引就代表了一类相似或者相同的 ducument。 Type类型，每个索引里可以有一个或者多个 type，type 是 index 的一个逻辑分类，比如商品 index 下有多个 type：日化商品 type、电器商品 type、生鲜商品 type。每个 type 下的 document 的 field 可能不太一样。 shard单台机器无法存储大量数据，es 可以将一个索引中的数据切分为多个 shard，分布在多台服务器上存储。有了 shard 就可以横向扩展，存储更多数据，让搜索和分析等操作分布到多台服务器上去执行，提升吞吐量和性能。每个 shard 都是一个 lucene index。 replica任何一个服务器随时可能故障或宕机，此时 shard 可能就会丢失，因此可以为每个 shard 创建多个 replica 副本。replica 可以在 shard 故障时提供备用服务，保证数据不丢失，多个 replica 还可以提升搜索操作的吞吐量和性能。primary shard（建立索引时一次设置，不能修改，默认 5 个），replica shard（随时修改数量，默认 1 个），默认每个索引 10 个 shard，5 个 primary shard，5个 replica shard，最小的高可用配置，是 2 台服务器。 这么说吧，shard 分为 primary shard 和 replica shard。而 primary shard 一般简称为 shard，而 replica shard 一般简称为 replica。 es 核心概念 vs. db 核心概念 es db index 数据库 type 数据表 docuemnt 一行数据 以上是一个简单的类比。 面试题es 的分布式架构原理能说一下么（es 是如何实现分布式的啊）？ 面试官心理分析在搜索这块，lucene 是最流行的搜索库。几年前业内一般都问，你了解 lucene 吗？你知道倒排索引的原理吗？现在早已经 out 了，因为现在很多项目都是直接用基于 lucene 的分布式搜索引擎—— ElasticSearch，简称为 es。 而现在分布式搜索基本已经成为大部分互联网行业的 Java 系统的标配，其中尤为流行的就是 es，前几年 es 没火的时候，大家一般用 solr。但是这两年基本大部分企业和项目都开始转向 es 了。 所以互联网面试，肯定会跟你聊聊分布式搜索引擎，也就一定会聊聊 es，如果你确实不知道，那你真的就 out 了。 如果面试官问你第一个问题，确实一般都会问你 es 的分布式架构设计能介绍一下么？就看看你对分布式搜索引擎架构的一个基本理解。 面试题剖析ElasticSearch 设计的理念就是分布式搜索引擎，底层其实还是基于 lucene 的。核心思想就是在多台机器上启动多个 es 进程实例，组成了一个 es 集群。 es 中存储数据的基本单位是索引，比如说你现在要在 es 中存储一些订单数据，你就应该在 es 中创建一个索引 order_idx，所有的订单数据就都写到这个索引里面去，一个索引差不多就是相当于是 mysql 里的一张表。 1index -&gt; type -&gt; mapping -&gt; document -&gt; field。 这样吧，为了做个更直白的介绍，我在这里做个类比。但是切记，不要划等号，类比只是为了便于理解。 index 相当于 mysql 里的一张表。而 type 没法跟 mysql 里去对比，一个 index 里可以有多个 type，每个 type 的字段都是差不多的，但是有一些略微的差别。假设有一个 index，是订单 index，里面专门是放订单数据的。就好比说你在 mysql 中建表，有些订单是实物商品的订单，比如一件衣服、一双鞋子；有些订单是虚拟商品的订单，比如游戏点卡，话费充值。就两种订单大部分字段是一样的，但是少部分字段可能有略微的一些差别。 所以就会在订单 index 里，建两个 type，一个是实物商品订单 type，一个是虚拟商品订单 type，这两个 type 大部分字段是一样的，少部分字段是不一样的。 很多情况下，一个 index 里可能就一个 type，但是确实如果说是一个 index 里有多个 type 的情况（注意，mapping types 这个概念在 ElasticSearch 7.X 已被完全移除，详细说明可以参考官方文档），你可以认为 index 是一个类别的表，具体的每个 type 代表了 mysql 中的一个表。每个 type 有一个 mapping，如果你认为一个 type 是具体的一个表，index 就代表多个 type 同属于的一个类型，而 mapping 就是这个 type 的表结构定义，你在 mysql 中创建一个表，肯定是要定义表结构的，里面有哪些字段，每个字段是什么类型。实际上你往 index 里的一个 type 里面写的一条数据，叫做一条 document，一条 document 就代表了 mysql 中某个表里的一行，每个 document 有多个 field，每个 field 就代表了这个 document 中的一个字段的值。 你搞一个索引，这个索引可以拆分成多个 shard，每个 shard 存储部分数据。拆分多个 shard 是有好处的，一是支持横向扩展，比如你数据量是 3T，3 个 shard，每个 shard 就 1T 的数据，若现在数据量增加到 4T，怎么扩展，很简单，重新建一个有 4 个 shard 的索引，将数据导进去；二是提高性能，数据分布在多个 shard，即多台服务器上，所有的操作，都会在多台机器上并行分布式执行，提高了吞吐量和性能。 接着就是这个 shard 的数据实际是有多个备份，就是说每个 shard 都有一个 primary shard，负责写入数据，但是还有几个 replica shard。primary shard 写入数据之后，会将数据同步到其他几个 replica shard 上去。 通过这个 replica 的方案，每个 shard 的数据都有多个备份，如果某个机器宕机了，没关系啊，还有别的数据副本在别的机器上呢。高可用了吧。 es 集群多个节点，会自动选举一个节点为 master 节点，这个 master 节点其实就是干一些管理的工作的，比如维护索引元数据、负责切换 primary shard 和 replica shard 身份等。要是 master 节点宕机了，那么会重新选举一个节点为 master 节点。 如果是非 master节点宕机了，那么会由 master 节点，让那个宕机节点上的 primary shard 的身份转移到其他机器上的 replica shard。接着你要是修复了那个宕机机器，重启了之后，master 节点会控制将缺失的 replica shard 分配过去，同步后续修改的数据之类的，让集群恢复正常。 说得更简单一点，就是说如果某个非 master 节点宕机了。那么此节点上的 primary shard 不就没了。那好，master 会让 primary shard 对应的 replica shard（在其他机器上）切换为 primary shard。如果宕机的机器修复了，修复后的节点也不再是 primary shard，而是 replica shard。 其实上述就是 ElasticSearch 作为分布式搜索引擎最基本的一个架构设计。 面试题es 在数据量很大的情况下（数十亿级别）如何提高查询效率啊？ 面试官心理分析这个问题是肯定要问的，说白了，就是看你有没有实际干过 es，因为啥？其实 es 性能并没有你想象中那么好的。很多时候数据量大了，特别是有几亿条数据的时候，可能你会懵逼的发现，跑个搜索怎么一下 5~10s，坑爹了。第一次搜索的时候，是 5~10s，后面反而就快了，可能就几百毫秒。 你就很懵，每个用户第一次访问都会比较慢，比较卡么？所以你要是没玩儿过 es，或者就是自己玩玩儿 demo，被问到这个问题容易懵逼，显示出你对 es 确实玩儿的不怎么样？ 面试题剖析说实话，es 性能优化是没有什么银弹的，啥意思呢？就是不要期待着随手调一个参数，就可以万能的应对所有的性能慢的场景。也许有的场景是你换个参数，或者调整一下语法，就可以搞定，但是绝对不是所有场景都可以这样。 性能优化的杀手锏——filesystem cache你往 es 里写的数据，实际上都写到磁盘文件里去了，查询的时候，操作系统会将磁盘文件里的数据自动缓存到 filesystem cache 里面去。 es 的搜索引擎严重依赖于底层的 filesystem cache，你如果给 filesystem cache 更多的内存，尽量让内存可以容纳所有的 idx segment file 索引数据文件，那么你搜索的时候就基本都是走内存的，性能会非常高。 性能差距究竟可以有多大？我们之前很多的测试和压测，如果走磁盘一般肯定上秒，搜索性能绝对是秒级别的，1秒、5秒、10秒。但如果是走 filesystem cache，是走纯内存的，那么一般来说性能比走磁盘要高一个数量级，基本上就是毫秒级的，从几毫秒到几百毫秒不等。 这里有个真实的案例。某个公司 es 节点有 3 台机器，每台机器看起来内存很多，64G，总内存就是 64 * 3 = 192G。每台机器给 es jvm heap 是 32G，那么剩下来留给 filesystem cache 的就是每台机器才 32G，总共集群里给 filesystem cache 的就是 32 * 3 = 96G 内存。而此时，整个磁盘上索引数据文件，在 3 台机器上一共占用了 1T 的磁盘容量，es 数据量是 1T，那么每台机器的数据量是 300G。这样性能好吗？ filesystem cache 的内存才 100G，十分之一的数据可以放内存，其他的都在磁盘，然后你执行搜索操作，大部分操作都是走磁盘，性能肯定差。 归根结底，你要让 es 性能要好，最佳的情况下，就是你的机器的内存，至少可以容纳你的总数据量的一半。 根据我们自己的生产环境实践经验，最佳的情况下，是仅仅在 es 中就存少量的数据，就是你要用来搜索的那些索引，如果内存留给 filesystem cache 的是 100G，那么你就将索引数据控制在 100G 以内，这样的话，你的数据几乎全部走内存来搜索，性能非常之高，一般可以在 1 秒以内。 比如说你现在有一行数据。id,name,age .... 30 个字段。但是你现在搜索，只需要根据 id,name,age 三个字段来搜索。如果你傻乎乎往 es 里写入一行数据所有的字段，就会导致说 90% 的数据是不用来搜索的，结果硬是占据了 es 机器上的 filesystem cache 的空间，单条数据的数据量越大，就会导致 filesystem cahce 能缓存的数据就越少。其实，仅仅写入 es 中要用来检索的少数几个字段就可以了，比如说就写入 es id,name,age 三个字段，然后你可以把其他的字段数据存在 mysql/hbase 里，我们一般是建议用 es + hbase 这么一个架构。 hbase 的特点是适用于海量数据的在线存储，就是对 hbase 可以写入海量数据，但是不要做复杂的搜索，做很简单的一些根据 id 或者范围进行查询的这么一个操作就可以了。从 es 中根据 name 和 age 去搜索，拿到的结果可能就 20 个 doc id，然后根据 doc id 到 hbase 里去查询每个 doc id 对应的完整的数据，给查出来，再返回给前端。 写入 es 的数据最好小于等于，或者是略微大于 es 的 filesystem cache 的内存容量。然后你从 es 检索可能就花费 20ms，然后再根据 es 返回的 id 去 hbase 里查询，查 20 条数据，可能也就耗费个 30ms，可能你原来那么玩儿，1T 数据都放 es，会每次查询都是 5~10s，现在可能性能就会很高，每次查询就是 50ms。 数据预热假如说，哪怕是你就按照上述的方案去做了，es 集群中每个机器写入的数据量还是超过了 filesystem cache 一倍，比如说你写入一台机器 60G 数据，结果 filesystem cache 就 30G，还是有 30G 数据留在了磁盘上。 其实可以做数据预热。 举个例子，拿微博来说，你可以把一些大V，平时看的人很多的数据，你自己提前后台搞个系统，每隔一会儿，自己的后台系统去搜索一下热数据，刷到 filesystem cache 里去，后面用户实际上来看这个热数据的时候，他们就是直接从内存里搜索了，很快。 或者是电商，你可以将平时查看最多的一些商品，比如说 iphone 8，热数据提前后台搞个程序，每隔 1 分钟自己主动访问一次，刷到 filesystem cache 里去。 对于那些你觉得比较热的、经常会有人访问的数据，最好做一个专门的缓存预热子系统，就是对热数据每隔一段时间，就提前访问一下，让数据进入 filesystem cache 里面去。这样下次别人访问的时候，性能一定会好很多。 冷热分离es 可以做类似于 mysql 的水平拆分，就是说将大量的访问很少、频率很低的数据，单独写一个索引，然后将访问很频繁的热数据单独写一个索引。最好是将冷数据写入一个索引中，然后热数据写入另外一个索引中，这样可以确保热数据在被预热之后，尽量都让他们留在 filesystem os cache 里，别让冷数据给冲刷掉。 你看，假设你有 6 台机器，2 个索引，一个放冷数据，一个放热数据，每个索引 3 个 shard。3 台机器放热数据 index，另外 3 台机器放冷数据 index。然后这样的话，你大量的时间是在访问热数据 index，热数据可能就占总数据量的 10%，此时数据量很少，几乎全都保留在 filesystem cache 里面了，就可以确保热数据的访问性能是很高的。但是对于冷数据而言，是在别的 index 里的，跟热数据 index 不在相同的机器上，大家互相之间都没什么联系了。如果有人访问冷数据，可能大量数据是在磁盘上的，此时性能差点，就 10% 的人去访问冷数据，90% 的人在访问热数据，也无所谓了。 document 模型设计对于 MySQL，我们经常有一些复杂的关联查询。在 es 里该怎么玩儿，es 里面的复杂的关联查询尽量别用，一旦用了性能一般都不太好。 最好是先在 Java 系统里就完成关联，将关联好的数据直接写入 es 中。搜索的时候，就不需要利用 es 的搜索语法来完成 join 之类的关联搜索了。 document 模型设计是非常重要的，很多操作，不要在搜索的时候才想去执行各种复杂的乱七八糟的操作。es 能支持的操作就那么多，不要考虑用 es 做一些它不好操作的事情。如果真的有那种操作，尽量在 document 模型设计的时候，写入的时候就完成。另外对于一些太复杂的操作，比如 join/nested/parent-child 搜索都要尽量避免，性能都很差的。 分页性能优化es 的分页是较坑的，为啥呢？举个例子吧，假如你每页是 10 条数据，你现在要查询第 100 页，实际上是会把每个 shard 上存储的前 1000 条数据都查到一个协调节点上，如果你有个 5 个 shard，那么就有 5000 条数据，接着协调节点对这 5000 条数据进行一些合并、处理，再获取到最终第 100 页的 10 条数据。 分布式的，你要查第 100 页的 10 条数据，不可能说从 5 个 shard，每个 shard 就查 2 条数据，最后到协调节点合并成 10 条数据吧？你必须得从每个 shard 都查 1000 条数据过来，然后根据你的需求进行排序、筛选等等操作，最后再次分页，拿到里面第 100 页的数据。你翻页的时候，翻的越深，每个 shard 返回的数据就越多，而且协调节点处理的时间越长，非常坑爹。所以用 es 做分页的时候，你会发现越翻到后面，就越是慢。 我们之前也是遇到过这个问题，用 es 作分页，前几页就几十毫秒，翻到 10 页或者几十页的时候，基本上就要 5~10 秒才能查出来一页数据了。 有什么解决方案吗？ 不允许深度分页（默认深度分页性能很差）跟产品经理说，你系统不允许翻那么深的页，默认翻的越深，性能就越差。 类似于 app 里的推荐商品不断下拉出来一页一页的类似于微博中，下拉刷微博，刷出来一页一页的，你可以用 scroll api，关于如何使用，自行上网搜索。 scroll 会一次性给你生成所有数据的一个快照，然后每次滑动向后翻页就是通过游标 scroll_id 移动，获取下一页下一页这样子，性能会比上面说的那种分页性能要高很多很多，基本上都是毫秒级的。 但是，唯一的一点就是，这个适合于那种类似微博下拉翻页的，不能随意跳到任何一页的场景。也就是说，你不能先进入第 10 页，然后去第 120 页，然后又回到第 58 页，不能随意乱跳页。所以现在很多产品，都是不允许你随意翻页的，app，也有一些网站，做的就是你只能往下拉，一页一页的翻。 初始化时必须指定 scroll 参数，告诉 es 要保存此次搜索的上下文多长时间。你需要确保用户不会持续不断翻页翻几个小时，否则可能因为超时而失败。 除了用 scroll api，你也可以用 search_after 来做，search_after 的思想是使用前一页的结果来帮助检索下一页的数据，显然，这种方式也不允许你随意翻页，你只能一页页往后翻。初始化时，需要使用一个唯一值的字段作为 sort 字段。 面试题es 生产集群的部署架构是什么？每个索引的数据量大概有多少？每个索引大概有多少个分片？ 面试官心理分析这个问题，包括后面的 redis 什么的，谈到 es、redis、mysql 分库分表等等技术，面试必问！就是你生产环境咋部署的？说白了，这个问题没啥技术含量，就是看你有没有在真正的生产环境里干过这事儿！ 有些同学可能是没在生产环境中干过的，没实际去拿线上机器部署过 es 集群，也没实际玩儿过，也没往 es 集群里面导入过几千万甚至是几亿的数据量，可能你就不太清楚这里面的一些生产项目中的细节。 如果你是自己就玩儿过 demo，没碰过真实的 es 集群，那你可能此时会懵。别懵，你一定要云淡风轻的回答出来这个问题，表示你确实干过这事儿。 面试题剖析其实这个问题没啥，如果你确实干过 es，那你肯定了解你们生产 es 集群的实际情况，部署了几台机器？有多少个索引？每个索引有多大数据量？每个索引给了多少个分片？你肯定知道！ 但是如果你确实没干过，也别虚，我给你说一个基本的版本，你到时候就简单说一下就好了。 es 生产集群我们部署了 5 台机器，每台机器是 6 核 64G 的，集群总内存是 320G。 我们 es 集群的日增量数据大概是 2000 万条，每天日增量数据大概是 500MB，每月增量数据大概是 6 亿，15G。目前系统已经运行了几个月，现在 es 集群里数据总量大概是 100G 左右。 目前线上有 5 个索引（这个结合你们自己业务来，看看自己有哪些数据可以放 es 的），每个索引的数据量大概是 20G，所以这个数据量之内，我们每个索引分配的是 8 个 shard，比默认的 5 个 shard 多了 3 个 shard。 大概就这么说一下就行了。 面试题es 写入数据的工作原理是什么啊？es 查询数据的工作原理是什么啊？底层的 lucene 介绍一下呗？倒排索引了解吗？ 面试官心理分析问这个，其实面试官就是要看看你了解不了解 es 的一些基本原理，因为用 es 无非就是写入数据，搜索数据。你要是不明白你发起一个写入和搜索请求的时候，es 在干什么，那你真的是…… 对 es 基本就是个黑盒，你还能干啥？你唯一能干的就是用 es 的 api 读写数据了。要是出点什么问题，你啥都不知道，那还能指望你什么呢？ 面试题剖析es 写数据过程 客户端选择一个 node 发送请求过去，这个 node 就是 coordinating node（协调节点）。 coordinating node 对 document 进行路由，将请求转发给对应的 node（有 primary shard）。 实际的 node 上的 primary shard 处理请求，然后将数据同步到 replica node。 coordinating node 如果发现 primary node 和所有 replica node 都搞定之后，就返回响应结果给客户端。 es 读数据过程可以通过 doc id 来查询，会根据 doc id 进行 hash，判断出来当时把 doc id 分配到了哪个 shard 上面去，从那个 shard 去查询。 客户端发送请求到任意一个 node，成为 coordinate node。 coordinate node 对 doc id 进行哈希路由，将请求转发到对应的 node，此时会使用 round-robin 随机轮询算法，在 primary shard 以及其所有 replica 中随机选择一个，让读请求负载均衡。 接收请求的 node 返回 document 给 coordinate node。 coordinate node 返回 document 给客户端。 es 搜索数据过程es 最强大的是做全文检索，就是比如你有三条数据： 123java真好玩儿啊java好难学啊j2ee特别牛 你根据 java 关键词来搜索，将包含 java的 document 给搜索出来。es 就会给你返回：java真好玩儿啊，java好难学啊。 客户端发送请求到一个 coordinate node。 协调节点将搜索请求转发到所有的 shard 对应的 primary shard 或 replica shard，都可以。 query phase：每个 shard 将自己的搜索结果（其实就是一些 doc id）返回给协调节点，由协调节点进行数据的合并、排序、分页等操作，产出最终结果。 fetch phase：接着由协调节点根据 doc id 去各个节点上拉取实际的 document 数据，最终返回给客户端。 写请求是写入 primary shard，然后同步给所有的 replica shard；读请求可以从 primary shard 或 replica shard 读取，采用的是随机轮询算法。 写数据底层原理 先写入内存 buffer，在 buffer 里的时候数据是搜索不到的；同时将数据写入 translog 日志文件。 如果 buffer 快满了，或者到一定时间，就会将内存 buffer 数据 refresh 到一个新的 segment file 中，但是此时数据不是直接进入 segment file 磁盘文件，而是先进入 os cache 。这个过程就是 refresh。 每隔 1 秒钟，es 将 buffer 中的数据写入一个新的 segment file，每秒钟会产生一个新的磁盘文件 segment file，这个 segment file 中就存储最近 1 秒内 buffer 中写入的数据。 但是如果 buffer 里面此时没有数据，那当然不会执行 refresh 操作，如果 buffer 里面有数据，默认 1 秒钟执行一次 refresh 操作，刷入一个新的 segment file 中。 操作系统里面，磁盘文件其实都有一个东西，叫做 os cache，即操作系统缓存，就是说数据写入磁盘文件之前，会先进入 os cache，先进入操作系统级别的一个内存缓存中去。只要 buffer 中的数据被 refresh 操作刷入 os cache中，这个数据就可以被搜索到了。 为什么叫 es 是准实时的？ NRT，全称 near real-time。默认是每隔 1 秒 refresh 一次的，所以 es 是准实时的，因为写入的数据 1 秒之后才能被看到。可以通过 es 的 restful api 或者 java api，手动执行一次 refresh 操作，就是手动将 buffer 中的数据刷入 os cache中，让数据立马就可以被搜索到。只要数据被输入 os cache 中，buffer 就会被清空了，因为不需要保留 buffer 了，数据在 translog 里面已经持久化到磁盘去一份了。 重复上面的步骤，新的数据不断进入 buffer 和 translog，不断将 buffer 数据写入一个又一个新的 segment file 中去，每次 refresh 完 buffer 清空，translog 保留。随着这个过程推进，translog 会变得越来越大。当 translog 达到一定长度的时候，就会触发 commit 操作。 commit 操作发生第一步，就是将 buffer 中现有数据 refresh 到 os cache 中去，清空 buffer。然后，将一个 commit point 写入磁盘文件，里面标识着这个 commit point 对应的所有 segment file，同时强行将 os cache 中目前所有的数据都 fsync 到磁盘文件中去。最后清空 现有 translog 日志文件，重启一个 translog，此时 commit 操作完成。 这个 commit 操作叫做 flush。默认 30 分钟自动执行一次 flush，但如果 translog 过大，也会触发 flush。flush 操作就对应着 commit 的全过程，我们可以通过 es api，手动执行 flush 操作，手动将 os cache 中的数据 fsync 强刷到磁盘上去。 translog 日志文件的作用是什么？你执行 commit 操作之前，数据要么是停留在 buffer 中，要么是停留在 os cache 中，无论是 buffer 还是 os cache 都是内存，一旦这台机器死了，内存中的数据就全丢了。所以需要将数据对应的操作写入一个专门的日志文件 translog 中，一旦此时机器宕机，再次重启的时候，es 会自动读取 translog 日志文件中的数据，恢复到内存 buffer 和 os cache 中去。 translog 其实也是先写入 os cache 的，默认每隔 5 秒刷一次到磁盘中去，所以默认情况下，可能有 5 秒的数据会仅仅停留在 buffer 或者 translog 文件的 os cache 中，如果此时机器挂了，会丢失 5 秒钟的数据。但是这样性能比较好，最多丢 5 秒的数据。也可以将 translog 设置成每次写操作必须是直接 fsync 到磁盘，但是性能会差很多。 实际上你在这里，如果面试官没有问你 es 丢数据的问题，你可以在这里给面试官炫一把，你说，其实 es 第一是准实时的，数据写入 1 秒后可以搜索到；可能会丢失数据的。有 5 秒的数据，停留在 buffer、translog os cache、segment file os cache 中，而不在磁盘上，此时如果宕机，会导致 5 秒的数据丢失。 总结一下，数据先写入内存 buffer，然后每隔 1s，将数据 refresh 到 os cache，到了 os cache 数据就能被搜索到（所以我们才说 es 从写入到能被搜索到，中间有 1s 的延迟）。每隔 5s，将数据写入 translog 文件（这样如果机器宕机，内存数据全没，最多会有 5s 的数据丢失），translog 大到一定程度，或者默认每隔 30mins，会触发 commit 操作，将缓冲区的数据都 flush 到 segment file 磁盘文件中。 数据写入 segment file 之后，同时就建立好了倒排索引。 删除/更新数据底层原理如果是删除操作，commit 的时候会生成一个 .del 文件，里面将某个 doc 标识为 deleted 状态，那么搜索的时候根据 .del 文件就知道这个 doc 是否被删除了。 如果是更新操作，就是将原来的 doc 标识为 deleted 状态，然后新写入一条数据。 buffer 每 refresh 一次，就会产生一个 segment file，所以默认情况下是 1 秒钟一个 segment file，这样下来 segment file 会越来越多，此时会定期执行 merge。每次 merge 的时候，会将多个 segment file 合并成一个，同时这里会将标识为 deleted 的 doc 给物理删除掉，然后将新的 segment file 写入磁盘，这里会写一个 commit point，标识所有新的 segment file，然后打开 segment file 供搜索使用，同时删除旧的 segment file。 底层 lucene简单来说，lucene 就是一个 jar 包，里面包含了封装好的各种建立倒排索引的算法代码。我们用 Java 开发的时候，引入 lucene jar，然后基于 lucene 的 api 去开发就可以了。 通过 lucene，我们可以将已有的数据建立索引，lucene 会在本地磁盘上面，给我们组织索引的数据结构。 倒排索引在搜索引擎中，每个文档都有一个对应的文档 ID，文档内容被表示为一系列关键词的集合。例如，文档 1 经过分词，提取了 20 个关键词，每个关键词都会记录它在文档中出现的次数和出现位置。 那么，倒排索引就是关键词到文档 ID 的映射，每个关键词都对应着一系列的文件，这些文件中都出现了关键词。 举个栗子。 有以下文档： DocId Doc 1 谷歌地图之父跳槽 Facebook 2 谷歌地图之父加盟 Facebook 3 谷歌地图创始人拉斯离开谷歌加盟 Facebook 4 谷歌地图之父跳槽 Facebook 与 Wave 项目取消有关 5 谷歌地图之父拉斯加盟社交网站 Facebook 对文档进行分词之后，得到以下倒排索引。 WordId Word DocIds 1 谷歌 1,2,3,4,5 2 地图 1,2,3,4,5 3 之父 1,2,4,5 4 跳槽 1,4 5 Facebook 1,2,3,4,5 6 加盟 2,3,5 7 创始人 3 8 拉斯 3,5 9 离开 3 10 与 4 .. .. .. 另外，实用的倒排索引还可以记录更多的信息，比如文档频率信息，表示在文档集合中有多少个文档包含某个单词。 那么，有了倒排索引，搜索引擎可以很方便地响应用户的查询。比如用户输入查询 Facebook，搜索系统查找倒排索引，从中读出包含这个单词的文档，这些文档就是提供给用户的搜索结果。 要注意倒排索引的两个重要细节： 倒排索引中的所有词项对应一个或多个文档； 倒排索引中的词项根据字典顺序升序排列 上面只是一个简单的栗子，并没有严格按照字典顺序升序排列。 面试题 为什么使用消息队列？ 消息队列有什么优点和缺点？ Kafka、ActiveMQ、RabbitMQ、RocketMQ 都有什么区别，以及适合哪些场景？ 面试官心理分析其实面试官主要是想看看： 第一，你知不知道你们系统里为什么要用消息队列这个东西？不少候选人，说自己项目里用了 Redis、MQ，但是其实他并不知道自己为什么要用这个东西。其实说白了，就是为了用而用，或者是别人设计的架构，他从头到尾都没思考过。没有对自己的架构问过为什么的人，一定是平时没有思考的人，面试官对这类候选人印象通常很不好。因为面试官担心你进了团队之后只会木头木脑的干呆活儿，不会自己思考。 第二，你既然用了消息队列这个东西，你知不知道用了有什么好处&amp;坏处？你要是没考虑过这个，那你盲目弄个 MQ 进系统里，后面出了问题你是不是就自己溜了给公司留坑？你要是没考虑过引入一个技术可能存在的弊端和风险，面试官把这类候选人招进来了，基本可能就是挖坑型选手。就怕你干 1 年挖一堆坑，自己跳槽了，给公司留下无穷后患。 第三，既然你用了 MQ，可能是某一种 MQ，那么你当时做没做过调研？你别傻乎乎的自己拍脑袋看个人喜好就瞎用了一个 MQ，比如 Kafka，甚至都从没调研过业界流行的 MQ 到底有哪几种。每一个 MQ 的优点和缺点是什么。每一个 MQ 没有绝对的好坏，但是就是看用在哪个场景可以扬长避短，利用其优势，规避其劣势。如果是一个不考虑技术选型的候选人招进了团队，leader 交给他一个任务，去设计个什么系统，他在里面用一些技术，可能都没考虑过选型，最后选的技术可能并不一定合适，一样是留坑。 面试题剖析为什么使用消息队列其实就是问问你消息队列都有哪些使用场景，然后你项目里具体是什么场景，说说你在这个场景里用消息队列是什么？ 面试官问你这个问题，期望的一个回答是说，你们公司有个什么业务场景，这个业务场景有个什么技术挑战，如果不用 MQ 可能会很麻烦，但是你现在用了 MQ 之后带给了你很多的好处。 先说一下消息队列常见的使用场景吧，其实场景有很多，但是比较核心的有 3 个：解耦、异步、削峰。 解耦看这么个场景。A 系统发送数据到 BCD 三个系统，通过接口调用发送。如果 E 系统也要这个数据呢？那如果 C 系统现在不需要了呢？A 系统负责人几乎崩溃…… 在这个场景中，A 系统跟其它各种乱七八糟的系统严重耦合，A 系统产生一条比较关键的数据，很多系统都需要 A 系统将这个数据发送过来。A 系统要时时刻刻考虑 BCDE 四个系统如果挂了该咋办？要不要重发，要不要把消息存起来？头发都白了啊！ 如果使用 MQ，A 系统产生一条数据，发送到 MQ 里面去，哪个系统需要数据自己去 MQ 里面消费。如果新系统需要数据，直接从 MQ 里消费即可；如果某个系统不需要这条数据了，就取消对 MQ 消息的消费即可。这样下来，A 系统压根儿不需要去考虑要给谁发送数据，不需要维护这个代码，也不需要考虑人家是否调用成功、失败超时等情况。 总结：通过一个 MQ，Pub/Sub 发布订阅消息这么一个模型，A 系统就跟其它系统彻底解耦了。 面试技巧：你需要去考虑一下你负责的系统中是否有类似的场景，就是一个系统或者一个模块，调用了多个系统或者模块，互相之间的调用很复杂，维护起来很麻烦。但是其实这个调用是不需要直接同步调用接口的，如果用 MQ 给它异步化解耦，也是可以的，你就需要去考虑在你的项目里，是不是可以运用这个 MQ 去进行系统的解耦。在简历中体现出来这块东西，用 MQ 作解耦。 异步再来看一个场景，A 系统接收一个请求，需要在自己本地写库，还需要在 BCD 三个系统写库，自己本地写库要 3ms，BCD 三个系统分别写库要 300ms、450ms、200ms。最终请求总延时是 3 + 300 + 450 + 200 = 953ms，接近 1s，用户感觉搞个什么东西，慢死了慢死了。用户通过浏览器发起请求，等待个 1s，这几乎是不可接受的。 一般互联网类的企业，对于用户直接的操作，一般要求是每个请求都必须在 200 ms 以内完成，对用户几乎是无感知的。 如果使用 MQ，那么 A 系统连续发送 3 条消息到 MQ 队列中，假如耗时 5ms，A 系统从接受一个请求到返回响应给用户，总时长是 3 + 5 = 8ms，对于用户而言，其实感觉上就是点个按钮，8ms 以后就直接返回了，爽！网站做得真好，真快！ 削峰每天 0:00 到 12:00，A 系统风平浪静，每秒并发请求数量就 50 个。结果每次一到 12:00 ~ 13:00 ，每秒并发请求数量突然会暴增到 5k+ 条。但是系统是直接基于 MySQL 的，大量的请求涌入 MySQL，每秒钟对 MySQL 执行约 5k 条 SQL。 一般的 MySQL，扛到每秒 2k 个请求就差不多了，如果每秒请求到 5k 的话，可能就直接把 MySQL 给打死了，导致系统崩溃，用户也就没法再使用系统了。 但是高峰期一过，到了下午的时候，就成了低峰期，可能也就 1w 的用户同时在网站上操作，每秒中的请求数量可能也就 50 个请求，对整个系统几乎没有任何的压力。 ![mq-5].(/images/mq-5.png) 如果使用 MQ，每秒 5k 个请求写入 MQ，A 系统每秒钟最多处理 2k 个请求，因为 MySQL 每秒钟最多处理 2k 个。A 系统从 MQ 中慢慢拉取请求，每秒钟就拉取 2k 个请求，不要超过自己每秒能处理的最大请求数量就 ok，这样下来，哪怕是高峰期的时候，A 系统也绝对不会挂掉。而 MQ 每秒钟 5k 个请求进来，就 2k 个请求出去，结果就导致在中午高峰期（1 个小时），可能有几十万甚至几百万的请求积压在 MQ 中。 这个短暂的高峰期积压是 ok 的，因为高峰期过了之后，每秒钟就 50 个请求进 MQ，但是 A 系统依然会按照每秒 2k 个请求的速度在处理。所以说，只要高峰期一过，A 系统就会快速将积压的消息给解决掉。 消息队列有什么优缺点优点上面已经说了，就是在特殊场景下有其对应的好处，解耦、异步、削峰。 缺点有以下几个： 系统可用性降低系统引入的外部依赖越多，越容易挂掉。本来你就是 A 系统调用 BCD 三个系统的接口就好了，人 ABCD 四个系统好好的，没啥问题，你偏加个 MQ 进来，万一 MQ 挂了咋整，MQ 一挂，整套系统崩溃的，你不就完了？如何保证消息队列的高可用，可以点击这里查看。 系统复杂度提高硬生生加个 MQ 进来，你怎么保证消息没有重复消费？怎么处理消息丢失的情况？怎么保证消息传递的顺序性？头大头大，问题一大堆，痛苦不已。 一致性问题A 系统处理完了直接返回成功了，人都以为你这个请求就成功了；但是问题是，要是 BCD 三个系统那里，BD 两个系统写库成功了，结果 C 系统写库失败了，咋整？你这数据就不一致了。 所以消息队列实际是一种非常复杂的架构，你引入它有很多好处，但是也得针对它带来的坏处做各种额外的技术方案和架构来规避掉，做好之后，你会发现，妈呀，系统复杂度提升了一个数量级，也许是复杂了 10 倍。但是关键时刻，用，还是得用的。 Kafka、ActiveMQ、RabbitMQ、RocketMQ 有什么优缺点？ 特性 ActiveMQ RabbitMQ RocketMQ Kafka 单机吞吐量 万级，比 RocketMQ、Kafka 低一个数量级 同 ActiveMQ 10 万级，支撑高吞吐 10 万级，高吞吐，一般配合大数据类的系统来进行实时数据计算、日志采集等场景 topic 数量对吞吐量的影响 topic 可以达到几百/几千的级别，吞吐量会有较小幅度的下降，这是 RocketMQ 的一大优势，在同等机器下，可以支撑大量的 topic topic 从几十到几百个时候，吞吐量会大幅度下降，在同等机器下，Kafka 尽量保证 topic 数量不要过多，如果要支撑大规模的 topic，需要增加更多的机器资源 时效性 ms 级 微秒级，这是 RabbitMQ 的一大特点，延迟最低 ms 级 延迟在 ms 级以内 可用性 高，基于主从架构实现高可用 同 ActiveMQ 非常高，分布式架构 非常高，分布式，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用 消息可靠性 有较低的概率丢失数据 基本不丢 经过参数优化配置，可以做到 0 丢失 同 RocketMQ 功能支持 MQ 领域的功能极其完备 基于 erlang 开发，并发能力很强，性能极好，延时很低 MQ 功能较为完善，还是分布式的，扩展性好 功能较为简单，主要支持简单的 MQ 功能，在大数据领域的实时计算以及日志采集被大规模使用 综上，各种对比之后，有如下建议： 一般的业务系统要引入 MQ，最早大家都用 ActiveMQ，但是现在确实大家用的不多了，没经过大规模吞吐量场景的验证，社区也不是很活跃，所以大家还是算了吧，我个人不推荐用这个了； 后来大家开始用 RabbitMQ，但是确实 erlang 语言阻止了大量的 Java 工程师去深入研究和掌控它，对公司而言，几乎处于不可控的状态，但是确实人家是开源的，比较稳定的支持，活跃度也高； 不过现在确实越来越多的公司会去用 RocketMQ，确实很不错，毕竟是阿里出品，但社区可能有突然黄掉的风险（目前 RocketMQ 已捐给 Apache，但 GitHub 上的活跃度其实不算高）对自己公司技术实力有绝对自信的，推荐用 RocketMQ，否则回去老老实实用 RabbitMQ 吧，人家有活跃的开源社区，绝对不会黄。 所以中小型公司，技术实力较为一般，技术挑战不是特别高，用 RabbitMQ 是不错的选择；大型公司，基础架构研发实力较强，用 RocketMQ 是很好的选择。 如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。 面试题如何设计一个高并发系统？ 面试官心理分析说实话，如果面试官问你这个题目，那么你必须要使出全身吃奶劲了。为啥？因为你没看到现在很多公司招聘的 JD 里都是说啥，有高并发就经验者优先。 如果你确实有真才实学，在互联网公司里干过高并发系统，那你确实拿 offer 基本如探囊取物，没啥问题。面试官也绝对不会这样来问你，否则他就是蠢。 假设你在某知名电商公司干过高并发系统，用户上亿，一天流量几十亿，高峰期并发量上万，甚至是十万。那么人家一定会仔细盘问你的系统架构，你们系统啥架构？怎么部署的？部署了多少台机器？缓存咋用的？MQ 咋用的？数据库咋用的？就是深挖你到底是如何扛住高并发的。 因为真正干过高并发的人一定知道，脱离了业务的系统架构都是在纸上谈兵，真正在复杂业务场景而且还高并发的时候，那系统架构一定不是那么简单的，用个 redis，用 mq 就能搞定？当然不是，真实的系统架构搭配上业务之后，会比这种简单的所谓“高并发架构”要复杂很多倍。 如果有面试官问你个问题说，如何设计一个高并发系统？那么不好意思，一定是因为你实际上没干过高并发系统。面试官看你简历就没啥出彩的，感觉就不咋地，所以就会问问你，如何设计一个高并发系统？其实说白了本质就是看看你有没有自己研究过，有没有一定的知识积累。 最好的当然是招聘个真正干过高并发的哥儿们咯，但是这种哥儿们人数稀缺，不好招。所以可能次一点的就是招一个自己研究过的哥儿们，总比招一个啥也不会的哥儿们好吧！ 所以这个时候你必须得做一把个人秀了，秀出你所有关于高并发的知识！ 面试题剖析其实所谓的高并发，如果你要理解这个问题呢，其实就得从高并发的根源出发，为啥会有高并发？为啥高并发就很牛逼？ 我说的浅显一点，很简单，就是因为刚开始系统都是连接数据库的，但是要知道数据库支撑到每秒并发两三千的时候，基本就快完了。所以才有说，很多公司，刚开始干的时候，技术比较 low，结果业务发展太快，有的时候系统扛不住压力就挂了。 当然会挂了，凭什么不挂？你数据库如果瞬间承载每秒 5000/8000，甚至上万的并发，一定会宕机，因为比如 mysql 就压根儿扛不住这么高的并发量。 所以为啥高并发牛逼？就是因为现在用互联网的人越来越多，很多 app、网站、系统承载的都是高并发请求，可能高峰期每秒并发量几千，很正常的。如果是什么双十一之类的，每秒并发几万几十万都有可能。 那么如此之高的并发量，加上原本就如此之复杂的业务，咋玩儿？真正厉害的，一定是在复杂业务系统里玩儿过高并发架构的人，但是你没有，那么我给你说一下你该怎么回答这个问题： 可以分为以下 6 点： 系统拆分 缓存 MQ 分库分表 读写分离 ElasticSearch 系统拆分将一个系统拆分为多个子系统，用 dubbo 来搞。然后每个系统连一个数据库，这样本来就一个库，现在多个数据库，不也可以扛高并发么。 缓存缓存，必须得用缓存。大部分的高并发场景，都是读多写少，那你完全可以在数据库和缓存里都写一份，然后读的时候大量走缓存不就得了。毕竟人家 redis 轻轻松松单机几万的并发。所以你可以考虑考虑你的项目里，那些承载主要请求的读场景，怎么用缓存来抗高并发。 MQMQ，必须得用 MQ。可能你还是会出现高并发写的场景，比如说一个业务操作里要频繁搞数据库几十次，增删改增删改，疯了。那高并发绝对搞挂你的系统，你要是用 redis 来承载写那肯定不行，人家是缓存，数据随时就被 LRU 了，数据格式还无比简单，没有事务支持。所以该用 mysql 还得用 mysql 啊。那你咋办？用 MQ 吧，大量的写请求灌入 MQ 里，排队慢慢玩儿，后边系统消费后慢慢写，控制在 mysql 承载范围之内。所以你得考虑考虑你的项目里，那些承载复杂写业务逻辑的场景里，如何用 MQ 来异步写，提升并发性。MQ 单机抗几万并发也是 ok 的，这个之前还特意说过。 分库分表分库分表，可能到了最后数据库层面还是免不了抗高并发的要求，好吧，那么就将一个数据库拆分为多个库，多个库来扛更高的并发；然后将一个表拆分为多个表，每个表的数据量保持少一点，提高 sql 跑的性能。 读写分离读写分离，这个就是说大部分时候数据库可能也是读多写少，没必要所有请求都集中在一个库上吧，可以搞个主从架构，主库写入，从库读取，搞一个读写分离。读流量太多的时候，还可以加更多的从库。 ElasticSearchElasticsearch，简称 es。es 是分布式的，可以随便扩容，分布式天然就可以支撑高并发，因为动不动就可以扩容加机器来扛更高的并发。那么一些比较简单的查询、统计类的操作，可以考虑用 es 来承载，还有一些全文搜索类的操作，也可以考虑用 es 来承载。 上面的 6 点，基本就是高并发系统肯定要干的一些事儿，大家可以仔细结合之前讲过的知识考虑一下，到时候你可以系统的把这块阐述一下，然后每个部分要注意哪些问题，之前都讲过了，你都可以阐述阐述，表明你对这块是有点积累的。 说句实话，毕竟你真正厉害的一点，不是在于弄明白一些技术，或者大概知道一个高并发系统应该长什么样？其实实际上在真正的复杂的业务系统里，做高并发要远远比上面提到的点要复杂几十倍到上百倍。你需要考虑：哪些需要分库分表，哪些不需要分库分表，单库单表跟分库分表如何 join，哪些数据要放到缓存里去，放哪些数据才可以扛住高并发的请求，你需要完成对一个复杂业务系统的分析之后，然后逐步逐步的加入高并发的系统架构的改造，这个过程是无比复杂的，一旦做过一次，并且做好了，你在这个市场上就会非常的吃香。 其实大部分公司，真正看重的，不是说你掌握高并发相关的一些基本的架构知识，架构中的一些技术，RocketMQ、Kafka、Redis、Elasticsearch，高并发这一块，你了解了，也只能是次一等的人才。对一个有几十万行代码的复杂的分布式系统，一步一步架构、设计以及实践过高并发架构的人，这个经验是难能可贵的。 面试题如何保证消息队列的高可用？ 面试官心理分析如果有人问到你 MQ 的知识，高可用是必问的。上一讲提到，MQ 会导致系统可用性降低。所以只要你用了 MQ，接下来问的一些要点肯定就是围绕着 MQ 的那些缺点怎么来解决了。 要是你傻乎乎的就干用了一个 MQ，各种问题从来没考虑过，那你就杯具了，面试官对你的感觉就是，只会简单使用一些技术，没任何思考，马上对你的印象就不太好了。这样的同学招进来要是做个 20k 薪资以内的普通小弟还凑合，要是做薪资 20k+ 的高工，那就惨了，让你设计个系统，里面肯定一堆坑，出了事故公司受损失，团队一起背锅。 面试题剖析这个问题这么问是很好的，因为不能问你 Kafka 的高可用性怎么保证？ActiveMQ 的高可用性怎么保证？一个面试官要是这么问就显得很没水平，人家可能用的就是 RabbitMQ，没用过 Kafka，你上来问人家 Kafka 干什么？这不是摆明了刁难人么。 所以有水平的面试官，问的是 MQ 的高可用性怎么保证？这样就是你用过哪个 MQ，你就说说你对那个 MQ 的高可用性的理解。 RabbitMQ 的高可用性RabbitMQ 是比较有代表性的，因为是基于主从（非分布式）做高可用性的，我们就以 RabbitMQ 为例子讲解第一种 MQ 的高可用性怎么实现。 RabbitMQ 有三种模式：单机模式、普通集群模式、镜像集群模式。 单机模式单机模式，就是 Demo 级别的，一般就是你本地启动了玩玩儿的😄，没人生产用单机模式。 普通集群模式（无高可用性）普通集群模式，意思就是在多台机器上启动多个 RabbitMQ 实例，每个机器启动一个。你创建的 queue，只会放在一个 RabbitMQ 实例上，但是每个实例都同步 queue 的元数据（元数据可以认为是 queue 的一些配置信息，通过元数据，可以找到 queue 所在实例）。你消费的时候，实际上如果连接到了另外一个实例，那么那个实例会从 queue 所在实例上拉取数据过来。 这种方式确实很麻烦，也不怎么好，没做到所谓的分布式，就是个普通集群。因为这导致你要么消费者每次随机连接一个实例然后拉取数据，要么固定连接那个 queue 所在实例消费数据，前者有数据拉取的开销，后者导致单实例性能瓶颈。 而且如果那个放 queue 的实例宕机了，会导致接下来其他实例就无法从那个实例拉取，如果你开启了消息持久化，让 RabbitMQ 落地存储消息的话，消息不一定会丢，得等这个实例恢复了，然后才可以继续从这个 queue 拉取数据。 所以这个事儿就比较尴尬了，这就没有什么所谓的高可用性，这方案主要是提高吞吐量的，就是说让集群中多个节点来服务某个 queue 的读写操作。 镜像集群模式（高可用性）这种模式，才是所谓的 RabbitMQ 的高可用模式。跟普通集群模式不一样的是，在镜像集群模式下，你创建的 queue，无论元数据还是 queue 里的消息都会存在于多个实例上，就是说，每个 RabbitMQ 节点都有这个 queue 的一个完整镜像，包含 queue 的全部数据的意思。然后每次你写消息到 queue 的时候，都会自动把消息同步到多个实例的 queue 上。 那么如何开启这个镜像集群模式呢？其实很简单，RabbitMQ 有很好的管理控制台，就是在后台新增一个策略，这个策略是镜像集群模式的策略，指定的时候是可以要求数据同步到所有节点的，也可以要求同步到指定数量的节点，再次创建 queue 的时候，应用这个策略，就会自动将数据同步到其他的节点上去了。 这样的话，好处在于，你任何一个机器宕机了，没事儿，其它机器（节点）还包含了这个 queue 的完整数据，别的 consumer 都可以到其它节点上去消费数据。坏处在于，第一，这个性能开销也太大了吧，消息需要同步到所有机器上，导致网络带宽压力和消耗很重！第二，这么玩儿，不是分布式的，就没有扩展性可言了，如果某个 queue 负载很重，你加机器，新增的机器也包含了这个 queue 的所有数据，并没有办法线性扩展你的 queue。你想，如果这个 queue 的数据量很大，大到这个机器上的容量无法容纳了，此时该怎么办呢？ Kafka 的高可用性Kafka 一个最基本的架构认识：由多个 broker 组成，每个 broker 是一个节点；你创建一个 topic，这个 topic 可以划分为多个 partition，每个 partition 可以存在于不同的 broker 上，每个 partition 就放一部分数据。 这就是天然的分布式消息队列，就是说一个 topic 的数据，是分散放在多个机器上的，每个机器就放一部分数据。 实际上 RabbmitMQ 之类的，并不是分布式消息队列，它就是传统的消息队列，只不过提供了一些集群、HA(High Availability, 高可用性) 的机制而已，因为无论怎么玩儿，RabbitMQ 一个 queue 的数据都是放在一个节点里的，镜像集群下，也是每个节点都放这个 queue 的完整数据。 Kafka 0.8 以前，是没有 HA 机制的，就是任何一个 broker 宕机了，那个 broker 上的 partition 就废了，没法写也没法读，没有什么高可用性可言。 比如说，我们假设创建了一个 topic，指定其 partition 数量是 3 个，分别在三台机器上。但是，如果第二台机器宕机了，会导致这个 topic 的 1/3 的数据就丢了，因此这个是做不到高可用的。 Kafka 0.8 以后，提供了 HA 机制，就是 replica（复制品） 副本机制。每个 partition 的数据都会同步到其它机器上，形成自己的多个 replica 副本。所有 replica 会选举一个 leader 出来，那么生产和消费都跟这个 leader 打交道，然后其他 replica 就是 follower。写的时候，leader 会负责把数据同步到所有 follower 上去，读的时候就直接读 leader 上的数据即可。只能读写 leader？很简单，要是你可以随意读写每个 follower，那么就要 care 数据一致性的问题，系统复杂度太高，很容易出问题。Kafka 会均匀地将一个 partition 的所有 replica 分布在不同的机器上，这样才可以提高容错性。 这么搞，就有所谓的高可用性了，因为如果某个 broker 宕机了，没事儿，那个 broker上面的 partition 在其他机器上都有副本的。如果这个宕机的 broker 上面有某个 partition 的 leader，那么此时会从 follower 中重新选举一个新的 leader 出来，大家继续读写那个新的 leader 即可。这就有所谓的高可用性了。 写数据的时候，生产者就写 leader，然后 leader 将数据落地写本地磁盘，接着其他 follower 自己主动从 leader 来 pull 数据。一旦所有 follower 同步好数据了，就会发送 ack 给 leader，leader 收到所有 follower 的 ack 之后，就会返回写成功的消息给生产者。（当然，这只是其中一种模式，还可以适当调整这个行为） 消费的时候，只会从 leader 去读，但是只有当一个消息已经被所有 follower 都同步成功返回 ack 的时候，这个消息才会被消费者读到。 看到这里，相信你大致明白了 Kafka 是如何保证高可用机制的了，对吧？不至于一无所知，现场还能给面试官画画图。要是遇上面试官确实是 Kafka 高手，深挖了问，那你只能说不好意思，太深入的你没研究过。 面试题如何保证 redis 的高并发和高可用？redis 的主从复制原理能介绍一下么？redis 的哨兵原理能介绍一下么？ 面试官心理分析其实问这个问题，主要是考考你，redis 单机能承载多高并发？如果单机扛不住如何扩容扛更多的并发？redis 会不会挂？既然 redis 会挂那怎么保证 redis 是高可用的？ 其实针对的都是项目中你肯定要考虑的一些问题，如果你没考虑过，那确实你对生产系统中的问题思考太少。 面试题剖析如果你用 redis 缓存技术的话，肯定要考虑如何用 redis 来加多台机器，保证 redis 是高并发的，还有就是如何让 redis 保证自己不是挂掉以后就直接死掉了，即 redis 高可用。 由于此节内容较多，因此，会分为两个小节进行讲解。 redis 主从架构 redis 基于哨兵实现高可用 redis 实现高并发主要依靠主从架构，一主多从，一般来说，很多项目其实就足够了，单主用来写入数据，单机几万 QPS，多从用来查询数据，多个从实例可以提供每秒 10w 的 QPS。 如果想要在实现高并发的同时，容纳大量的数据，那么就需要 redis 集群，使用 redis 集群之后，可以提供每秒几十万的读写并发。 redis 高可用，如果是做主从架构部署，那么加上哨兵就可以了，就可以实现，任何一个实例宕机，可以进行主备切换。 面试题如何保证消息不被重复消费？或者说，如何保证消息消费的幂等性？ 面试官心理分析其实这是很常见的一个问题，这俩问题基本可以连起来问。既然是消费消息，那肯定要考虑会不会重复消费？能不能避免重复消费？或者重复消费了也别造成系统异常可以吗？这个是 MQ 领域的基本问题，其实本质上还是问你使用消息队列如何保证幂等性，这个是你架构里要考虑的一个问题。 面试题剖析回答这个问题，首先你别听到重复消息这个事儿，就一无所知吧，你先大概说一说可能会有哪些重复消费的问题。 首先，比如 RabbitMQ、RocketMQ、Kafka，都有可能会出现消息重复消费的问题，正常。因为这问题通常不是 MQ 自己保证的，是由我们开发来保证的。挑一个 Kafka 来举个例子，说说怎么重复消费吧。 Kafka 实际上有个 offset 的概念，就是每个消息写进去，都有一个 offset，代表消息的序号，然后 consumer 消费了数据之后，每隔一段时间（定时定期），会把自己消费过的消息的 offset 提交一下，表示“我已经消费过了，下次我要是重启啥的，你就让我继续从上次消费到的 offset 来继续消费吧”。 但是凡事总有意外，比如我们之前生产经常遇到的，就是你有时候重启系统，看你怎么重启了，如果碰到点着急的，直接 kill 进程了，再重启。这会导致 consumer 有些消息处理了，但是没来得及提交 offset，尴尬了。重启之后，少数消息会再次消费一次。 举个栗子。 有这么个场景。数据 1/2/3 依次进入 kafka，kafka 会给这三条数据每条分配一个 offset，代表这条数据的序号，我们就假设分配的 offset 依次是 152/153/154。消费者从 kafka 去消费的时候，也是按照这个顺序去消费。假如当消费者消费了 offset=153 的这条数据，刚准备去提交 offset 到 zookeeper，此时消费者进程被重启了。那么此时消费过的数据 1/2 的 offset 并没有提交，kafka 也就不知道你已经消费了 offset=153 这条数据。那么重启之后，消费者会找 kafka 说，嘿，哥儿们，你给我接着把上次我消费到的那个地方后面的数据继续给我传递过来。由于之前的 offset 没有提交成功，那么数据 1/2 会再次传过来，如果此时消费者没有去重的话，那么就会导致重复消费。 如果消费者干的事儿是拿一条数据就往数据库里写一条，会导致说，你可能就把数据 1/2 在数据库里插入了 2 次，那么数据就错啦。 其实重复消费不可怕，可怕的是你没考虑到重复消费之后，怎么保证幂等性。 举个例子吧。假设你有个系统，消费一条消息就往数据库里插入一条数据，要是你一个消息重复两次，你不就插入了两条，这数据不就错了？但是你要是消费到第二次的时候，自己判断一下是否已经消费过了，若是就直接扔了，这样不就保留了一条数据，从而保证了数据的正确性。 一条数据重复出现两次，数据库里就只有一条数据，这就保证了系统的幂等性。 幂等性，通俗点说，就一个数据，或者一个请求，给你重复来多次，你得确保对应的数据是不会改变的，不能出错。 所以第二个问题来了，怎么保证消息队列消费的幂等性？ 其实还是得结合业务来思考，我这里给几个思路： 比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update 一下好吧。 比如你是写 Redis，那没问题了，反正每次都是 set，天然幂等性。 比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据的时候，里面加一个全局唯一的 id，类似订单 id 之类的东西，然后你这里消费到了之后，先根据这个 id 去比如 Redis 里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个 id 写 Redis。如果消费过了，那你就别处理了，保证别重复处理相同的消息即可。 比如基于数据库的唯一键来保证重复数据不会重复插入多条。因为有唯一键约束了，重复数据插入只会报错，不会导致数据库中出现脏数据。 当然，如何保证 MQ 的消费是幂等性的，需要结合具体的业务来看。 面试题如何保证消息的顺序性？ 面试官心理分析其实这个也是用 MQ 的时候必问的话题，第一看看你了不了解顺序这个事儿？第二看看你有没有办法保证消息是有顺序的？这是生产系统中常见的问题。 面试题剖析我举个例子，我们以前做过一个 mysql binlog 同步的系统，压力还是非常大的，日同步数据要达到上亿，就是说数据从一个 mysql 库原封不动地同步到另一个 mysql 库里面去（mysql -&gt; mysql）。常见的一点在于说比如大数据 team，就需要同步一个 mysql 库过来，对公司的业务系统的数据做各种复杂的操作。 你在 mysql 里增删改一条数据，对应出来了增删改 3 条 binlog 日志，接着这三条 binlog 发送到 MQ 里面，再消费出来依次执行，起码得保证人家是按照顺序来的吧？不然本来是：增加、修改、删除；你楞是换了顺序给执行成删除、修改、增加，不全错了么。 本来这个数据同步过来，应该最后这个数据被删除了；结果你搞错了这个顺序，最后这个数据保留下来了，数据同步就出错了。 先看看顺序会错乱的俩场景： RabbitMQ：一个 queue，多个 consumer。比如，生产者向 RabbitMQ 里发送了三条数据，顺序依次是 data1/data2/data3，压入的是 RabbitMQ 的一个内存队列。有三个消费者分别从 MQ 中消费这三条数据中的一条，结果消费者2先执行完操作，把 data2 存入数据库，然后是 data1/data3。这不明显乱了。 Kafka：比如说我们建了一个 topic，有三个 partition。生产者在写的时候，其实可以指定一个 key，比如说我们指定了某个订单 id 作为 key，那么这个订单相关的数据，一定会被分发到同一个 partition 中去，而且这个 partition 中的数据一定是有顺序的。消费者从 partition 中取出来数据的时候，也一定是有顺序的。到这里，顺序还是 ok 的，没有错乱。接着，我们在消费者里可能会搞多个线程来并发处理消息。因为如果消费者是单线程消费处理，而处理比较耗时的话，比如处理一条消息耗时几十 ms，那么 1 秒钟只能处理几十条消息，这吞吐量太低了。而多个线程并发跑的话，顺序可能就乱掉了。 解决方案RabbitMQ拆分多个 queue，每个 queue 一个 consumer，就是多一些 queue 而已，确实是麻烦点；或者就一个 queue 但是对应一个 consumer，然后这个 consumer 内部用内存队列做排队，然后分发给底层不同的 worker 来处理。 Kafka 一个 topic，一个 partition，一个 consumer，内部单线程消费，单线程吞吐量太低，一般不会用这个。 写 N 个内存 queue，具有相同 key 的数据都到同一个内存 queue；然后对于 N 个线程，每个线程分别消费一个内存 queue 即可，这样就能保证顺序性。 面试题 如何保证消息的顺序性？ 面试官心理分析其实这个也是用 MQ 的时候必问的话题，第一看看你了不了解顺序这个事儿？第二看看你有没有办法保证消息是有顺序的？这是生产系统中常见的问题。 面试题剖析我举个例子，我们以前做过一个 mysql binlog 同步的系统，压力还是非常大的，日同步数据要达到上亿，就是说数据从一个 mysql 库原封不动地同步到另一个 mysql 库里面去（mysql -&gt; mysql）。常见的一点在于说比如大数据 team，就需要同步一个 mysql 库过来，对公司的业务系统的数据做各种复杂的操作。 你在 mysql 里增删改一条数据，对应出来了增删改 3 条 binlog 日志，接着这三条 binlog 发送到 MQ 里面，再消费出来依次执行，起码得保证人家是按照顺序来的吧？不然本来是：增加、修改、删除；你楞是换了顺序给执行成删除、修改、增加，不全错了么。 本来这个数据同步过来，应该最后这个数据被删除了；结果你搞错了这个顺序，最后这个数据保留下来了，数据同步就出错了。 先看看顺序会错乱的俩场景： RabbitMQ：一个 queue，多个 consumer。比如，生产者向 RabbitMQ 里发送了三条数据，顺序依次是 data1/data2/data3，压入的是 RabbitMQ 的一个内存队列。有三个消费者分别从 MQ 中消费这三条数据中的一条，结果消费者2先执行完操作，把 data2 存入数据库，然后是 data1/data3。这不明显乱了。 Kafka：比如说我们建了一个 topic，有三个 partition。生产者在写的时候，其实可以指定一个 key，比如说我们指定了某个订单 id 作为 key，那么这个订单相关的数据，一定会被分发到同一个 partition 中去，而且这个 partition 中的数据一定是有顺序的。消费者从 partition 中取出来数据的时候，也一定是有顺序的。到这里，顺序还是 ok 的，没有错乱。接着，我们在消费者里可能会搞多个线程来并发处理消息。因为如果消费者是单线程消费处理，而处理比较耗时的话，比如处理一条消息耗时几十 ms，那么 1 秒钟只能处理几十条消息，这吞吐量太低了。而多个线程并发跑的话，顺序可能就乱掉了。 解决方案RabbitMQ拆分多个 queue，每个 queue 一个 consumer，就是多一些 queue 而已，确实是麻烦点；或者就一个 queue 但是对应一个 consumer，然后这个 consumer 内部用内存队列做排队，然后分发给底层不同的 worker 来处理。 Kafka 一个 topic，一个 partition，一个 consumer，内部单线程消费，单线程吞吐量太低，一般不会用这个。 写 N 个内存 queue，具有相同 key 的数据都到同一个内存 queue；然后对于 N 个线程，每个线程分别消费一个内存 queue 即可，这样就能保证顺序性。 面试题如果让你写一个消息队列，该如何进行架构设计？说一下你的思路。 面试官心理分析其实聊到这个问题，一般面试官要考察两块： 你有没有对某一个消息队列做过较为深入的原理的了解，或者从整体了解把握住一个消息队列的架构原理。 看看你的设计能力，给你一个常见的系统，就是消息队列系统，看看你能不能从全局把握一下整体架构设计，给出一些关键点出来。 说实话，问类似问题的时候，大部分人基本都会蒙，因为平时从来没有思考过类似的问题，大多数人就是平时埋头用，从来不去思考背后的一些东西。类似的问题，比如，如果让你来设计一个 Spring 框架你会怎么做？如果让你来设计一个 Dubbo 框架你会怎么做？如果让你来设计一个 MyBatis 框架你会怎么做？ 面试题剖析其实回答这类问题，说白了，不求你看过那技术的源码，起码你要大概知道那个技术的基本原理、核心组成部分、基本架构构成，然后参照一些开源的技术把一个系统设计出来的思路说一下就好。 比如说这个消息队列系统，我们从以下几个角度来考虑一下： 首先这个 mq 得支持可伸缩性吧，就是需要的时候快速扩容，就可以增加吞吐量和容量，那怎么搞？设计个分布式的系统呗，参照一下 kafka 的设计理念，broker -&gt; topic -&gt; partition，每个 partition 放一个机器，就存一部分数据。如果现在资源不够了，简单啊，给 topic 增加 partition，然后做数据迁移，增加机器，不就可以存放更多数据，提供更高的吞吐量了？ 其次你得考虑一下这个 mq 的数据要不要落地磁盘吧？那肯定要了，落磁盘才能保证别进程挂了数据就丢了。那落磁盘的时候怎么落啊？顺序写，这样就没有磁盘随机读写的寻址开销，磁盘顺序读写的性能是很高的，这就是 kafka 的思路。 其次你考虑一下你的 mq 的可用性啊？这个事儿，具体参考之前可用性那个环节讲解的 kafka 的高可用保障机制。多副本 -&gt; leader &amp; follower -&gt; broker 挂了重新选举 leader 即可对外服务。 能不能支持数据 0 丢失啊？可以的，参考我们之前说的那个 kafka 数据零丢失方案。 mq 肯定是很复杂的，面试官问你这个问题，其实是个开放题，他就是看看你有没有从架构角度整体构思和设计的思维以及能力。确实这个问题可以刷掉一大批人，因为大部分人平时不思考这些东西。 面试题如何解决消息队列的延时以及过期失效问题？消息队列满了以后该怎么处理？有几百万消息持续积压几小时，说说怎么解决？ 面试官心理分析你看这问法，其实本质针对的场景，都是说，可能你的消费端出了问题，不消费了；或者消费的速度极其慢。接着就坑爹了，可能你的消息队列集群的磁盘都快写满了，都没人消费，这个时候怎么办？或者是这整个就积压了几个小时，你这个时候怎么办？或者是你积压的时间太长了，导致比如 RabbitMQ 设置了消息过期时间后就没了怎么办？ 所以就这事儿，其实线上挺常见的，一般不出，一出就是大 case。一般常见于，举个例子，消费端每次消费之后要写 mysql，结果 mysql 挂了，消费端 hang 那儿了，不动了；或者是消费端出了个什么岔子，导致消费速度极其慢。 面试题剖析关于这个事儿，我们一个一个来梳理吧，先假设一个场景，我们现在消费端出故障了，然后大量消息在 mq 里积压，现在出事故了，慌了。 大量消息在 mq 里积压了几个小时了还没解决几千万条数据在 MQ 里积压了七八个小时，从下午 4 点多，积压到了晚上 11 点多。这个是我们真实遇到过的一个场景，确实是线上故障了，这个时候要不然就是修复 consumer 的问题，让它恢复消费速度，然后傻傻的等待几个小时消费完毕。这个肯定不能在面试的时候说吧。 一个消费者一秒是 1000 条，一秒 3 个消费者是 3000 条，一分钟就是 18 万条。所以如果你积压了几百万到上千万的数据，即使消费者恢复了，也需要大概 1 小时的时间才能恢复过来。 一般这个时候，只能临时紧急扩容了，具体操作步骤和思路如下： 先修复 consumer 的问题，确保其恢复消费速度，然后将现有 consumer 都停掉。 新建一个 topic，partition 是原来的 10 倍，临时建立好原先 10 倍的 queue 数量。 然后写一个临时的分发数据的 consumer 程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的 10 倍数量的 queue。 接着临时征用 10 倍的机器来部署 consumer，每一批 consumer 消费一个临时 queue 的数据。这种做法相当于是临时将 queue 资源和 consumer 资源扩大 10 倍，以正常的 10 倍速度来消费数据。 等快速消费完积压数据之后，得恢复原先部署的架构，重新用原先的 consumer 机器来消费消息。 mq 中的消息过期失效了假设你用的是 RabbitMQ，RabbtiMQ 是可以设置过期时间的，也就是 TTL。如果消息在 queue 中积压超过一定的时间就会被 RabbitMQ 给清理掉，这个数据就没了。那这就是第二个坑了。这就不是说数据会大量积压在 mq 里，而是大量的数据会直接搞丢。 这个情况下，就不是说要增加 consumer 消费积压的消息，因为实际上没啥积压，而是丢了大量的消息。我们可以采取一个方案，就是批量重导，这个我们之前线上也有类似的场景干过。就是大量积压的时候，我们当时就直接丢弃数据了，然后等过了高峰期以后，比如大家一起喝咖啡熬夜到晚上12点以后，用户都睡觉了。这个时候我们就开始写程序，将丢失的那批数据，写个临时程序，一点一点的查出来，然后重新灌入 mq 里面去，把白天丢的数据给他补回来。也只能是这样了。 假设 1 万个订单积压在 mq 里面，没有处理，其中 1000 个订单都丢了，你只能手动写程序把那 1000 个订单给查出来，手动发到 mq 里去再补一次。 mq 都快写满了如果消息积压在 mq 里，你很长时间都没有处理掉，此时导致 mq 都快写满了，咋办？这个还有别的办法吗？没有，谁让你第一个方案执行的太慢了，你临时写程序，接入数据来消费，消费一个丢弃一个，都不要了，快速消费掉所有的消息。然后走第二个方案，到了晚上再补数据吧。 面试题你们有没有做 MySQL 读写分离？如何实现 MySQL 的读写分离？MySQL 主从复制原理的是啥？如何解决 MySQL 主从同步的延时问题？ 面试官心理分析高并发这个阶段，肯定是需要做读写分离的，啥意思？因为实际上大部分的互联网公司，一些网站，或者是 app，其实都是读多写少。所以针对这个情况，就是写一个主库，但是主库挂多个从库，然后从多个从库来读，那不就可以支撑更高的读并发压力了吗？ 面试题剖析如何实现 MySQL 的读写分离？其实很简单，就是基于主从复制架构，简单来说，就搞一个主库，挂多个从库，然后我们就单单只是写主库，然后主库会自动把数据给同步到从库上去。 MySQL 主从复制原理的是啥？主库将变更写入 binlog 日志，然后从库连接到主库之后，从库有一个 IO 线程，将主库的 binlog 日志拷贝到自己本地，写入一个 relay 中继日志中。接着从库中有一个 SQL 线程会从中继日志读取 binlog，然后执行 binlog 日志中的内容，也就是在自己本地再次执行一遍 SQL，这样就可以保证自己跟主库的数据是一样的。 这里有一个非常重要的一点，就是从库同步主库数据的过程是串行化的，也就是说主库上并行的操作，在从库上会串行执行。所以这就是一个非常重要的点了，由于从库从主库拷贝日志以及串行执行 SQL 的特点，在高并发场景下，从库的数据一定会比主库慢一些，是有延时的。所以经常出现，刚写入主库的数据可能是读不到的，要过几十毫秒，甚至几百毫秒才能读取到。 而且这里还有另外一个问题，就是如果主库突然宕机，然后恰好数据还没同步到从库，那么有些数据可能在从库上是没有的，有些数据可能就丢失了。 所以 MySQL 实际上在这一块有两个机制，一个是半同步复制，用来解决主库数据丢失问题；一个是并行复制，用来解决主从同步延时问题。 这个所谓半同步复制，也叫 semi-sync 复制，指的就是主库写入 binlog 日志之后，就会将强制此时立即将数据同步到从库，从库将日志写入自己本地的 relay log 之后，接着会返回一个 ack 给主库，主库接收到至少一个从库的 ack 之后才会认为写操作完成了。 所谓并行复制，指的是从库开启多个线程，并行读取 relay log 中不同库的日志，然后并行重放不同库的日志，这是库级别的并行。 MySQL 主从同步延时问题（精华）以前线上确实处理过因为主从同步延时问题而导致的线上的 bug，属于小型的生产事故。 是这个么场景。有个同学是这样写代码逻辑的。先插入一条数据，再把它查出来，然后更新这条数据。在生产环境高峰期，写并发达到了 2000/s，这个时候，主从复制延时大概是在小几十毫秒。线上会发现，每天总有那么一些数据，我们期望更新一些重要的数据状态，但在高峰期时候却没更新。用户跟客服反馈，而客服就会反馈给我们。 我们通过 MySQL 命令： 1show status 查看 Seconds_Behind_Master，可以看到从库复制主库的数据落后了几 ms。 一般来说，如果主从延迟较为严重，有以下解决方案： 分库，将一个主库拆分为多个主库，每个主库的写并发就减少了几倍，此时主从延迟可以忽略不计。 打开 MySQL 支持的并行复制，多个库并行复制。如果说某个库的写入并发就是特别高，单库写并发达到了 2000/s，并行复制还是没意义。 重写代码，写代码的同学，要慎重，插入数据时立马查询可能查不到。 如果确实是存在必须先插入，立马要求就查询到，然后立马就要反过来执行一些操作，对这个查询设置直连主库。不推荐这种方法，你要是这么搞，读写分离的意义就丧失了。 面试题了解什么是 redis 的雪崩、穿透和击穿？redis 崩溃之后会怎么样？系统该如何应对这种情况？如何处理 redis 的穿透？ 面试官心理分析其实这是问到缓存必问的，因为缓存雪崩和穿透，是缓存最大的两个问题，要么不出现，一旦出现就是致命性的问题，所以面试官一定会问你。 面试题剖析缓存雪崩对于系统 A，假设每天高峰期每秒 5000 个请求，本来缓存在高峰期可以扛住每秒 4000 个请求，但是缓存机器意外发生了全盘宕机。缓存挂了，此时 1 秒 5000 个请求全部落数据库，数据库必然扛不住，它会报一下警，然后就挂了。此时，如果没有采用什么特别的方案来处理这个故障，DBA 很着急，重启数据库，但是数据库立马又被新的流量给打死了。 这就是缓存雪崩。 大约在 3 年前，国内比较知名的一个互联网公司，曾因为缓存事故，导致雪崩，后台系统全部崩溃，事故从当天下午持续到晚上凌晨 3~4 点，公司损失了几千万。 缓存雪崩的事前事中事后的解决方案如下。 事前：redis 高可用，主从+哨兵，redis cluster，避免全盘崩溃。 事中：本地 ehcache 缓存 + hystrix 限流&amp;降级，避免 MySQL 被打死。 事后：redis 持久化，一旦重启，自动从磁盘上加载数据，快速恢复缓存数据。 用户发送一个请求，系统 A 收到请求后，先查本地 ehcache 缓存，如果没查到再查 redis。如果 ehcache 和 redis 都没有，再查数据库，将数据库中的结果，写入 ehcache 和 redis 中。 限流组件，可以设置每秒的请求，有多少能通过组件，剩余的未通过的请求，怎么办？走降级！可以返回一些默认的值，或者友情提示，或者空白的值。 好处： 数据库绝对不会死，限流组件确保了每秒只有多少个请求能通过。 只要数据库不死，就是说，对用户来说，2/5 的请求都是可以被处理的。 只要有 2/5 的请求可以被处理，就意味着你的系统没死，对用户来说，可能就是点击几次刷不出来页面，但是多点几次，就可以刷出来一次。 缓存穿透对于系统A，假设一秒 5000 个请求，结果其中 4000 个请求是黑客发出的恶意攻击。 黑客发出的那 4000 个攻击，缓存中查不到，每次你去数据库里查，也查不到。 举个栗子。数据库 id 是从 1 开始的，结果黑客发过来的请求 id 全部都是负数。这样的话，缓存中不会有，请求每次都“视缓存于无物”，直接查询数据库。这种恶意攻击场景的缓存穿透就会直接把数据库给打死。 解决方式很简单，每次系统 A 从数据库中只要没查到，就写一个空值到缓存里去，比如 set -999 UNKNOWN。然后设置一个过期时间，这样的话，下次有相同的 key 来访问的时候，在缓存失效之前，都可以直接从缓存中取数据。 缓存击穿缓存击穿，就是说某个 key 非常热点，访问非常频繁，处于集中式高并发访问的情况，当这个 key 在失效的瞬间，大量的请求就击穿了缓存，直接请求数据库，就像是在一道屏障上凿开了一个洞。 解决方式也很简单，可以将热点数据设置为永远不过期；或者基于 redis or zookeeper 实现互斥锁，等待第一个请求构建完缓存之后，再释放锁，进而其它请求才能通过该 key 访问数据。 面试题redis 的并发竞争问题是什么？如何解决这个问题？了解 redis 事务的 CAS 方案吗？ 面试官心理分析这个也是线上非常常见的一个问题，就是多客户端同时并发写一个 key，可能本来应该先到的数据后到了，导致数据版本错了；或者是多客户端同时获取一个 key，修改值之后再写回去，只要顺序错了，数据就错了。 而且 redis 自己就有天然解决这个问题的 CAS 类的乐观锁方案。 面试题剖析某个时刻，多个系统实例都去更新某个 key。可以基于 zookeeper 实现分布式锁。每个系统通过 zookeeper 获取分布式锁，确保同一时间，只能有一个系统实例在操作某个 key，别人都不允许读和写。 你要写入缓存的数据，都是从 mysql 里查出来的，都得写入 mysql 中，写入 mysql 中的时候必须保存一个时间戳，从 mysql 查出来的时候，时间戳也查出来。 每次要写之前，先判断一下当前这个 value 的时间戳是否比缓存里的 value 的时间戳要新。如果是的话，那么可以写，否则，就不能用旧的数据覆盖新的数据。 面试题redis 集群模式的工作原理能说一下么？在集群模式下，redis 的 key 是如何寻址的？分布式寻址都有哪些算法？了解一致性 hash 算法吗？ 面试官心理分析在前几年，redis 如果要搞几个节点，每个节点存储一部分的数据，得借助一些中间件来实现，比如说有 codis，或者 twemproxy，都有。有一些 redis 中间件，你读写 redis 中间件，redis 中间件负责将你的数据分布式存储在多台机器上的 redis 实例中。 这两年，redis 不断在发展，redis 也不断有新的版本，现在的 redis 集群模式，可以做到在多台机器上，部署多个 redis 实例，每个实例存储一部分的数据，同时每个 redis 主实例可以挂 redis 从实例，自动确保说，如果 redis 主实例挂了，会自动切换到 redis 从实例上来。 现在 redis 的新版本，大家都是用 redis cluster 的，也就是 redis 原生支持的 redis 集群模式，那么面试官肯定会就 redis cluster 对你来个几连炮。要是你没用过 redis cluster，正常，以前很多人用 codis 之类的客户端来支持集群，但是起码你得研究一下 redis cluster 吧。 如果你的数据量很少，主要是承载高并发高性能的场景，比如你的缓存一般就几个 G，单机就足够了，可以使用 replication，一个 master 多个 slaves，要几个 slave 跟你要求的读吞吐量有关，然后自己搭建一个 sentinel 集群去保证 redis 主从架构的高可用性。 redis cluster，主要是针对海量数据+高并发+高可用的场景。redis cluster 支撑 N 个 redis master node，每个 master node 都可以挂载多个 slave node。这样整个 redis 就可以横向扩容了。如果你要支撑更大数据量的缓存，那就横向扩容更多的 master 节点，每个 master 节点就能存放更多的数据了。 面试题剖析redis cluster 介绍 自动将数据进行分片，每个 master 上放一部分数据 提供内置的高可用支持，部分 master 不可用时，还是可以继续工作的 在 redis cluster 架构下，每个 redis 要放开两个端口号，比如一个是 6379，另外一个就是 加1w 的端口号，比如 16379。 16379 端口号是用来进行节点间通信的，也就是 cluster bus 的东西，cluster bus 的通信，用来进行故障检测、配置更新、故障转移授权。cluster bus 用了另外一种二进制的协议，gossip 协议，用于节点间进行高效的数据交换，占用更少的网络带宽和处理时间。 节点间的内部通信机制基本通信原理集群元数据的维护有两种方式：集中式、Gossip 协议。redis cluster 节点间采用 gossip 协议进行通信。 集中式是将集群元数据（节点信息、故障等等）几种存储在某个节点上。集中式元数据集中存储的一个典型代表，就是大数据领域的 storm。它是分布式的大数据实时计算引擎，是集中式的元数据存储的结构，底层基于 zookeeper（分布式协调的中间件）对所有元数据进行存储维护。 redis 维护集群元数据采用另一个方式， gossip 协议，所有节点都持有一份元数据，不同的节点如果出现了元数据的变更，就不断将元数据发送给其它的节点，让其它节点也进行元数据的变更。 集中式的好处在于，元数据的读取和更新，时效性非常好，一旦元数据出现了变更，就立即更新到集中式的存储中，其它节点读取的时候就可以感知到；不好在于，所有的元数据的更新压力全部集中在一个地方，可能会导致元数据的存储有压力。 gossip 好处在于，元数据的更新比较分散，不是集中在一个地方，更新请求会陆陆续续打到所有节点上去更新，降低了压力；不好在于，元数据的更新有延时，可能导致集群中的一些操作会有一些滞后。 10000 端口：每个节点都有一个专门用于节点间通信的端口，就是自己提供服务的端口号+10000，比如 7001，那么用于节点间通信的就是 17001 端口。每个节点每隔一段时间都会往另外几个节点发送 ping 消息，同时其它几个节点接收到 ping 之后返回 pong。 交换的信息：信息包括故障信息，节点的增加和删除，hash slot 信息等等。 gossip 协议gossip 协议包含多种消息，包含 ping,pong,meet,fail 等等。 meet：某个节点发送 meet 给新加入的节点，让新节点加入集群中，然后新节点就会开始与其它节点进行通信。 1redis-trib.rb add-node 其实内部就是发送了一个 gossip meet 消息给新加入的节点，通知那个节点去加入我们的集群。 ping：每个节点都会频繁给其它节点发送 ping，其中包含自己的状态还有自己维护的集群元数据，互相通过 ping 交换元数据。 pong：返回 ping 和 meeet，包含自己的状态和其它信息，也用于信息广播和更新。 fail：某个节点判断另一个节点 fail 之后，就发送 fail 给其它节点，通知其它节点说，某个节点宕机啦。 ping 消息深入ping 时要携带一些元数据，如果很频繁，可能会加重网络负担。 每个节点每秒会执行 10 次 ping，每次会选择 5 个最久没有通信的其它节点。当然如果发现某个节点通信延时达到了 cluster_node_timeout / 2，那么立即发送 ping，避免数据交换延时过长，落后的时间太长了。比如说，两个节点之间都 10 分钟没有交换数据了，那么整个集群处于严重的元数据不一致的情况，就会有问题。所以 cluster_node_timeout 可以调节，如果调得比较大，那么会降低 ping 的频率。 每次 ping，会带上自己节点的信息，还有就是带上 1/10 其它节点的信息，发送出去，进行交换。至少包含 3 个其它节点的信息，最多包含 总节点数减 2 个其它节点的信息。 分布式寻址算法 hash 算法（大量缓存重建） 一致性 hash 算法（自动缓存迁移）+ 虚拟节点（自动负载均衡） redis cluster 的 hash slot 算法 hash 算法来了一个 key，首先计算 hash 值，然后对节点数取模。然后打在不同的 master 节点上。一旦某一个 master 节点宕机，所有请求过来，都会基于最新的剩余 master 节点数去取模，尝试去取数据。这会导致大部分的请求过来，全部无法拿到有效的缓存，导致大量的流量涌入数据库。 一致性 hash 算法一致性 hash 算法将整个 hash 值空间组织成一个虚拟的圆环，整个空间按顺时针方向组织，下一步将各个 master 节点（使用服务器的 ip 或主机名）进行 hash。这样就能确定每个节点在其哈希环上的位置。 来了一个 key，首先计算 hash 值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，遇到的第一个 master 节点就是 key 所在位置。 在一致性哈希算法中，如果一个节点挂了，受影响的数据仅仅是此节点到环空间前一个节点（沿着逆时针方向行走遇到的第一个节点）之间的数据，其它不受影响。增加一个节点也同理。 燃鹅，一致性哈希算法在节点太少时，容易因为节点分布不均匀而造成缓存热点的问题。为了解决这种热点问题，一致性 hash 算法引入了虚拟节点机制，即对每一个节点计算多个 hash，每个计算结果位置都放置一个虚拟节点。这样就实现了数据的均匀分布，负载均衡。 redis cluster 的 hash slot 算法redis cluster 有固定的 16384 个 hash slot，对每个 key 计算 CRC16 值，然后对 16384 取模，可以获取 key 对应的 hash slot。 redis cluster 中每个 master 都会持有部分 slot，比如有 3 个 master，那么可能每个 master 持有 5000 多个 hash slot。hash slot 让 node 的增加和移除很简单，增加一个 master，就将其他 master 的 hash slot 移动部分过去，减少一个 master，就将它的 hash slot 移动到其他 master 上去。移动 hash slot 的成本是非常低的。客户端的 api，可以对指定的数据，让他们走同一个 hash slot，通过 hash tag 来实现。 任何一台机器宕机，另外两个节点，不影响的。因为 key 找的是 hash slot，不是机器。 redis cluster 的高可用与主备切换原理redis cluster 的高可用的原理，几乎跟哨兵是类似的。 判断节点宕机如果一个节点认为另外一个节点宕机，那么就是 pfail，主观宕机。如果多个节点都认为另外一个节点宕机了，那么就是 fail，客观宕机，跟哨兵的原理几乎一样，sdown，odown。 在 cluster-node-timeout 内，某个节点一直没有返回 pong，那么就被认为 pfail。 如果一个节点认为某个节点 pfail 了，那么会在 gossip ping 消息中，ping 给其他节点，如果超过半数的节点都认为 pfail 了，那么就会变成 fail。 从节点过滤对宕机的 master node，从其所有的 slave node 中，选择一个切换成 master node。 检查每个 slave node 与 master node 断开连接的时间，如果超过了 cluster-node-timeout * cluster-slave-validity-factor，那么就没有资格切换成 master。 从节点选举每个从节点，都根据自己对 master 复制数据的 offset，来设置一个选举时间，offset 越大（复制数据越多）的从节点，选举时间越靠前，优先进行选举。 所有的 master node 开始 slave 选举投票，给要进行选举的 slave 进行投票，如果大部分 master node（N/2 + 1）都投票给了某个从节点，那么选举通过，那个从节点可以切换成 master。 从节点执行主备切换，从节点切换为主节点。 与哨兵比较整个流程跟哨兵相比，非常类似，所以说，redis cluster 功能强大，直接集成了 replication 和 sentinel 的功能。 面试题如何保证缓存与数据库的双写一致性？ 面试官心理分析你只要用缓存，就可能会涉及到缓存与数据库双存储双写，你只要是双写，就一定会有数据一致性的问题，那么你如何解决一致性问题？ 面试题剖析一般来说，如果允许缓存可以稍微的跟数据库偶尔有不一致的情况，也就是说如果你的系统不是严格要求 “缓存+数据库” 必须保持一致性的话，最好不要做这个方案，即：读请求和写请求串行化，串到一个内存队列里去。 串行化可以保证一定不会出现不一致的情况，但是它也会导致系统的吞吐量大幅度降低，用比正常情况下多几倍的机器去支撑线上的一个请求。 Cache Aside Pattern最经典的缓存+数据库读写的模式，就是 Cache Aside Pattern。 读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。 更新的时候，先更新数据库，然后再删除缓存。 为什么是删除缓存，而不是更新缓存？ 原因很简单，很多时候，在复杂点的缓存场景，缓存不单单是数据库中直接取出来的值。 比如可能更新了某个表的一个字段，然后其对应的缓存，是需要查询另外两个表的数据并进行运算，才能计算出缓存最新的值的。 另外更新缓存的代价有时候是很高的。是不是说，每次修改数据库的时候，都一定要将其对应的缓存更新一份？也许有的场景是这样，但是对于比较复杂的缓存数据计算的场景，就不是这样了。如果你频繁修改一个缓存涉及的多个表，缓存也频繁更新。但是问题在于，这个缓存到底会不会被频繁访问到？ 举个栗子，一个缓存涉及的表的字段，在 1 分钟内就修改了 20 次，或者是 100 次，那么缓存更新 20 次、100 次；但是这个缓存在 1 分钟内只被读取了 1 次，有大量的冷数据。实际上，如果你只是删除缓存的话，那么在 1 分钟内，这个缓存不过就重新计算一次而已，开销大幅度降低。用到缓存才去算缓存。 其实删除缓存，而不是更新缓存，就是一个 lazy 计算的思想，不要每次都重新做复杂的计算，不管它会不会用到，而是让它到需要被使用的时候再重新计算。像 mybatis，hibernate，都有懒加载思想。查询一个部门，部门带了一个员工的 list，没有必要说每次查询部门，都里面的 1000 个员工的数据也同时查出来啊。80% 的情况，查这个部门，就只是要访问这个部门的信息就可以了。先查部门，同时要访问里面的员工，那么这个时候只有在你要访问里面的员工的时候，才会去数据库里面查询 1000 个员工。 最初级的缓存不一致问题及解决方案问题：先更新数据库，再删除缓存。如果删除缓存失败了，那么会导致数据库中是新数据，缓存中是旧数据，数据就出现了不一致。 解决思路：先删除缓存，再更新数据库。如果数据库更新失败了，那么数据库中是旧数据，缓存中是空的，那么数据不会不一致。因为读的时候缓存没有，所以去读了数据库中的旧数据，然后更新到缓存中。 比较复杂的数据不一致问题分析数据发生了变更，先删除了缓存，然后要去修改数据库，此时还没修改。一个请求过来，去读缓存，发现缓存空了，去查询数据库，查到了修改前的旧数据，放到了缓存中。随后数据变更的程序完成了数据库的修改。完了，数据库和缓存中的数据不一样了… 为什么上亿流量高并发场景下，缓存会出现这个问题？ 只有在对一个数据在并发的进行读写的时候，才可能会出现这种问题。其实如果说你的并发量很低的话，特别是读并发很低，每天访问量就 1 万次，那么很少的情况下，会出现刚才描述的那种不一致的场景。但是问题是，如果每天的是上亿的流量，每秒并发读是几万，每秒只要有数据更新的请求，就可能会出现上述的数据库+缓存不一致的情况。 解决方案如下： 更新数据的时候，根据数据的唯一标识，将操作路由之后，发送到一个 jvm 内部队列中。读取数据的时候，如果发现数据不在缓存中，那么将重新读取数据+更新缓存的操作，根据唯一标识路由之后，也发送同一个 jvm 内部队列中。 一个队列对应一个工作线程，每个工作线程串行拿到对应的操作，然后一条一条的执行。这样的话，一个数据变更的操作，先删除缓存，然后再去更新数据库，但是还没完成更新。此时如果一个读请求过来，没有读到缓存，那么可以先将缓存更新的请求发送到队列中，此时会在队列中积压，然后同步等待缓存更新完成。 这里有一个优化点，一个队列中，其实多个更新缓存请求串在一起是没意义的，因此可以做过滤，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可。 待那个队列对应的工作线程完成了上一个操作的数据库的修改之后，才会去执行下一个操作，也就是缓存更新的操作，此时会从数据库中读取最新的值，然后写入缓存中。 如果请求还在等待时间范围内，不断轮询发现可以取到值了，那么就直接返回；如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的旧值。 高并发的场景下，该解决方案要注意的问题： 读请求长时阻塞 由于读请求进行了非常轻度的异步化，所以一定要注意读超时的问题，每个读请求必须在超时时间范围内返回。 该解决方案，最大的风险点在于说，可能数据更新很频繁，导致队列中积压了大量更新操作在里面，然后读请求会发生大量的超时，最后导致大量的请求直接走数据库。务必通过一些模拟真实的测试，看看更新数据的频率是怎样的。 另外一点，因为一个队列中，可能会积压针对多个数据项的更新操作，因此需要根据自己的业务情况进行测试，可能需要部署多个服务，每个服务分摊一些数据的更新操作。如果一个内存队列里居然会挤压 100 个商品的库存修改操作，每隔库存修改操作要耗费 10ms 去完成，那么最后一个商品的读请求，可能等待 10 * 100 = 1000ms = 1s 后，才能得到数据，这个时候就导致读请求的长时阻塞。 一定要做根据实际业务系统的运行情况，去进行一些压力测试，和模拟线上环境，去看看最繁忙的时候，内存队列可能会挤压多少更新操作，可能会导致最后一个更新操作对应的读请求，会 hang 多少时间，如果读请求在 200ms 返回，如果你计算过后，哪怕是最繁忙的时候，积压 10 个更新操作，最多等待 200ms，那还可以的。 如果一个内存队列中可能积压的更新操作特别多，那么你就要加机器，让每个机器上部署的服务实例处理更少的数据，那么每个内存队列中积压的更新操作就会越少。 其实根据之前的项目经验，一般来说，数据的写频率是很低的，因此实际上正常来说，在队列中积压的更新操作应该是很少的。像这种针对读高并发、读缓存架构的项目，一般来说写请求是非常少的，每秒的 QPS 能到几百就不错了。 我们来实际粗略测算一下。 如果一秒有 500 的写操作，如果分成 5 个时间片，每 200ms 就 100 个写操作，放到 20 个内存队列中，每个内存队列，可能就积压 5 个写操作。每个写操作性能测试后，一般是在 20ms 左右就完成，那么针对每个内存队列的数据的读请求，也就最多 hang 一会儿，200ms 以内肯定能返回了。 经过刚才简单的测算，我们知道，单机支撑的写 QPS 在几百是没问题的，如果写 QPS 扩大了 10 倍，那么就扩容机器，扩容 10 倍的机器，每个机器 20 个队列。 读请求并发量过高 这里还必须做好压力测试，确保恰巧碰上上述情况的时候，还有一个风险，就是突然间大量读请求会在几十毫秒的延时 hang 在服务上，看服务能不能扛的住，需要多少机器才能扛住最大的极限情况的峰值。 但是因为并不是所有的数据都在同一时间更新，缓存也不会同一时间失效，所以每次可能也就是少数数据的缓存失效了，然后那些数据对应的读请求过来，并发量应该也不会特别大。 多服务实例部署的请求路由 可能这个服务部署了多个实例，那么必须保证说，执行数据更新操作，以及执行缓存更新操作的请求，都通过 Nginx 服务器路由到相同的服务实例上。 比如说，对同一个商品的读写请求，全部路由到同一台机器上。可以自己去做服务间的按照某个请求参数的 hash 路由，也可以用 Nginx 的 hash 路由功能等等。 热点商品的路由问题，导致请求的倾斜 万一某个商品的读写请求特别高，全部打到相同的机器的相同的队列里面去了，可能会造成某台机器的压力过大。就是说，因为只有在商品数据更新的时候才会清空缓存，然后才会导致读写并发，所以其实要根据业务系统去看，如果更新频率不是太高的话，这个问题的影响并不是特别大，但是的确可能某些机器的负载会高一些。 面试题项目中缓存是如何使用的？为什么要用缓存？缓存使用不当会造成什么后果？ 面试官心理分析这个问题，互联网公司必问，要是一个人连缓存都不太清楚，那确实比较尴尬。 只要问到缓存，上来第一个问题，肯定是先问问你项目哪里用了缓存？为啥要用？不用行不行？如果用了以后可能会有什么不良的后果？ 这就是看看你对缓存这个东西背后有没有思考，如果你就是傻乎乎的瞎用，没法给面试官一个合理的解答，那面试官对你印象肯定不太好，觉得你平时思考太少，就知道干活儿。 面试题剖析项目中缓存是如何使用的？这个，需要结合自己项目的业务来。 为什么要用缓存？用缓存，主要有两个用途：高性能、高并发。 高性能假设这么个场景，你有个操作，一个请求过来，吭哧吭哧你各种乱七八糟操作 mysql，半天查出来一个结果，耗时 600ms。但是这个结果可能接下来几个小时都不会变了，或者变了也可以不用立即反馈给用户。那么此时咋办？ 缓存啊，折腾 600ms 查出来的结果，扔缓存里，一个 key 对应一个 value，下次再有人查，别走 mysql 折腾 600ms 了，直接从缓存里，通过一个 key 查出来一个 value，2ms 搞定。性能提升 300 倍。 就是说对于一些需要复杂操作耗时查出来的结果，且确定后面不怎么变化，但是有很多读请求，那么直接将查询出来的结果放在缓存中，后面直接读缓存就好。 高并发mysql 这么重的数据库，压根儿设计不是让你玩儿高并发的，虽然也可以玩儿，但是天然支持不好。mysql 单机支撑到 2000QPS 也开始容易报警了。 所以要是你有个系统，高峰期一秒钟过来的请求有 1万，那一个 mysql 单机绝对会死掉。你这个时候就只能上缓存，把很多数据放缓存，别放 mysql。缓存功能简单，说白了就是 key-value 式操作，单机支撑的并发量轻松一秒几万十几万，支撑高并发 so easy。单机承载并发量是 mysql 单机的几十倍。 缓存是走内存的，内存天然就支撑高并发。 用了缓存之后会有什么不良后果？常见的缓存问题有以下几个： 缓存与数据库双写不一致 缓存雪崩、缓存穿透 缓存并发竞争 后面再详细说明。 面试题redis 都有哪些数据类型？分别在哪些场景下使用比较合适？ 面试官心理分析除非是面试官感觉看你简历，是工作 3 年以内的比较初级的同学，可能对技术没有很深入的研究，面试官才会问这类问题。否则，在宝贵的面试时间里，面试官实在不想多问。 其实问这个问题，主要有两个原因： 看看你到底有没有全面的了解 redis 有哪些功能，一般怎么来用，啥场景用什么，就怕你别就会最简单的 KV 操作； 看看你在实际项目里都怎么玩儿过 redis。 要是你回答的不好，没说出几种数据类型，也没说什么场景，你完了，面试官对你印象肯定不好，觉得你平时就是做个简单的 set 和 get。 面试题剖析redis 主要有以下几种数据类型： string hash list set sorted set string这是最简单的类型，就是普通的 set 和 get，做简单的 KV 缓存。 1set college szu hash这个是类似 map 的一种结构，这个一般就是可以将结构化的数据，比如一个对象（前提是这个对象没嵌套其他的对象）给缓存在 redis 里，然后每次读写缓存的时候，可以就操作 hash 里的某个字段。 1234hset person name bingohset person age 20hset person id 1hget person name 12345person = &#123; "name": "bingo", "age": 20, "id": 1&#125; listlist 是有序列表，这个可以玩儿出很多花样。 比如可以通过 list 存储一些列表型的数据结构，类似粉丝列表、文章的评论列表之类的东西。 比如可以通过 lrange 命令，读取某个闭区间内的元素，可以基于 list 实现分页查询，这个是很棒的一个功能，基于 redis 实现简单的高性能分页，可以做类似微博那种下拉不断分页的东西，性能高，就一页一页走。 12# 0开始位置，-1结束位置，结束位置为-1时，表示列表的最后一个位置，即查看所有。lrange mylist 0 -1 比如可以搞个简单的消息队列，从 list 头怼进去，从 list 尾巴那里弄出来。 123456lpush mylist 1lpush mylist 2lpush mylist 3 4 5# 1rpop mylist setset 是无序集合，自动去重。 直接基于 set 将系统里需要去重的数据扔进去，自动就给去重了，如果你需要对一些数据进行快速的全局去重，你当然也可以基于 jvm 内存里的 HashSet 进行去重，但是如果你的某个系统部署在多台机器上呢？得基于 redis 进行全局的 set 去重。 可以基于 set 玩儿交集、并集、差集的操作，比如交集吧，可以把两个人的粉丝列表整一个交集，看看俩人的共同好友是谁？对吧。 把两个大 V 的粉丝都放在两个 set 中，对两个 set 做交集。 1234567891011121314151617181920212223242526272829303132#-------操作一个set-------# 添加元素sadd mySet 1# 查看全部元素smembers mySet# 判断是否包含某个值sismember mySet 3# 删除某个/些元素srem mySet 1srem mySet 2 4# 查看元素个数scard mySet# 随机删除一个元素spop mySet#-------操作多个set-------# 将一个set的元素移动到另外一个setsmove yourSet mySet 2# 求两set的交集sinter yourSet mySet# 求两set的并集sunion yourSet mySet# 求在yourSet中而不在mySet中的元素sdiff yourSet mySet sorted setsorted set 是排序的 set，去重但可以排序，写进去的时候给一个分数，自动根据分数排序。 12345678910zadd board 85 zhangsanzadd board 72 lisizadd board 96 wangwuzadd board 63 zhaoliu# 获取排名前三的用户（默认是升序，所以需要 rev 改为降序）zrevrange board 0 3# 获取某用户的排名zrank board zhaoliu 面试题redis 的过期策略都有哪些？内存淘汰机制都有哪些？手写一下 LRU 代码实现？ 面试官心理分析如果你连这个问题都不知道，上来就懵了，回答不出来，那线上你写代码的时候，想当然的认为写进 redis 的数据就一定会存在，后面导致系统各种 bug，谁来负责？ 常见的有两个问题： 往 redis 写入的数据怎么没了？ 可能有同学会遇到，在生产环境的 redis 经常会丢掉一些数据，写进去了，过一会儿可能就没了。我的天，同学，你问这个问题就说明 redis 你就没用对啊。redis 是缓存，你给当存储了是吧？ 啥叫缓存？用内存当缓存。内存是无限的吗，内存是很宝贵而且是有限的，磁盘是廉价而且是大量的。可能一台机器就几十个 G 的内存，但是可以有几个 T 的硬盘空间。redis 主要是基于内存来进行高性能、高并发的读写操作的。 那既然内存是有限的，比如 redis 就只能用 10G，你要是往里面写了 20G 的数据，会咋办？当然会干掉 10G 的数据，然后就保留 10G 的数据了。那干掉哪些数据？保留哪些数据？当然是干掉不常用的数据，保留常用的数据了。 数据明明过期了，怎么还占用着内存？ 这是由 redis 的过期策略来决定。 面试题剖析redis 过期策略redis 过期策略是：定期删除+惰性删除。 所谓定期删除，指的是 redis 默认是每隔 100ms 就随机抽取一些设置了过期时间的 key，检查其是否过期，如果过期就删除。 假设 redis 里放了 10w 个 key，都设置了过期时间，你每隔几百毫秒，就检查 10w 个 key，那 redis 基本上就死了，cpu 负载会很高的，消耗在你的检查过期 key 上了。注意，这里可不是每隔 100ms 就遍历所有的设置过期时间的 key，那样就是一场性能上的灾难。实际上 redis 是每隔 100ms 随机抽取一些 key 来检查和删除的。 但是问题是，定期删除可能会导致很多过期 key 到了时间并没有被删除掉，那咋整呢？所以就是惰性删除了。这就是说，在你获取某个 key 的时候，redis 会检查一下 ，这个 key 如果设置了过期时间那么是否过期了？如果过期了此时就会删除，不会给你返回任何东西。 获取 key 的时候，如果此时 key 已经过期，就删除，不会返回任何东西。 但是实际上这还是有问题的，如果定期删除漏掉了很多过期 key，然后你也没及时去查，也就没走惰性删除，此时会怎么样？如果大量过期 key 堆积在内存里，导致 redis 内存块耗尽了，咋整？ 答案是：走内存淘汰机制。 内存淘汰机制redis 内存淘汰机制有以下几个： noeviction: 当内存不足以容纳新写入数据时，新写入操作会报错，这个一般没人用吧，实在是太恶心了。 allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key（这个是最常用的）。 allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个 key，这个一般没人用吧，为啥要随机，肯定是把最近最少使用的 key 给干掉啊。 volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的 key（这个一般不太合适）。 volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个 key。 volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的 key 优先移除。 手写一个 LRU 算法你可以现场手写最原始的 LRU 算法，那个代码量太大了，似乎不太现实。 不求自己纯手工从底层开始打造出自己的 LRU，但是起码要知道如何利用已有的 JDK 数据结构实现一个 Java 版的 LRU。 1234567891011121314151617181920class LRUCache&lt;K, V&gt; extends LinkedHashMap&lt;K, V&gt; &#123; private final int CACHE_SIZE; /** * 传递进来最多能缓存多少数据 * * @param cacheSize 缓存大小 */ public LRUCache(int cacheSize) &#123; // true 表示让 linkedHashMap 按照访问顺序来进行排序，最近访问的放在头部，最老访问的放在尾部。 super((int) Math.ceil(cacheSize / 0.75) + 1, 0.75f, true); CACHE_SIZE = cacheSize; &#125; @Override protected boolean removeEldestEntry(Map.Entry&lt;K, V&gt; eldest) &#123; // 当 map中的数据量大于指定的缓存个数的时候，就自动删除最老的数据。 return size() &gt; CACHE_SIZE; &#125;&#125; Redis 主从架构单机的 redis，能够承载的 QPS 大概就在上万到几万不等。对于缓存来说，一般都是用来支撑读高并发的。因此架构做成主从(master-slave)架构，一主多从，主负责写，并且将数据复制到其它的 slave 节点，从节点负责读。所有的读请求全部走从节点。这样也可以很轻松实现水平扩容，支撑读高并发。 redis replication -&gt; 主从架构 -&gt; 读写分离 -&gt; 水平扩容支撑读高并发 redis replication 的核心机制 redis 采用异步方式复制数据到 slave 节点，不过 redis2.8 开始，slave node 会周期性地确认自己每次复制的数据量； 一个 master node 是可以配置多个 slave node 的； slave node 也可以连接其他的 slave node； slave node 做复制的时候，不会 block master node 的正常工作； slave node 在做复制的时候，也不会 block 对自己的查询操作，它会用旧的数据集来提供服务；但是复制完成的时候，需要删除旧数据集，加载新数据集，这个时候就会暂停对外服务了； slave node 主要用来进行横向扩容，做读写分离，扩容的 slave node 可以提高读的吞吐量。 注意，如果采用了主从架构，那么建议必须开启 master node 的持久化，不建议用 slave node 作为 master node 的数据热备，因为那样的话，如果你关掉 master 的持久化，可能在 master 宕机重启的时候数据是空的，然后可能一经过复制， slave node 的数据也丢了。 另外，master 的各种备份方案，也需要做。万一本地的所有文件丢失了，从备份中挑选一份 rdb 去恢复 master，这样才能确保启动的时候，是有数据的，即使采用了后续讲解的高可用机制，slave node 可以自动接管 master node，但也可能 sentinel 还没检测到 master failure，master node 就自动重启了，还是可能导致上面所有的 slave node 数据被清空。 redis 主从复制的核心原理当启动一个 slave node 的时候，它会发送一个 PSYNC 命令给 master node。 如果这是 slave node 初次连接到 master node，那么会触发一次 full resynchronization 全量复制。此时 master 会启动一个后台线程，开始生成一份 RDB 快照文件，同时还会将从客户端 client 新收到的所有写命令缓存在内存中。RDB 文件生成完毕后， master 会将这个 RDB 发送给 slave，slave 会先写入本地磁盘，然后再从本地磁盘加载到内存中，接着 master 会将内存中缓存的写命令发送到 slave，slave 也会同步这些数据。slave node 如果跟 master node 有网络故障，断开了连接，会自动重连，连接之后 master node 仅会复制给 slave 部分缺少的数据。 主从复制的断点续传从 redis2.8 开始，就支持主从复制的断点续传，如果主从复制过程中，网络连接断掉了，那么可以接着上次复制的地方，继续复制下去，而不是从头开始复制一份。 master node 会在内存中维护一个 backlog，master 和 slave 都会保存一个 replica offset 还有一个 master run id，offset 就是保存在 backlog 中的。如果 master 和 slave 网络连接断掉了，slave 会让 master 从上次 replica offset 开始继续复制，如果没有找到对应的 offset，那么就会执行一次 resynchronization。 如果根据 host+ip 定位 master node，是不靠谱的，如果 master node 重启或者数据出现了变化，那么 slave node 应该根据不同的 run id 区分。 无磁盘化复制master 在内存中直接创建 RDB，然后发送给 slave，不会在自己本地落地磁盘了。只需要在配置文件中开启 repl-diskless-sync yes 即可。 1234repl-diskless-sync yes# 等待 5s 后再开始复制，因为要等更多 slave 重新连接过来repl-diskless-sync-delay 5 过期 key 处理slave 不会过期 key，只会等待 master 过期 key。如果 master 过期了一个 key，或者通过 LRU 淘汰了一个 key，那么会模拟一条 del 命令发送给 slave。 复制的完整流程slave node 启动时，会在自己本地保存 master node 的信息，包括 master node 的host和ip，但是复制流程没开始。 slave node 内部有个定时任务，每秒检查是否有新的 master node 要连接和复制，如果发现，就跟 master node 建立 socket 网络连接。然后 slave node 发送 ping 命令给 master node。如果 master 设置了 requirepass，那么 slave node 必须发送 masterauth 的口令过去进行认证。master node 第一次执行全量复制，将所有数据发给 slave node。而在后续，master node 持续将写命令，异步复制给 slave node。 全量复制 master 执行 bgsave ，在本地生成一份 rdb 快照文件。 master node 将 rdb 快照文件发送给 slave node，如果 rdb 复制时间超过 60秒（repl-timeout），那么 slave node 就会认为复制失败，可以适当调大这个参数(对于千兆网卡的机器，一般每秒传输 100MB，6G 文件，很可能超过 60s) master node 在生成 rdb 时，会将所有新的写命令缓存在内存中，在 slave node 保存了 rdb 之后，再将新的写命令复制给 slave node。 如果在复制期间，内存缓冲区持续消耗超过 64MB，或者一次性超过 256MB，那么停止复制，复制失败。 1client-output-buffer-limit slave 256MB 64MB 60 slave node 接收到 rdb 之后，清空自己的旧数据，然后重新加载 rdb 到自己的内存中，同时基于旧的数据版本对外提供服务。 如果 slave node 开启了 AOF，那么会立即执行 BGREWRITEAOF，重写 AOF。 增量复制 如果全量复制过程中，master-slave 网络连接断掉，那么 slave 重新连接 master 时，会触发增量复制。 master 直接从自己的 backlog 中获取部分丢失的数据，发送给 slave node，默认 backlog 就是 1MB。 master 就是根据 slave 发送的 psync 中的 offset 来从 backlog 中获取数据的。 heartbeat主从节点互相都会发送 heartbeat 信息。 master 默认每隔 10秒 发送一次 heartbeat，slave node 每隔 1秒 发送一个 heartbeat。 异步复制master 每次接收到写命令之后，先在内部写入数据，然后异步发送给 slave node。 redis 如何才能做到高可用如果系统在 365 天内，有 99.99% 的时间，都是可以哗哗对外提供服务的，那么就说系统是高可用的。 一个 slave 挂掉了，是不会影响可用性的，还有其它的 slave 在提供相同数据下的相同的对外的查询服务。 但是，如果 master node 死掉了，会怎么样？没法写数据了，写缓存的时候，全部失效了。slave node 还有什么用呢，没有 master 给它们复制数据了，系统相当于不可用了。 redis 的高可用架构，叫做 failover 故障转移，也可以叫做主备切换。 master node 在故障时，自动检测，并且将某个 slave node 自动切换为 master node 的过程，叫做主备切换。这个过程，实现了 redis 的主从架构下的高可用。 后面会详细说明 redis 基于哨兵的高可用性。 面试题redis 的持久化有哪几种方式？不同的持久化机制都有什么优缺点？持久化机制具体底层是如何实现的？ 面试官心理分析redis 如果仅仅只是将数据缓存在内存里面，如果 redis 宕机了再重启，内存里的数据就全部都弄丢了啊。你必须得用 redis 的持久化机制，将数据写入内存的同时，异步的慢慢的将数据写入磁盘文件里，进行持久化。 如果 redis 宕机重启，自动从磁盘上加载之前持久化的一些数据就可以了，也许会丢失少许数据，但是至少不会将所有数据都弄丢。 这个其实一样，针对的都是 redis 的生产环境可能遇到的一些问题，就是 redis 要是挂了再重启，内存里的数据不就全丢了？能不能重启的时候把数据给恢复了？ 面试题剖析持久化主要是做灾难恢复、数据恢复，也可以归类到高可用的一个环节中去，比如你 redis 整个挂了，然后 redis 就不可用了，你要做的事情就是让 redis 变得可用，尽快变得可用。 重启 redis，尽快让它对外提供服务，如果没做数据备份，这时候 redis 启动了，也不可用啊，数据都没了。 很可能说，大量的请求过来，缓存全部无法命中，在 redis 里根本找不到数据，这个时候就死定了，出现缓存雪崩问题。所有请求没有在 redis 命中，就会去 mysql 数据库这种数据源头中去找，一下子 mysql 承接高并发，然后就挂了… 如果你把 redis 持久化做好，备份和恢复方案做到企业级的程度，那么即使你的 redis 故障了，也可以通过备份数据，快速恢复，一旦恢复立即对外提供服务。 redis 持久化的两种方式 RDB：RDB 持久化机制，是对 redis 中的数据执行周期性的持久化。 AOF：AOF 机制对每条写入命令作为日志，以 append-only 的模式写入一个日志文件中，在 redis 重启的时候，可以通过回放 AOF 日志中的写入指令来重新构建整个数据集。 通过 RDB 或 AOF，都可以将 redis 内存中的数据给持久化到磁盘上面来，然后可以将这些数据备份到别的地方去，比如说阿里云等云服务。 如果 redis 挂了，服务器上的内存和磁盘上的数据都丢了，可以从云服务上拷贝回来之前的数据，放到指定的目录中，然后重新启动 redis，redis 就会自动根据持久化数据文件中的数据，去恢复内存中的数据，继续对外提供服务。 如果同时使用 RDB 和 AOF 两种持久化机制，那么在 redis 重启的时候，会使用 AOF 来重新构建数据，因为 AOF 中的数据更加完整。 RDB 优缺点 RDB 会生成多个数据文件，每个数据文件都代表了某一个时刻中 redis 的数据，这种多个数据文件的方式，非常适合做冷备，可以将这种完整的数据文件发送到一些远程的安全存储上去，比如说 Amazon 的 S3 云服务上去，在国内可以是阿里云的 ODPS 分布式存储上，以预定好的备份策略来定期备份 redis 中的数据。 RDB 对 redis 对外提供的读写服务，影响非常小，可以让 redis 保持高性能，因为 redis 主进程只需要 fork 一个子进程，让子进程执行磁盘 IO 操作来进行 RDB 持久化即可。 相对于 AOF 持久化机制来说，直接基于 RDB 数据文件来重启和恢复 redis 进程，更加快速。 如果想要在 redis 故障时，尽可能少的丢失数据，那么 RDB 没有 AOF 好。一般来说，RDB 数据快照文件，都是每隔 5 分钟，或者更长时间生成一次，这个时候就得接受一旦 redis 进程宕机，那么会丢失最近 5 分钟的数据。 RDB 每次在 fork 子进程来执行 RDB 快照数据文件生成的时候，如果数据文件特别大，可能会导致对客户端提供的服务暂停数毫秒，或者甚至数秒。 AOF 优缺点 AOF 可以更好的保护数据不丢失，一般 AOF 会每隔 1 秒，通过一个后台线程执行一次fsync操作，最多丢失 1 秒钟的数据。 AOF 日志文件以 append-only 模式写入，所以没有任何磁盘寻址的开销，写入性能非常高，而且文件不容易破损，即使文件尾部破损，也很容易修复。 AOF 日志文件即使过大的时候，出现后台重写操作，也不会影响客户端的读写。因为在 rewrite log 的时候，会对其中的指令进行压缩，创建出一份需要恢复数据的最小日志出来。在创建新日志文件的时候，老的日志文件还是照常写入。当新的 merge 后的日志文件 ready 的时候，再交换新老日志文件即可。 AOF 日志文件的命令通过非常可读的方式进行记录，这个特性非常适合做灾难性的误删除的紧急恢复。比如某人不小心用 flushall 命令清空了所有数据，只要这个时候后台 rewrite 还没有发生，那么就可以立即拷贝 AOF 文件，将最后一条 flushall 命令给删了，然后再将该 AOF 文件放回去，就可以通过恢复机制，自动恢复所有数据。 对于同一份数据来说，AOF 日志文件通常比 RDB 数据快照文件更大。 AOF 开启后，支持的写 QPS 会比 RDB 支持的写 QPS 低，因为 AOF 一般会配置成每秒 fsync 一次日志文件，当然，每秒一次 fsync，性能也还是很高的。（如果实时写入，那么 QPS 会大降，redis 性能会大大降低） 以前 AOF 发生过 bug，就是通过 AOF 记录的日志，进行数据恢复的时候，没有恢复一模一样的数据出来。所以说，类似 AOF 这种较为复杂的基于命令日志 / merge / 回放的方式，比基于 RDB 每次持久化一份完整的数据快照文件的方式，更加脆弱一些，容易有 bug。不过 AOF 就是为了避免 rewrite 过程导致的 bug，因此每次 rewrite 并不是基于旧的指令日志进行 merge 的，而是基于当时内存中的数据进行指令的重新构建，这样健壮性会好很多。 RDB 和 AOF 到底该如何选择 不要仅仅使用 RDB，因为那样会导致你丢失很多数据； 也不要仅仅使用 AOF，因为那样有两个问题：第一，你通过 AOF 做冷备，没有 RDB 做冷备来的恢复速度更快；第二，RDB 每次简单粗暴生成数据快照，更加健壮，可以避免 AOF 这种复杂的备份和恢复机制的 bug； redis 支持同时开启开启两种持久化方式，我们可以综合使用 AOF 和 RDB 两种持久化机制，用 AOF 来保证数据不丢失，作为数据恢复的第一选择; 用 RDB 来做不同程度的冷备，在 AOF 文件都丢失或损坏不可用的时候，还可以使用 RDB 来进行快速的数据恢复。 面试题生产环境中的 redis 是怎么部署的？ 面试官心理分析看看你了解不了解你们公司的 redis 生产集群的部署架构，如果你不了解，那么确实你就很失职了，你的 redis 是主从架构？集群架构？用了哪种集群方案？有没有做高可用保证？有没有开启持久化机制确保可以进行数据恢复？线上 redis 给几个 G 的内存？设置了哪些参数？压测后你们 redis 集群承载多少 QPS？ 兄弟，这些你必须是门儿清的，否则你确实是没好好思考过。 面试题剖析redis cluster，10 台机器，5 台机器部署了 redis 主实例，另外 5 台机器部署了 redis 的从实例，每个主实例挂了一个从实例，5 个节点对外提供读写服务，每个节点的读写高峰qps可能可以达到每秒 5 万，5 台机器最多是 25 万读写请求/s。 机器是什么配置？32G 内存+ 8 核 CPU + 1T 磁盘，但是分配给 redis 进程的是10g内存，一般线上生产环境，redis 的内存尽量不要超过 10g，超过 10g 可能会有问题。 5 台机器对外提供读写，一共有 50g 内存。 因为每个主实例都挂了一个从实例，所以是高可用的，任何一个主实例宕机，都会自动故障迁移，redis 从实例会自动变成主实例继续提供读写服务。 你往内存里写的是什么数据？每条数据的大小是多少？商品数据，每条数据是 10kb。100 条数据是 1mb，10 万条数据是 1g。常驻内存的是 200 万条商品数据，占用内存是 20g，仅仅不到总内存的 50%。目前高峰期每秒就是 3500 左右的请求量。 其实大型的公司，会有基础架构的 team 负责缓存集群的运维。 Redis 哨兵集群实现高可用哨兵的介绍sentinel，中文名是哨兵。哨兵是 redis 集群机构中非常重要的一个组件，主要有以下功能： 集群监控：负责监控 redis master 和 slave 进程是否正常工作。 消息通知：如果某个 redis 实例有故障，那么哨兵负责发送消息作为报警通知给管理员。 故障转移：如果 master node 挂掉了，会自动转移到 slave node 上。 配置中心：如果故障转移发生了，通知 client 客户端新的 master 地址。 哨兵用于实现 redis 集群的高可用，本身也是分布式的，作为一个哨兵集群去运行，互相协同工作。 故障转移时，判断一个 master node 是否宕机了，需要大部分的哨兵都同意才行，涉及到了分布式选举的问题。 即使部分哨兵节点挂掉了，哨兵集群还是能正常工作的，因为如果一个作为高可用机制重要组成部分的故障转移系统本身是单点的，那就很坑爹了。 哨兵的核心知识 哨兵至少需要 3 个实例，来保证自己的健壮性。 哨兵 + redis 主从的部署架构，是不保证数据零丢失的，只能保证 redis 集群的高可用性。 对于哨兵 + redis 主从这种复杂的部署架构，尽量在测试环境和生产环境，都进行充足的测试和演练。 哨兵集群必须部署 2 个以上节点，如果哨兵集群仅仅部署了 2 个哨兵实例，quorum = 1。 1234+----+ +----+| M1 |---------| R1 || S1 | | S2 |+----+ +----+ 配置 quorum=1，如果 master 宕机， s1 和 s2 中只要有 1 个哨兵认为 master 宕机了，就可以进行切换，同时 s1 和 s2 会选举出一个哨兵来执行故障转移。但是同时这个时候，需要 majority，也就是大多数哨兵都是运行的。 123452 个哨兵，majority=23 个哨兵，majority=24 个哨兵，majority=25 个哨兵，majority=3... 如果此时仅仅是 M1 进程宕机了，哨兵 s1 正常运行，那么故障转移是 OK 的。但是如果是整个 M1 和 S1 运行的机器宕机了，那么哨兵只有 1 个，此时就没有 majority 来允许执行故障转移，虽然另外一台机器上还有一个 R1，但是故障转移不会执行。 经典的 3 节点哨兵集群是这样的： 123456789 +----+ | M1 | | S1 | +----+ |+----+ | +----+| R2 |----+----| R3 || S2 | | S3 |+----+ +----+ 配置 quorum=2，如果 M1 所在机器宕机了，那么三个哨兵还剩下 2 个，S2 和 S3 可以一致认为 master 宕机了，然后选举出一个来执行故障转移，同时 3 个哨兵的 majority 是 2，所以还剩下的 2 个哨兵运行着，就可以允许执行故障转移。 redis 哨兵主备切换的数据丢失问题两种情况和导致数据丢失主备切换的过程，可能会导致数据丢失： 异步复制导致的数据丢失 因为 master-&gt;slave 的复制是异步的，所以可能有部分数据还没复制到 slave，master 就宕机了，此时这部分数据就丢失了。 脑裂导致的数据丢失 脑裂，也就是说，某个 master 所在机器突然脱离了正常的网络，跟其他 slave 机器不能连接，但是实际上 master 还运行着。此时哨兵可能就会认为 master 宕机了，然后开启选举，将其他 slave 切换成了 master。这个时候，集群里就会有两个 master ，也就是所谓的脑裂。 此时虽然某个 slave 被切换成了 master，但是可能 client 还没来得及切换到新的 master，还继续向旧 master 写数据。因此旧 master 再次恢复的时候，会被作为一个 slave 挂到新的 master 上去，自己的数据会清空，重新从新的 master 复制数据。而新的 master 并没有后来 client 写入的数据，因此，这部分数据也就丢失了。 数据丢失问题的解决方案进行如下配置： 12min-slaves-to-write 1min-slaves-max-lag 10 表示，要求至少有 1 个 slave，数据复制和同步的延迟不能超过 10 秒。 如果说一旦所有的 slave，数据复制和同步的延迟都超过了 10 秒钟，那么这个时候，master 就不会再接收任何请求了。 减少异步复制数据的丢失 有了 min-slaves-max-lag 这个配置，就可以确保说，一旦 slave 复制数据和 ack 延时太长，就认为可能 master 宕机后损失的数据太多了，那么就拒绝写请求，这样可以把 master 宕机时由于部分数据未同步到 slave 导致的数据丢失降低的可控范围内。 减少脑裂的数据丢失 如果一个 master 出现了脑裂，跟其他 slave 丢了连接，那么上面两个配置可以确保说，如果不能继续给指定数量的 slave 发送数据，而且 slave 超过 10 秒没有给自己 ack 消息，那么就直接拒绝客户端的写请求。因此在脑裂场景下，最多就丢失 10 秒的数据。 sdown 和 odown 转换机制 sdown 是主观宕机，就一个哨兵如果自己觉得一个 master 宕机了，那么就是主观宕机 odown 是客观宕机，如果 quorum 数量的哨兵都觉得一个 master 宕机了，那么就是客观宕机 sdown 达成的条件很简单，如果一个哨兵 ping 一个 master，超过了 is-master-down-after-milliseconds 指定的毫秒数之后，就主观认为 master 宕机了；如果一个哨兵在指定时间内，收到了 quorum 数量的其它哨兵也认为那个 master 是 sdown 的，那么就认为是 odown 了。 哨兵集群的自动发现机制哨兵互相之间的发现，是通过 redis 的 pub/sub 系统实现的，每个哨兵都会往 __sentinel__:hello 这个 channel 里发送一个消息，这时候所有其他哨兵都可以消费到这个消息，并感知到其他的哨兵的存在。 每隔两秒钟，每个哨兵都会往自己监控的某个 master+slaves 对应的 __sentinel__:hello channel 里发送一个消息，内容是自己的 host、ip 和 runid 还有对这个 master 的监控配置。 每个哨兵也会去监听自己监控的每个 master+slaves 对应的 __sentinel__:hello channel，然后去感知到同样在监听这个 master+slaves 的其他哨兵的存在。 每个哨兵还会跟其他哨兵交换对 master 的监控配置，互相进行监控配置的同步。 slave 配置的自动纠正哨兵会负责自动纠正 slave 的一些配置，比如 slave 如果要成为潜在的 master 候选人，哨兵会确保 slave 复制现有 master 的数据；如果 slave 连接到了一个错误的 master 上，比如故障转移之后，那么哨兵会确保它们连接到正确的 master 上。 slave-&gt;master 选举算法如果一个 master 被认为 odown 了，而且 majority 数量的哨兵都允许主备切换，那么某个哨兵就会执行主备切换操作，此时首先要选举一个 slave 来，会考虑 slave 的一些信息： 跟 master 断开连接的时长 slave 优先级 复制 offset run id 如果一个 slave 跟 master 断开连接的时间已经超过了 down-after-milliseconds 的 10 倍，外加 master 宕机的时长，那么 slave 就被认为不适合选举为 master。 1(down-after-milliseconds * 10) + milliseconds_since_master_is_in_SDOWN_state 接下来会对 slave 进行排序： 按照 slave 优先级进行排序，slave priority 越低，优先级就越高。 如果 slave priority 相同，那么看 replica offset，哪个 slave 复制了越多的数据，offset 越靠后，优先级就越高。 如果上面两个条件都相同，那么选择一个 run id 比较小的那个 slave。 quorum 和 majority每次一个哨兵要做主备切换，首先需要 quorum 数量的哨兵认为 odown，然后选举出一个哨兵来做切换，这个哨兵还需要得到 majority 哨兵的授权，才能正式执行切换。 如果 quorum &lt; majority，比如 5 个哨兵，majority 就是 3，quorum 设置为 2，那么就 3 个哨兵授权就可以执行切换。 但是如果 quorum &gt;= majority，那么必须 quorum 数量的哨兵都授权，比如 5 个哨兵，quorum 是 5，那么必须 5 个哨兵都同意授权，才能执行切换。 configuration epoch哨兵会对一套 redis master+slaves 进行监控，有相应的监控的配置。 执行切换的那个哨兵，会从要切换到的新 master（salve-&gt;master）那里得到一个 configuration epoch，这就是一个 version 号，每次切换的 version 号都必须是唯一的。 如果第一个选举出的哨兵切换失败了，那么其他哨兵，会等待 failover-timeout 时间，然后接替继续执行切换，此时会重新获取一个新的 configuration epoch，作为新的 version 号。 configuration 传播哨兵完成切换之后，会在自己本地更新生成最新的 master 配置，然后同步给其他的哨兵，就是通过之前说的 pub/sub 消息机制。 这里之前的 version 号就很重要了，因为各种消息都是通过一个 channel 去发布和监听的，所以一个哨兵完成一次新的切换之后，新的 master 配置是跟着新的 version 号的。其他的哨兵都是根据版本号的大小来更新自己的 master 配置的。 面试题redis 和 memcached 有什么区别？redis 的线程模型是什么？为什么 redis 单线程却能支撑高并发？ 面试官心理分析这个是问 redis 的时候，最基本的问题吧，redis 最基本的一个内部原理和特点，就是 redis 实际上是个单线程工作模型，你要是这个都不知道，那后面玩儿 redis 的时候，出了问题岂不是什么都不知道？ 还有可能面试官会问问你 redis 和 memcached 的区别，但是 memcached 是早些年各大互联网公司常用的缓存方案，但是现在近几年基本都是 redis，没什么公司用 memcached 了。 面试题剖析redis 和 memcached 有啥区别？redis 支持复杂的数据结构redis 相比 memcached 来说，拥有更多的数据结构，能支持更丰富的数据操作。如果需要缓存能够支持更复杂的结构和操作， redis 会是不错的选择。 redis 原生支持集群模式在 redis3.x 版本中，便能支持 cluster 模式，而 memcached 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据。 性能对比由于 redis 只使用单核，而 memcached 可以使用多核，所以平均每一个核上 redis 在存储小数据时比 memcached 性能更高。而在 100k 以上的数据中，memcached 性能要高于 redis。虽然 redis 最近也在存储大数据的性能上进行优化，但是比起 memcached，还是稍有逊色。 redis 的线程模型redis 内部使用文件事件处理器 file event handler，这个文件事件处理器是单线程的，所以 redis 才叫做单线程的模型。它采用 IO 多路复用机制同时监听多个 socket，将产生事件的 socket 压入内存队列中，事件分派器根据 socket 上的事件类型来选择对应的事件处理器进行处理。 文件事件处理器的结构包含 4 个部分： 多个 socket IO 多路复用程序 文件事件分派器 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器） 多个 socket 可能会并发产生不同的操作，每个操作对应不同的文件事件，但是 IO 多路复用程序会监听多个 socket，会将产生事件的 socket 放入队列中排队，事件分派器每次从队列中取出一个 socket，根据 socket 的事件类型交给对应的事件处理器进行处理。 来看客户端与 redis 的一次通信过程： 要明白，通信是通过 socket 来完成的，不懂的同学可以先去看一看 socket 网络编程。 首先，redis 服务端进程初始化的时候，会将 server socket 的 AE_READABLE 事件与连接应答处理器关联。 客户端 socket01 向 redis 进程的 server socket 请求建立连接，此时 server socket 会产生一个 AE_READABLE 事件，IO 多路复用程序监听到 server socket 产生的事件后，将该 socket 压入队列中。文件事件分派器从队列中获取 socket，交给连接应答处理器。连接应答处理器会创建一个能与客户端通信的 socket01，并将该 socket01 的 AE_READABLE 事件与命令请求处理器关联。 假设此时客户端发送了一个 set key value 请求，此时 redis 中的 socket01 会产生 AE_READABLE 事件，IO 多路复用程序将 socket01 压入队列，此时事件分派器从队列中获取到 socket01 产生的 AE_READABLE 事件，由于前面 socket01 的 AE_READABLE 事件已经与命令请求处理器关联，因此事件分派器将事件交给命令请求处理器来处理。命令请求处理器读取 socket01 的 key value 并在自己内存中完成 key value 的设置。操作完成后，它会将 socket01 的 AE_WRITABLE 事件与命令回复处理器关联。 如果此时客户端准备好接收返回结果了，那么 redis 中的 socket01 会产生一个 AE_WRITABLE 事件，同样压入队列中，事件分派器找到相关联的命令回复处理器，由命令回复处理器对 socket01 输入本次操作的一个结果，比如 ok，之后解除 socket01 的 AE_WRITABLE 事件与命令回复处理器的关联。 这样便完成了一次通信。关于 Redis 的一次通信过程，推荐读者阅读《Redis 设计与实现——黄健宏》进行系统学习。 为啥 redis 单线程模型也能效率这么高？ 纯内存操作。 核心是基于非阻塞的 IO 多路复用机制。 C 语言实现，一般来说，C 语言实现的程序“距离”操作系统更近，执行速度相对会更快。 单线程反而避免了多线程的频繁上下文切换问题，预防了多线程可能产生的竞争问题。]]></content>
      <categories>
        <category>javaEE</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Echarts]]></title>
    <url>%2Fblog4%2F2019%2F10%2F19%2FEcharts%2F</url>
    <content type="text"></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>可视化</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前端商城项目实战]]></title>
    <url>%2Fblog4%2F2019%2F10%2F19%2Fqianduanshanghceng%2F</url>
    <content type="text"></content>
      <tags>
        <tag>项目实战</tag>
        <tag>前端</tag>
        <tag>vue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vscode之旅]]></title>
    <url>%2Fblog4%2F2019%2F10%2F18%2Fvscode%2F</url>
    <content type="text"><![CDATA[这几天学习vue 感觉vscode 的命令行挺爽的 配置一把 1安装vscode我安装的是zip版本的 2、输入Configure Display Language或者configure language，选择“Configure Display Language” 重启 ok 显示中文 2 搭建前端环境2.1 极速html前端神奇 Mithril Emmet vscode下快速编写html ！直接生成html页面 2.2 vue环境搭建搭建vue的生产环境 1 更改配置和缓存目录 12npm config set prefix &quot;D:\java\vue\global&quot;npm config set cache &quot;D:\java\vue\Cache&quot; 2 安装cnpm 大部分的npm都在国外的服务器 1npm install -g cnpm --registry=https://registry.npm.taobao.org 安装vue-cli 1npm install -g vue-cli 将环境变量添加到系统变量 cmd 检查 vue -V 配置成功 vue-cli是搭建vue项目的脚手架 能极大的程度帮助我们创建一个全新的vue项目 3 vue的简单介绍vue init webpack xxx 查看vue的帮助 vue init xxx vscode在选择shell的时候可以有多重选择 4vscode集成开发vue项目vetur 拓展的安装 注意选择init create a new project with an official template $ vue init webpack my-project # create a new project straight from a github template $ vue init username/repo my-projectvue list 查看vue的命令 vue init webpack + name 一路yes即可 cnpm install 安装依赖 测试 cd my_first_vue npm run dev 成功 ok 开始vue的学习]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch]]></title>
    <url>%2Fblog4%2F2019%2F10%2F18%2FElasticSearch%2F</url>
    <content type="text"><![CDATA[12 ElasticSearch1 ElasticSearch的介绍用户访问我们的首页，一般都会直接搜索来寻找自己想要购买的商品。 而商品的数量非常多，而且分类繁杂。如何能正确的显示出用户想要的商品，并进行合理的过滤，尽快促成交易，是搜索系统要研究的核心。 面对这样复杂的搜索业务和数据量，使用传统数据库搜索就显得力不从心，一般我们都会使用全文检索技术，比如之前大家学习过的Solr。 1.1.简介Elastic有一条完整的产品线及解决方案：Elasticsearch、Kibana、Logstash等，前面说的三个就是大家常说的ELK技术栈。 1.1.1.ElasticElastic官网：https://www.elastic.co/cn/ 1.1.2.ElasticsearchElasticsearch官网：https://www.elastic.co/cn/products/elasticsearch 如上所述，Elasticsearch具备以下特点： 分布式，无需人工搭建集群（solr就需要人为配置，使用Zookeeper作为注册中心） Restful风格，一切API都遵循Rest原则，容易上手 近实时搜索，数据更新在Elasticsearch中几乎是完全同步的 1.1.3.版本使用的ElasticSearch的版本是6.3.0 需要虚拟机JDK1.8及以上 1.2 安装配置出于安全考虑，elasticsearch默认不允许以root账号运行。 创建用户 1useradd mage 设置密码 1passwd mage 切换用户 1su mage 1.2.2.上传安装包,并解压我们将安装包上传到：/home/mage目录 解压缩： 1tar -zxvf elasticsearch-6.2.4.tar.gz 我们把目录重命名： 1mv elasticsearch-6.3.0/ elasticsearch 进入，查看目录结构： Elasticsearch基于Lucene的，而Lucene底层是java实现，因此我们需要配置jvm参数。 编辑jvm.options： 1vim jvm.options 默认配置如下： 12-Xms1g-Xmx1g 内存占用太多了，我们调小一些： 12-Xms512m-Xmx512m elasticsearch.yml 1vim elasticsearch.yml 修改数据和日志目录： 12path.data: /home/mage/elasticsearch/data # 数据目录位置path.logs: /home/mage/elasticsearch/logs # 日志目录位置 我们把data和logs目录修改指向了elasticsearch的安装目录。但是这两个目录并不存在，因此我们需要创建出来。 进入elasticsearch的根目录，然后创建： 12mkdir datamkdir logs 修改绑定的ip： 1network.host: 0.0.0.0 # 绑定到0.0.0.0，允许任何ip来访问 默认只允许本机访问，修改为0.0.0.0后则可以远程访问 目前我们是做的单机安装，如果要做集群，只需要在这个配置文件中添加其它节点信息即可。 elasticsearch.yml的其它可配置信息： 属性名 说明 cluster.name 配置elasticsearch的集群名称，默认是elasticsearch。建议修改成一个有意义的名称。 node.name 节点名，es会默认随机指定一个名字，建议指定一个有意义的名称，方便管理 path.conf 设置配置文件的存储路径，tar或zip包安装默认在es根目录下的config文件夹，rpm安装默认在/etc/ elasticsearch path.data 设置索引数据的存储路径，默认是es根目录下的data文件夹，可以设置多个存储路径，用逗号隔开 path.logs 设置日志文件的存储路径，默认是es根目录下的logs文件夹 path.plugins 设置插件的存放路径，默认是es根目录下的plugins文件夹 bootstrap.memory_lock 设置为true可以锁住ES使用的内存，避免内存进行swap network.host 设置bind_host和publish_host，设置为0.0.0.0允许外网访问 http.port 设置对外服务的http端口，默认为9200。 transport.tcp.port 集群结点之间通信端口 discovery.zen.ping.timeout 设置ES自动发现节点连接超时的时间，默认为3秒，如果网络延迟高可设置大些 discovery.zen.minimum_master_nodes 主结点数量的最少值 ,此值的公式为：(master_eligible_nodes / 2) + 1 ，比如：有3个符合要求的主结点，那么这里要设置为2 1.3.运行进入elasticsearch/bin目录，可以看到下面的执行文件： 然后输入命令： 1./elasticsearch 发现报错了，启动失败。 1.3.1.错误1：内核过低 1.3.2.错误2：文件权限不足再次启动，又出错了： 1[1]: max file descriptors [4096] for elasticsearch process likely too low, increase to at least [65536] 我们用的是mage用户，而不是root，所以文件权限不足。 首先用root用户登录。 然后修改配置文件: 1vim /etc/security/limits.conf 添加下面的内容： 1234567* soft nofile 65536* hard nofile 131072* soft nproc 4096* hard nproc 4096 1.3.3.错误3：线程数不够刚才报错中，还有一行： 1[1]: max number of threads [1024] for user [leyou] is too low, increase to at least [4096] 这是线程数不够。 继续修改配置： 1vim /etc/security/limits.d/90-nproc.conf 修改下面的内容： 1* soft nproc 1024 改为： 1* soft nproc 4096 1.3.4.错误4：进程虚拟内存1[3]: max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144] vm.max_map_count：限制一个进程可以拥有的VMA(虚拟内存区域)的数量，继续修改配置文件， ： 1vim /etc/sysctl.conf 添加下面内容： 1vm.max_map_count=655360 然后执行命令： 1sysctl -p 1.3.5.重启终端窗口所有错误修改完毕，一定要重启你的 Xshell终端，否则配置无效。 1.3.6.启动再次启动，终于成功了！ 可以看到绑定了两个端口: 9300：集群节点间通讯接口 9200：客户端访问接口 我们在浏览器中访问：http://192.168.2.101:9200 1.4.安装kibana1.4.1.什么是Kibana？ Kibana是一个基于Node.js的Elasticsearch索引库数据统计工具，可以利用Elasticsearch的聚合功能，生成各种图表，如柱形图，线状图，饼图等。 而且还提供了操作Elasticsearch索引数据的控制台，并且提供了一定的API提示，非常有利于我们学习Elasticsearch的语法。 1.4.2.安装因为Kibana依赖于node，我们的虚拟机没有安装node，所以需要在centos下安装node 最新版本与elasticsearch保持一致，也是6.3.0 1 首先安装node 查看是否安装 node -v下载地址 https://nodejs.org/dist/v0.10.20/ 首先安装kibina一致 给出kibina的下载地址 https://www.elastic.co/cn/downloads/past-releases 2 解压nodetar xvf xxx mv node_xxx node 配置环境 su root vi /etc/profile source /etc/profile node.js 安装完成 3 安装kibana1 解压 2 cd kibana 3 进入kibana主目录的config目录下 vim kibana.yml 1elasticsearch.url: "http://192.168.2.101:9200" 4 启动 bin/kibana 访问192.168.2.101 发现没有反应 此时我发现不能访问 因为localhost:5601 而我们向远程访问 设置 server.host: “0.0.0.0” 此时 我们注意到为0.0.0.0:5601 访问 192.168.2.101:5601 此时 kibana和elasticsearch安装完毕 1.5 安装ik分词器1.5.1 安装1.传zip包并解压到 elasticsearch的插件包里面 注意删掉那个zip文件否则的话会报错 2 重启elasticsearch 1.5.2 测试在kibana控制台输入下面的请求： 12345POST _analyze&#123; "analyzer": "ik_max_word", "text": "我是中国人"&#125; 得到结果 1.7 APIElasticsearch提供了Rest风格的API，即http请求接口，而且也提供了各种语言的客户端API 1.7.1.Rest风格API文档地址：https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html Low Level Rest Client是低级别封装，提供一些基础功能，但更灵活 High Level Rest Client，是在Low Level Rest Client基础上进行的高级别封装，功能更丰富和完善，而且API会变的简单 先学习Rest风格API，了解发起请求的底层实现，请求体格式等。 2.操作索引2.1.基本概念Elasticsearch也是基于Lucene的全文检索库，本质也是存储数据，很多概念与MySQL类似的。 对比关系： 1234567索引（indices）--------------------------------Databases 数据库 类型（type）-----------------------------Table 数据表 文档（Document）----------------Row 行 字段（Field）-------------------Columns 列 详细说明： 概念 说明 索引库（indices) indices是index的复数，代表许多的索引， 类型（type） 类型是模拟mysql中的table概念，一个索引库下可以有不同类型的索引，比如商品索引，订单索引，其数据格式不同。不过这会导致索引库混乱，因此未来版本中会移除这个概念 文档（document） 存入索引库原始的数据。比如每一条商品信息，就是一个文档 字段（field） 文档中的属性 映射配置（mappings） 字段的数据类型、属性、是否索引、是否存储等特性 是不是与Lucene和solr中的概念类似。 另外，在SolrCloud中，有一些集群相关的概念，在Elasticsearch也有类似的： 索引集（Indices，index的复数）：逻辑上的完整索引 collection1 分片（shard）：数据拆分后的各个部分 副本（replica）：每个分片的复制 要注意的是：Elasticsearch本身就是分布式的，因此即便你只有一个节点，Elasticsearch默认也会对你的数据进行分片和副本操作，当你向集群添加新数据时，数据也会在新加入的节点中进行平衡。 2.2.创建索引2.2.1.语法Elasticsearch采用Rest风格API，因此其API就是一次http请求，你可以用任何工具发起http请求 创建索引的请求格式： 请求方式：PUT 请求路径：/索引库名 请求参数：json格式： 123456&#123; "settings": &#123; "number_of_shards": 3, "number_of_replicas": 2 &#125;&#125; settings：索引库的设置 number_of_shards：分片数量 number_of_replicas：副本数量 2.2.2.测试 2.3.查看索引设置 语法 Get请求可以帮我们查看索引信息，格式： 1GET /索引库名 或者，我们可以使用*来查询所有索引库配置： 2.4.删除索引删除索引使用DELETE请求 语法 1DELETE /索引库名 示例 当然，我们也可以用HEAD请求，查看索引是否存在： 2.5.映射配置索引有了，接下来肯定是添加数据。但是，在添加数据之前必须定义映射。 什么是映射？ 映射是定义文档的过程，文档包含哪些字段，这些字段是否保存，是否索引，是否分词等 只有配置清楚，Elasticsearch才会帮我们进行索引库的创建（不一定） 2.5.1.创建映射字段 语法 请求方式依然是PUT 1234567891011PUT /索引库名/_mapping/类型名称&#123; &quot;properties&quot;: &#123; &quot;字段名&quot;: &#123; &quot;type&quot;: &quot;类型&quot;, &quot;index&quot;: true， &quot;store&quot;: true， &quot;analyzer&quot;: &quot;分词器&quot; &#125; &#125;&#125; 类型名称：就是前面将的type的概念，类似于数据库中的不同表字段名：任意填写 ，可以指定许多属性，例如： type：类型，可以是text、long、short、date、integer、object等 index：是否索引，默认为true store：是否存储，默认为false analyzer：分词器，这里的ik_max_word即使用ik分词器 示例 发起请求： 12345678910111213141516PUT nage/_mapping/goods&#123; "properties": &#123; "title": &#123; "type": "text", "analyzer": "ik_max_word" &#125;, "images": &#123; "type": "keyword", "index": "false" &#125;, "price": &#123; "type": "float" &#125; &#125;&#125; 响应结果： 123&#123; &quot;acknowledged&quot;: true&#125; 2.5.2.查看映射关系 语法： 1GET /索引库名/_mapping 示例： 1GET /mage/_mapping 响应： 123456789101112131415161718192021&#123; "mage": &#123; "mappings": &#123; "goods": &#123; "properties": &#123; "images": &#123; "type": "keyword", "index": false &#125;, "price": &#123; "type": "float" &#125;, "title": &#123; "type": "text", "analyzer": "ik_max_word" &#125; &#125; &#125; &#125; &#125;&#125; 2.5.3.字段属性详解2.5.3.1.typeElasticsearch中支持的数据类型非常丰富： 我们说几个关键的： String类型，又分两种： text：可分词，不可参与聚合 keyword：不可分词，数据会作为完整字段进行匹配，可以参与聚合 Numerical：数值类型，分两类 基本数据类型：long、interger、short、byte、double、float、half_float 浮点数的高精度类型：scaled_float 需要指定一个精度因子，比如10或100。elasticsearch会把真实值乘以这个因子后存储，取出时再还原。 Date：日期类型 elasticsearch可以对日期格式化为字符串存储，但是建议我们存储为毫秒值，存储为long，节省空间。 2.5.3.2.indexindex影响字段的索引情况。 true：字段会被索引，则可以用来进行搜索。默认值就是true false：字段不会被索引，不能用来搜索 index的默认值就是true，也就是说你不进行任何配置，所有字段都会被索引。 但是有些字段是我们不希望被索引的，比如商品的图片信息，就需要手动设置index为false。 2.5.3.3.store是否将数据进行额外存储。 在学习lucene和solr时，我们知道如果一个字段的store设置为false，那么在文档列表中就不会有这个字段的值，用户的搜索结果中不会显示出来。 但是在Elasticsearch中，即便store设置为false，也可以搜索到结果。 原因是Elasticsearch在创建文档索引时，会将文档中的原始数据备份，保存到一个叫做_source的属性中。而且我们可以通过过滤_source来选择哪些要显示，哪些不显示。 而如果设置store为true，就会在_source以外额外存储一份数据，多余，因此一般我们都会将store设置为false，事实上，store的默认值就是false。 2.5.3.4.boost激励因子，这个与lucene中一样 其它的不再一一讲解，用的不多，大家参考官方文档： 2.6.新增数据2.6.1.随机生成id通过POST请求，可以向一个已经存在的索引库中添加数据。 语法： 1234POST /索引库名/类型名&#123; &quot;key&quot;:&quot;value&quot;&#125; 示例： 123456POST /mage/goods/&#123; "title":"小米手机", "images":"http://image.mage.com/12479122.jpg", "price":2699.00&#125; 响应： 1234567891011121314&#123; "_index": "heima", "_type": "goods", "_id": "r9c1KGMBIhaxtY5rlRKv", "_version": 1, "result": "created", "_shards": &#123; "total": 3, "successful": 1, "failed": 0 &#125;, "_seq_no": 0, "_primary_term": 2&#125; 通过kibana查看数据： 123456get _search&#123; "query":&#123; "match_all":&#123;&#125; &#125;&#125; 123456789101112&#123; "_index": "mage", "_type": "goods", "_id": "r9c1KGMBIhaxtY5rlRKv", "_version": 1, "_score": 1, "_source": &#123; "title": "小米手机", "images": "http://image.mage.com/12479122.jpg", "price": 2699 &#125;&#125; _source：源文档信息，所有的数据都在里面。 _id：这条文档的唯一标示，与文档自己的id字段没有关联 2.6.2.自定义id如果我们想要自己新增的时候指定id，可以这么做： 1234POST /索引库名/类型/id值&#123; ...&#125; 示例： 123456POST /mage/goods/2&#123; "title":"大米手机", "images":"http://image.mage.com/12479122.jpg", "price":2899.00&#125; 得到的数据： 1234567891011&#123; "_index": "mage", "_type": "goods", "_id": "2", "_score": 1, "_source": &#123; "title": "大米手机", "images": "http://image.mage.com/12479122.jpg", "price": 2899 &#125;&#125; 2.6.3.智能判断在学习Solr时我们发现，我们在新增数据时，只能使用提前配置好映射属性的字段，否则就会报错。 不过在Elasticsearch中并没有这样的规定。 事实上Elasticsearch非常智能，你不需要给索引库设置任何mapping映射，它也可以根据你输入的数据来判断类型，动态添加数据映射。 测试一下： 12345678POST /mage/goods/3&#123; "title":"超米手机", "images":"http://image.mage.com/12479122.jpg", "price":2899.00, "stock": 200, "saleable":true&#125; 我们额外添加了stock库存，和saleable是否上架两个字段。 来看结果： 1234567891011121314&#123; "_index": "heima", "_type": "goods", "_id": "3", "_version": 1, "_score": 1, "_source": &#123; "title": "超米手机", "images": "http://image.mage.com/12479122.jpg", "price": 2899, "stock": 200, "saleable": true &#125;&#125; 在看下索引库的映射关系: 123456789101112131415161718192021222324252627&#123; "mage": &#123; "mappings": &#123; "goods": &#123; "properties": &#123; "images": &#123; "type": "keyword", "index": false &#125;, "price": &#123; "type": "float" &#125;, "saleable": &#123; "type": "boolean" &#125;, "stock": &#123; "type": "long" &#125;, "title": &#123; "type": "text", "analyzer": "ik_max_word" &#125; &#125; &#125; &#125; &#125;&#125; stock和saleable都被成功映射了。 2.7.修改数据把刚才新增的请求方式改为PUT，就是修改了。不过修改必须指定id， id对应文档存在，则修改 id对应文档不存在，则新增 比如，我们把id为3的数据进行修改： 12345678PUT /mage/goods/3&#123; "title":"超大米手机", "images":"http://image.mage.com/12479122.jpg", "price":3899.00, "stock": 100, "saleable":true&#125; 结果： 1234567891011121314151617181920212223242526272829&#123; "took": 17, "timed_out": false, "_shards": &#123; "total": 9, "successful": 9, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": 1, "max_score": 1, "hits": [ &#123; "_index": "heima", "_type": "goods", "_id": "3", "_score": 1, "_source": &#123; "title": "超大米手机", "images": "http://image.mage.com/12479122.jpg", "price": 3899, "stock": 100, "saleable": true &#125; &#125; ] &#125;&#125; 2.8.删除数据删除使用DELETE请求，同样，需要根据id进行删除： 语法 1DELETE /索引库名/类型名/id值 示例： 3.查询我们从4块来讲查询： 基本查询 _source过滤 结果过滤 高级查询 排序 3.1.基本查询 基本语法 12345678GET /索引库名/_search&#123; "query":&#123; "查询类型":&#123; "查询条件":"查询条件值" &#125; &#125;&#125; 这里的query代表一个查询对象，里面可以有不同的查询属性 查询类型： 例如：match_all， match，term ， range 等等 查询条件：查询条件会根据类型的不同，写法也有差异，后面详细讲解 3.1.1 查询所有（match_all) 示例： 123456GET /mage/_search&#123; "query":&#123; "match_all": &#123;&#125; &#125;&#125; query：代表查询对象 match_all：代表查询所有 结果： 1234567891011121314151617181920212223242526272829303132333435363738&#123; "took": 2, "timed_out": false, "_shards": &#123; "total": 3, "successful": 3, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": 2, "max_score": 1, "hits": [ &#123; "_index": "heima", "_type": "goods", "_id": "2", "_score": 1, "_source": &#123; "title": "大米手机", "images": "http://image.mage.com/12479122.jpg", "price": 2899 &#125; &#125;, &#123; "_index": "heima", "_type": "goods", "_id": "r9c1KGMBIhaxtY5rlRKv", "_score": 1, "_source": &#123; "title": "小米手机", "images": "http://image.mage.com/12479122.jpg", "price": 2699 &#125; &#125; ] &#125;&#125; took：查询花费时间，单位是毫秒 time_out：是否超时 _shards：分片信息 hits：搜索结果总览对象 total：搜索到的总条数 max_score：所有结果中文档得分的最高分 hits：搜索结果的文档对象数组，每个元素是一条搜索到的文档信息 _index：索引库 _type：文档类型 _id：文档id _score：文档得分 _source：文档的源数据 3.1.2 匹配查询（match）我们先加入一条数据，便于测试： 123456PUT /mage/goods/3&#123; "title":"小米电视4A", "images":"http://image.mage.com/12479122.jpg", "price":3899.00&#125; 现在，索引库中有2部手机，1台电视： or关系 match类型查询，会把查询条件进行分词，然后进行查询,多个词条之间是or的关系 12345678GET /mage/_search&#123; "query":&#123; "match":&#123; "title":"小米电视" &#125; &#125;&#125; 结果： 12345678910111213141516171819202122232425262728"hits": &#123; "total": 2, "max_score": 0.6931472, "hits": [ &#123; "_index": "heima", "_type": "goods", "_id": "tmUBomQB_mwm6wH_EC1-", "_score": 0.6931472, "_source": &#123; "title": "小米手机", "images": "http://image.mage.com/12479122.jpg", "price": 2699 &#125; &#125;, &#123; "_index": "heima", "_type": "goods", "_id": "3", "_score": 0.5753642, "_source": &#123; "title": "小米电视4A", "images": "http://image.mage.com/12479122.jpg", "price": 3899 &#125; &#125; ]&#125; 在上面的案例中，不仅会查询到电视，而且与小米相关的都会查询到，多个词之间是or的关系。 and关系 某些情况下，我们需要更精确查找，我们希望这个关系变成and，可以这样做： 1234567891011GET /mage/_search&#123; "query":&#123; "match": &#123; "title": &#123; "query": "小米电视", "operator": "and" &#125; &#125; &#125;&#125; 结果： 123456789101112131415161718192021222324252627&#123; "took": 2, "timed_out": false, "_shards": &#123; "total": 3, "successful": 3, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": 1, "max_score": 0.5753642, "hits": [ &#123; "_index": "heima", "_type": "goods", "_id": "3", "_score": 0.5753642, "_source": &#123; "title": "小米电视4A", "images": "http://image.mage.com/12479122.jpg", "price": 3899 &#125; &#125; ] &#125;&#125; 本例中，只有同时包含小米和电视的词条才会被搜索到。 or和and之间？ 在 or 与 and 间二选一有点过于非黑即白。 如果用户给定的条件分词后有 5 个查询词项，想查找只包含其中 4 个词的文档，该如何处理？将 operator 操作符参数设置成 and 只会将此文档排除。 有时候这正是我们期望的，但在全文搜索的大多数应用场景下，我们既想包含那些可能相关的文档，同时又排除那些不太相关的。换句话说，我们想要处于中间某种结果。 match 查询支持 minimum_should_match 最小匹配参数， 这让我们可以指定必须匹配的词项数用来表示一个文档是否相关。我们可以将其设置为某个具体数字，更常用的做法是将其设置为一个百分数，因为我们无法控制用户搜索时输入的单词数量： 1234567891011GET /mage/_search&#123; "query":&#123; "match":&#123; "title":&#123; "query":"小米曲面电视", "minimum_should_match": "75%" &#125; &#125; &#125;&#125; 本例中，搜索语句可以分为3个词，如果使用and关系，需要同时满足3个词才会被搜索到。这里我们采用最小品牌数：75%，那么也就是说只要匹配到总词条数量的75%即可，这里3*75% 约等于2。所以只要包含2个词条就算满足条件了。 结果： 3.1.3 多字段查询（multi_match）multi_match与match类似，不同的是它可以在多个字段中查询 123456789GET /mage/_search&#123; "query":&#123; "multi_match": &#123; "query": "小米", "fields": [ "title", "subTitle" ] &#125; &#125;&#125; 本例中，我们会在title字段和subtitle字段中查询小米这个词 3.1.4 词条匹配(term)term 查询被用于精确值 匹配，这些精确值可能是数字、时间、布尔或者那些未分词的字符串 12345678GET /heima/_search&#123; "query":&#123; "term":&#123; "price":2699.00 &#125; &#125;&#125; 结果： 123456789101112131415161718192021222324252627&#123; "took": 2, "timed_out": false, "_shards": &#123; "total": 3, "successful": 3, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": 1, "max_score": 1, "hits": [ &#123; "_index": "heima", "_type": "goods", "_id": "r9c1KGMBIhaxtY5rlRKv", "_score": 1, "_source": &#123; "title": "小米手机", "images": "http://image.mage.com/12479122.jpg", "price": 2699 &#125; &#125; ] &#125;&#125; 3.1.5 多词条精确匹配(terms)terms 查询和 term 查询一样，但它允许你指定多值进行匹配。如果这个字段包含了指定值中的任何一个值，那么这个文档满足条件： 12345678GET /heima/_search&#123; "query":&#123; "terms":&#123; "price":[2699.00,2899.00,3899.00] &#125; &#125;&#125; 结果： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&#123; "took": 4, "timed_out": false, "_shards": &#123; "total": 3, "successful": 3, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": 3, "max_score": 1, "hits": [ &#123; "_index": "heima", "_type": "goods", "_id": "2", "_score": 1, "_source": &#123; "title": "大米手机", "images": "http://image.mage.com/12479122.jpg", "price": 2899 &#125; &#125;, &#123; "_index": "heima", "_type": "goods", "_id": "r9c1KGMBIhaxtY5rlRKv", "_score": 1, "_source": &#123; "title": "小米手机", "images": "http://image.mage.com/12479122.jpg", "price": 2699 &#125; &#125;, &#123; "_index": "heima", "_type": "goods", "_id": "3", "_score": 1, "_source": &#123; "title": "小米电视4A", "images": "http://image.mage.com/12479122.jpg", "price": 3899 &#125; &#125; ] &#125;&#125; 3.2.结果过滤默认情况下，elasticsearch在搜索的结果中，会把文档中保存在_source的所有字段都返回。 如果我们只想获取其中的部分字段，我们可以添加_source的过滤 3.2.1.直接指定字段示例： 123456789GET /heima/_search&#123; "_source": ["title","price"], "query": &#123; "term": &#123; "price": 2699 &#125; &#125;&#125; 返回的结果： 1234567891011121314151617181920212223242526&#123; "took": 12, "timed_out": false, "_shards": &#123; "total": 3, "successful": 3, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": 1, "max_score": 1, "hits": [ &#123; "_index": "heima", "_type": "goods", "_id": "r9c1KGMBIhaxtY5rlRKv", "_score": 1, "_source": &#123; "price": 2699, "title": "小米手机" &#125; &#125; ] &#125;&#125; 3.2.2.指定includes和excludes我们也可以通过： includes：来指定想要显示的字段 excludes：来指定不想要显示的字段 二者都是可选的。 示例： 1234567891011GET /heima/_search&#123; "_source": &#123; "includes":["title","price"] &#125;, "query": &#123; "term": &#123; "price": 2699 &#125; &#125;&#125; 与下面的结果将是一样的： 1234567891011GET /heima/_search&#123; "_source": &#123; "excludes": ["images"] &#125;, "query": &#123; "term": &#123; "price": 2699 &#125; &#125;&#125; 3.3 高级查询3.3.1 布尔组合（bool)bool把各种其它查询通过must（与）、must_not（非）、should（或）的方式进行组合 12345678910GET /heima/_search&#123; "query":&#123; "bool":&#123; "must": &#123; "match": &#123; "title": "大米" &#125;&#125;, "must_not": &#123; "match": &#123; "title": "电视" &#125;&#125;, "should": &#123; "match": &#123; "title": "手机" &#125;&#125; &#125; &#125;&#125; 结果： 123456789101112131415161718192021222324252627&#123; "took": 10, "timed_out": false, "_shards": &#123; "total": 3, "successful": 3, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": 1, "max_score": 0.5753642, "hits": [ &#123; "_index": "heima", "_type": "goods", "_id": "2", "_score": 0.5753642, "_source": &#123; "title": "大米手机", "images": "http://image.mage.com/12479122.jpg", "price": 2899 &#125; &#125; ] &#125;&#125; 3.3.2 范围查询(range)range 查询找出那些落在指定区间内的数字或者时间 1234567891011GET /heima/_search&#123; "query":&#123; "range": &#123; "price": &#123; "gte": 1000.0, "lt": 2800.00 &#125; &#125; &#125;&#125; range查询允许以下字符： 操作符 说明 gt 大于 gte 大于等于 lt 小于 lte 小于等于 3.3.3 模糊查询(fuzzy)我们新增一个商品： 123456POST /heima/goods/4&#123; "title":"apple手机", "images":"http://image.mage.com/12479122.jpg", "price":6899.00&#125; fuzzy 查询是 term 查询的模糊等价。它允许用户搜索词条与实际词条的拼写出现偏差，但是偏差的编辑距离不得超过2： 12345678GET /heima/_search&#123; "query": &#123; "fuzzy": &#123; "title": "appla" &#125; &#125;&#125; 上面的查询，也能查询到apple手机 我们可以通过fuzziness来指定允许的编辑距离： 1234567891011GET /heima/_search&#123; "query": &#123; "fuzzy": &#123; "title": &#123; "value":"appla", "fuzziness":1 &#125; &#125; &#125;&#125; 3.4 过滤(filter) 条件查询中进行过滤 所有的查询都会影响到文档的评分及排名。如果我们需要在查询结果中进行过滤，并且不希望过滤条件影响评分，那么就不要把过滤条件作为查询条件来用。而是使用filter方式： 1234567891011GET /heima/_search&#123; "query":&#123; "bool":&#123; "must":&#123; "match": &#123; "title": "小米手机" &#125;&#125;, "filter":&#123; "range":&#123;"price":&#123;"gt":2000.00,"lt":3800.00&#125;&#125; &#125; &#125; &#125;&#125; 注意：filter中还可以再次进行bool组合条件过滤。 无查询条件，直接过滤 如果一次查询只有过滤，没有查询条件，不希望进行评分，我们可以使用constant_score取代只有 filter 语句的 bool 查询。在性能上是完全相同的，但对于提高查询简洁性和清晰度有很大帮助。 123456789GET /heima/_search&#123; "query":&#123; "constant_score": &#123; "filter": &#123; "range":&#123;"price":&#123;"gt":2000.00,"lt":3000.00&#125;&#125; &#125; &#125;&#125; 3.5 排序3.4.1 单字段排序sort 可以让我们按照不同的字段进行排序，并且通过order指定排序的方式 123456789101112131415GET /heima/_search&#123; "query": &#123; "match": &#123; "title": "小米手机" &#125; &#125;, "sort": [ &#123; "price": &#123; "order": "desc" &#125; &#125; ]&#125; 3.4.2 多字段排序假定我们想要结合使用 price和 _score（得分） 进行查询，并且匹配的结果首先按照价格排序，然后按照相关性得分排序： 123456789101112131415GET /goods/_search&#123; "query":&#123; "bool":&#123; "must":&#123; "match": &#123; "title": "小米手机" &#125;&#125;, "filter":&#123; "range":&#123;"price":&#123;"gt":200000,"lt":300000&#125;&#125; &#125; &#125; &#125;, "sort": [ &#123; "price": &#123; "order": "desc" &#125;&#125;, &#123; "_score": &#123; "order": "desc" &#125;&#125; ]&#125; 4. 聚合aggregations聚合可以让我们极其方便的实现对数据的统计、分析。例如： 什么品牌的手机最受欢迎？ 这些手机的平均价格、最高价格、最低价格？ 这些手机每月的销售情况如何？ 实现这些统计功能的比数据库的sql要方便的多，而且查询速度非常快，可以实现实时搜索效果。 4.1 基本概念Elasticsearch中的聚合，包含多种类型，最常用的两种，一个叫桶，一个叫度量： 桶（bucket） 桶的作用，是按照某种方式对数据进行分组，每一组数据在ES中称为一个桶，例如我们根据国籍对人划分，可以得到中国桶、英国桶，日本桶……或者我们按照年龄段对人进行划分：010,1020,2030,3040等。 Elasticsearch中提供的划分桶的方式有很多： Date Histogram Aggregation：根据日期阶梯分组，例如给定阶梯为周，会自动每周分为一组 Histogram Aggregation：根据数值阶梯分组，与日期类似 Terms Aggregation：根据词条内容分组，词条内容完全匹配的为一组 Range Aggregation：数值和日期的范围分组，指定开始和结束，然后按段分组 …… bucket aggregations 只负责对数据进行分组，并不进行计算，因此往往bucket中往往会嵌套另一种聚合：metrics aggregations即度量 度量（metrics） 分组完成以后，我们一般会对组中的数据进行聚合运算，例如求平均值、最大、最小、求和等，这些在ES中称为度量 比较常用的一些度量聚合方式： Avg Aggregation：求平均值 Max Aggregation：求最大值 Min Aggregation：求最小值 Percentiles Aggregation：求百分比 Stats Aggregation：同时返回avg、max、min、sum、count等 Sum Aggregation：求和 Top hits Aggregation：求前几 Value Count Aggregation：求总数 …… 为了测试聚合，我们先批量导入一些数据 创建索引： 12345678910111213141516171819PUT /cars&#123; "settings": &#123; "number_of_shards": 1, "number_of_replicas": 0 &#125;, "mappings": &#123; "transactions": &#123; "properties": &#123; "color": &#123; "type": "keyword" &#125;, "make": &#123; "type": "keyword" &#125; &#125; &#125; &#125;&#125; 注意：在ES中，需要进行聚合、排序、过滤的字段其处理方式比较特殊，因此不能被分词。这里我们将color和make这两个文字类型的字段设置为keyword类型，这个类型不会被分词，将来就可以参与聚合 导入数据 1234567891011121314151617POST /cars/transactions/_bulk&#123; "index": &#123;&#125;&#125;&#123; "price" : 10000, "color" : "red", "make" : "honda", "sold" : "2014-10-28" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 20000, "color" : "red", "make" : "honda", "sold" : "2014-11-05" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 30000, "color" : "green", "make" : "ford", "sold" : "2014-05-18" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 15000, "color" : "blue", "make" : "toyota", "sold" : "2014-07-02" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 12000, "color" : "green", "make" : "toyota", "sold" : "2014-08-19" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 20000, "color" : "red", "make" : "honda", "sold" : "2014-11-05" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 80000, "color" : "red", "make" : "bmw", "sold" : "2014-01-01" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 25000, "color" : "blue", "make" : "ford", "sold" : "2014-02-12" &#125; 4.2 聚合为桶首先，我们按照 汽车的颜色color来划分桶 1234567891011GET /cars/_search&#123; "size" : 0, "aggs" : &#123; "popular_colors" : &#123; "terms" : &#123; "field" : "color" &#125; &#125; &#125;&#125; size： 查询条数，这里设置为0，因为我们不关心搜索到的数据，只关心聚合结果，提高效率 aggs：声明这是一个聚合查询，是aggregations的缩写 popular_colors：给这次聚合起一个名字，任意。 terms：划分桶的方式，这里是根据词条划分 field：划分桶的字段 结果： 1234567891011121314151617181920212223242526272829303132333435&#123; "took": 1, "timed_out": false, "_shards": &#123; "total": 1, "successful": 1, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": 8, "max_score": 0, "hits": [] &#125;, "aggregations": &#123; "popular_colors": &#123; "doc_count_error_upper_bound": 0, "sum_other_doc_count": 0, "buckets": [ &#123; "key": "red", "doc_count": 4 &#125;, &#123; "key": "blue", "doc_count": 2 &#125;, &#123; "key": "green", "doc_count": 2 &#125; ] &#125; &#125;&#125; hits：查询结果为空，因为我们设置了size为0 aggregations：聚合的结果 popular_colors：我们定义的聚合名称 buckets：查找到的桶，每个不同的color字段值都会形成一个桶 key：这个桶对应的color字段的值 doc_count：这个桶中的文档数量 通过聚合的结果我们发现，目前红色的小车比较畅销！ 4.3 桶内度量前面的例子告诉我们每个桶里面的文档数量，这很有用。 但通常，我们的应用需要提供更复杂的文档度量。 例如，每种颜色汽车的平均价格是多少？ 因此，我们需要告诉Elasticsearch使用哪个字段，使用何种度量方式进行运算，这些信息要嵌套在桶内，度量的运算会基于桶内的文档进行 现在，我们为刚刚的聚合结果添加 求价格平均值的度量： 123456789101112131415161718GET /cars/_search&#123; "size" : 0, "aggs" : &#123; "popular_colors" : &#123; "terms" : &#123; "field" : "color" &#125;, "aggs":&#123; "avg_price": &#123; "avg": &#123; "field": "price" &#125; &#125; &#125; &#125; &#125;&#125; aggs：我们在上一个aggs(popular_colors)中添加新的aggs。可见度量也是一个聚合 avg_price：聚合的名称 avg：度量的类型，这里是求平均值 field：度量运算的字段 结果： 12345678910111213141516171819202122232425262728293031... "aggregations": &#123; "popular_colors": &#123; "doc_count_error_upper_bound": 0, "sum_other_doc_count": 0, "buckets": [ &#123; "key": "red", "doc_count": 4, "avg_price": &#123; "value": 32500 &#125; &#125;, &#123; "key": "blue", "doc_count": 2, "avg_price": &#123; "value": 20000 &#125; &#125;, &#123; "key": "green", "doc_count": 2, "avg_price": &#123; "value": 21000 &#125; &#125; ] &#125; &#125;... 可以看到每个桶中都有自己的avg_price字段，这是度量聚合的结果 4.4 桶内嵌套桶刚刚的案例中，我们在桶内嵌套度量运算。事实上桶不仅可以嵌套运算， 还可以再嵌套其它桶。也就是说在每个分组中，再分更多组。 比如：我们想统计每种颜色的汽车中，分别属于哪个制造商，按照make字段再进行分桶 1234567891011121314151617181920212223GET /cars/_search&#123; "size" : 0, "aggs" : &#123; "popular_colors" : &#123; "terms" : &#123; "field" : "color" &#125;, "aggs":&#123; "avg_price": &#123; "avg": &#123; "field": "price" &#125; &#125;, "maker":&#123; "terms":&#123; "field":"make" &#125; &#125; &#125; &#125; &#125;&#125; 原来的color桶和avg计算我们不变 maker：在嵌套的aggs下新添一个桶，叫做maker terms：桶的划分类型依然是词条 filed：这里根据make字段进行划分 部分结果： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374...&#123;"aggregations": &#123; "popular_colors": &#123; "doc_count_error_upper_bound": 0, "sum_other_doc_count": 0, "buckets": [ &#123; "key": "red", "doc_count": 4, "maker": &#123; "doc_count_error_upper_bound": 0, "sum_other_doc_count": 0, "buckets": [ &#123; "key": "honda", "doc_count": 3 &#125;, &#123; "key": "bmw", "doc_count": 1 &#125; ] &#125;, "avg_price": &#123; "value": 32500 &#125; &#125;, &#123; "key": "blue", "doc_count": 2, "maker": &#123; "doc_count_error_upper_bound": 0, "sum_other_doc_count": 0, "buckets": [ &#123; "key": "ford", "doc_count": 1 &#125;, &#123; "key": "toyota", "doc_count": 1 &#125; ] &#125;, "avg_price": &#123; "value": 20000 &#125; &#125;, &#123; "key": "green", "doc_count": 2, "maker": &#123; "doc_count_error_upper_bound": 0, "sum_other_doc_count": 0, "buckets": [ &#123; "key": "ford", "doc_count": 1 &#125;, &#123; "key": "toyota", "doc_count": 1 &#125; ] &#125;, "avg_price": &#123; "value": 21000 &#125; &#125; ] &#125; &#125;&#125;... 我们可以看到，新的聚合maker被嵌套在原来每一个color的桶中。 每个颜色下面都根据 make字段进行了分组 我们能读取到的信息： 红色车共有4辆 红色车的平均售价是 $32，500 美元。 其中3辆是 Honda 本田制造，1辆是 BMW 宝马制造。 4.5.划分桶的其它方式前面讲了，划分桶的方式有很多，例如： Date Histogram Aggregation：根据日期阶梯分组，例如给定阶梯为周，会自动每周分为一组 Histogram Aggregation：根据数值阶梯分组，与日期类似 Terms Aggregation：根据词条内容分组，词条内容完全匹配的为一组 Range Aggregation：数值和日期的范围分组，指定开始和结束，然后按段分组 刚刚的案例中，我们采用的是Terms Aggregation，即根据词条划分桶。 接下来，我们再学习几个比较实用的： 4.5.1.阶梯分桶Histogram 原理： histogram是把数值类型的字段，按照一定的阶梯大小进行分组。你需要指定一个阶梯值（interval）来划分阶梯大小。 举例： 比如你有价格字段，如果你设定interval的值为200，那么阶梯就会是这样的： 0，200，400，600，… 上面列出的是每个阶梯的key，也是区间的启点。 如果一件商品的价格是450，会落入哪个阶梯区间呢？计算公式如下： 1bucket_key = Math.floor((value - offset) / interval) * interval + offset value：就是当前数据的值，本例中是450 offset：起始偏移量，默认为0 interval：阶梯间隔，比如200 因此你得到的key = Math.floor((450 - 0) / 200) * 200 + 0 = 400 操作一下： 比如，我们对汽车的价格进行分组，指定间隔interval为5000： 123456789101112GET /cars/_search&#123; "size":0, "aggs":&#123; "price":&#123; "histogram": &#123; "field": "price", "interval": 5000 &#125; &#125; &#125;&#125; 结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081&#123; "took": 21, "timed_out": false, "_shards": &#123; "total": 5, "successful": 5, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": 8, "max_score": 0, "hits": [] &#125;, "aggregations": &#123; "price": &#123; "buckets": [ &#123; "key": 10000, "doc_count": 2 &#125;, &#123; "key": 15000, "doc_count": 1 &#125;, &#123; "key": 20000, "doc_count": 2 &#125;, &#123; "key": 25000, "doc_count": 1 &#125;, &#123; "key": 30000, "doc_count": 1 &#125;, &#123; "key": 35000, "doc_count": 0 &#125;, &#123; "key": 40000, "doc_count": 0 &#125;, &#123; "key": 45000, "doc_count": 0 &#125;, &#123; "key": 50000, "doc_count": 0 &#125;, &#123; "key": 55000, "doc_count": 0 &#125;, &#123; "key": 60000, "doc_count": 0 &#125;, &#123; "key": 65000, "doc_count": 0 &#125;, &#123; "key": 70000, "doc_count": 0 &#125;, &#123; "key": 75000, "doc_count": 0 &#125;, &#123; "key": 80000, "doc_count": 1 &#125; ] &#125; &#125;&#125; 你会发现，中间有大量的文档数量为0 的桶，看起来很丑。 我们可以增加一个参数min_doc_count为1，来约束最少文档数量为1，这样文档数量为0的桶会被过滤 示例： 12345678910111213GET /cars/_search&#123; "size":0, "aggs":&#123; "price":&#123; "histogram": &#123; "field": "price", "interval": 5000, "min_doc_count": 1 &#125; &#125; &#125;&#125; 结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445&#123; "took": 15, "timed_out": false, "_shards": &#123; "total": 5, "successful": 5, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": 8, "max_score": 0, "hits": [] &#125;, "aggregations": &#123; "price": &#123; "buckets": [ &#123; "key": 10000, "doc_count": 2 &#125;, &#123; "key": 15000, "doc_count": 1 &#125;, &#123; "key": 20000, "doc_count": 2 &#125;, &#123; "key": 25000, "doc_count": 1 &#125;, &#123; "key": 30000, "doc_count": 1 &#125;, &#123; "key": 80000, "doc_count": 1 &#125; ] &#125; &#125;&#125; 完美，！ 如果你用kibana将结果变为柱形图，会更好看： 4.5.2.范围分桶range范围分桶与阶梯分桶类似，也是把数字按照阶段进行分组，只不过range方式需要你自己指定每一组的起始和结束大小。 5.Spring Data ElasticsearchElasticsearch提供的Java客户端有一些不太方便的地方： 很多地方需要拼接Json字符串，在java中拼接字符串有多恐怖你应该懂的 需要自己把对象序列化为json存储 查询到结果也需要自己反序列化为对象 因此，我们这里就不讲解原生的Elasticsearch客户端API了。 而是学习Spring提供的套件：Spring Data Elasticsearch。 5.1.简介Spring Data Elasticsearch是Spring Data项目下的一个子模块。 查看 Spring Data的官网：http://projects.spring.io/spring-data/ Spring Data的使命是为数据访问提供熟悉且一致的基于Spring的编程模型，同时仍保留底层数据存储的特殊特性。 它使得使用数据访问技术，关系数据库和非关系数据库，map-reduce框架和基于云的数据服务变得容易。这是一个总括项目，其中包含许多特定于给定数据库的子项目。这些令人兴奋的技术项目背后，是由许多公司和开发人员合作开发的。 Spring Data的使命是为数据访问提供熟悉且一致的基于Spring的编程模型，同时仍保留底层数据存储的特殊特性。 它使得使用数据访问技术，关系数据库和非关系数据库，map-reduce框架和基于云的数据服务变得容易。这是一个总括项目，其中包含许多特定于给定数据库的子项目。这些令人兴奋的技术项目背后，是由许多公司和开发人员合作开发的。 Spring Data Elasticsearch的页面：https://projects.spring.io/spring-data-elasticsearch/ 特征： 支持Spring的基于@Configuration的java配置方式，或者XML配置方式 提供了用于操作ES的便捷工具类ElasticsearchTemplate。包括实现文档到POJO之间的自动智能映射。 利用Spring的数据转换服务实现的功能丰富的对象映射 基于注解的元数据映射方式，而且可扩展以支持更多不同的数据格式 根据持久层接口自动生成对应实现方法，无需人工编写基本操作代码（类似mybatis，根据接口自动得到实现）。当然，也支持人工定制查询 5.2 创建demo案例用spring脚手架创建 选择elasticsearch 依赖 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.6.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.mlz.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;mage-elasticsearch&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;mage-elasticsearch&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; applicaiton.yml 配置文12345spring: data: elasticsearch: cluster-name: my-es cluster-nodes: 192.168.2101:9300 这里注意集群的名称 我的elasticsearch集群的名称是my-es 在elasticsearch.yml文件中配置即可 5.3.实体类及注解首先我们准备好实体类： Item 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182package com.mlz.elasticsearch.pojo;/* * @创建人: MaLingZhao * @创建时间: 2019/4/19 * @描述： */import org.springframework.data.annotation.Id;import org.springframework.data.elasticsearch.annotations.Document;import org.springframework.data.elasticsearch.annotations.Field;import org.springframework.data.elasticsearch.annotations.FieldType;@Document(indexName = "item",type = "docs", shards = 1, replicas = 0)public class Item &#123; @Id private Long id; @Field(type = FieldType.Text, analyzer = "ik_max_word") private String title; //标题 @Field(type = FieldType.Keyword) private String category;// 分类 @Field(type = FieldType.Keyword) private String brand; // 品牌 @Field(type = FieldType.Double) private Double price; // 价格 @Field(index = false, type = FieldType.Keyword) private String images; // 图片地址 public Long getId() &#123; return id; &#125; public void setId(Long id) &#123; this.id = id; &#125; public String getTitle() &#123; return title; &#125; public void setTitle(String title) &#123; this.title = title; &#125; public String getCategory() &#123; return category; &#125; public void setCategory(String category) &#123; this.category = category; &#125; public String getBrand() &#123; return brand; &#125; public void setBrand(String brand) &#123; this.brand = brand; &#125; public Double getPrice() &#123; return price; &#125; public void setPrice(Double price) &#123; this.price = price; &#125; public String getImages() &#123; return images; &#125; public void setImages(String images) &#123; this.images = images; &#125;&#125; 5.4.Twmplate的索引操作5.4.1 创建索引和映射 创建索引 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.mlz.elasticsearch;/* * @创建人: MaLingZhao * @创建时间: 2019/04/19 * @描述： */import com.mlz.elasticsearch.pojo.Item;import org.elasticsearch.client.Client;import org.elasticsearch.client.transport.TransportClient;import org.elasticsearch.common.transport.InetSocketTransportAddress;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.data.elasticsearch.core.ElasticsearchTemplate;import org.springframework.test.context.junit4.SpringRunner;@SpringBootTest(classes = MageElasticsearchApplication.class)@RunWith(SpringRunner.class)public class IndexTest &#123; @Autowired private ElasticsearchTemplate elasticsearchTemplate; @Test public void testCreate()&#123; // 创建索引，会根据Item类的@Document注解信息来创建 elasticsearchTemplate.createIndex(Item.class); // 配置映射，会根据Item类中的id、Field等字段来自动完成映射 elasticsearchTemplate.putMapping(Item.class); &#125; @Test public void deleteIndex() &#123; elasticsearchTemplate.deleteIndex("mage"); &#125;&#125; 5.5 Repository的文档操作Spring Data 的强大之处，就在于你不用写任何DAO处理，自动根据方法名或类的信息进行CRUD操作。只要你定义一个接口，然后继承Repository提供的一些子接口，就能具备各种基本的CRUD功能。 我们只需要定义接口，然后继承它就OK了。 123456789101112131415161718192021package com.mlz.elasticsearch.repository;/* * @创建人: MaLingZhao * @创建时间: 2019/04/19 * @描述： */import com.mlz.elasticsearch.pojo.Item;import org.elasticsearch.client.ElasticsearchClient;import org.springframework.data.elasticsearch.repository.ElasticsearchRepository;public interface ItemRepository extends ElasticsearchRepository&lt;Item,Long&gt; &#123;&#125; 5.4.1.新增文档123456789@Autowiredprivate ItemRepository itemRepository;@Testpublic void index() &#123; Item item = new Item(1L, "小米手机7", " 手机", "小米", 3499.00, "http://image..com/13123.jpg"); itemRepository.save(item);&#125; 去页面查询看看： 1GET /item/_search 结果： 123456789101112131415161718192021222324252627282930&#123; "took": 14, "timed_out": false, "_shards": &#123; "total": 1, "successful": 1, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": 1, "max_score": 1, "hits": [ &#123; "_index": "item", "_type": "docs", "_id": "1", "_score": 1, "_source": &#123; "id": 1, "title": "小米手机7", "category": " 手机", "brand": "小米", "price": 3499, "images": "http://image.mage.com/13123.jpg" &#125; &#125; ] &#125;&#125; 5.4.2.批量新增代码： 12345678@Testpublic void indexList() &#123; List&lt;Item&gt; list = new ArrayList&lt;&gt;(); list.add(new Item(2L, "坚果手机R1", " 手机", "锤子", 3699.00, "http://image.mage.com/123.jpg")); list.add(new Item(3L, "华为META10", " 手机", "华为", 4499.00, "http://image.mage.com/3.jpg")); // 接收对象集合，实现批量新增 itemRepository.saveAll(list);&#125; 再次去页面查询： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&#123; "took": 5, "timed_out": false, "_shards": &#123; "total": 1, "successful": 1, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": 3, "max_score": 1, "hits": [ &#123; "_index": "item", "_type": "docs", "_id": "2", "_score": 1, "_source": &#123; "id": 2, "title": "坚果手机R1", "category": " 手机", "brand": "锤子", "price": 3699, "images": "http://image.mage.com/13123.jpg" &#125; &#125;, &#123; "_index": "item", "_type": "docs", "_id": "3", "_score": 1, "_source": &#123; "id": 3, "title": "华为META10", "category": " 手机", "brand": "华为", "price": 4499, "images": "http://image.mage.com/13123.jpg" &#125; &#125;, &#123; "_index": "item", "_type": "docs", "_id": "1", "_score": 1, "_source": &#123; "id": 1, "title": "小米手机7", "category": " 手机", "brand": "小米", "price": 3499, "images": "http://image.mage.com/13123.jpg" &#125; &#125; ] &#125;&#125; 5.4.3.修改文档修改和新增是同一个接口，区分的依据就是id，这一点跟我们在页面发起PUT请求是类似的。 5.4.4.基本查询ElasticsearchRepository提供了一些基本的查询方法： 我们来试试查询所有： 123456789101112@Testpublic void testQuery()&#123; Optional&lt;Item&gt; optional = this.itemRepository.findById(1l); System.out.println(optional.get());&#125;@Testpublic void testFind()&#123; // 查询全部，并按照价格降序排序 Iterable&lt;Item&gt; items = this.itemRepository.findAll(Sort.by(Sort.Direction.DESC, "price")); items.forEach(item-&gt; System.out.println(item));&#125; 结果： 5.4.5.自定义方法Spring Data 的另一个强大功能，是根据方法名称自动实现功能。 比如：你的方法名叫做：findByTitle，那么它就知道你是根据title查询，然后自动帮你完成，无需写实现类。 当然，方法名称要符合一定的约定： Keyword Sample Elasticsearch Query String And findByNameAndPrice {&quot;bool&quot; : {&quot;must&quot; : [ {&quot;field&quot; : {&quot;name&quot; : &quot;?&quot;}}, {&quot;field&quot; : {&quot;price&quot; : &quot;?&quot;}} ]}} Or findByNameOrPrice {&quot;bool&quot; : {&quot;should&quot; : [ {&quot;field&quot; : {&quot;name&quot; : &quot;?&quot;}}, {&quot;field&quot; : {&quot;price&quot; : &quot;?&quot;}} ]}} Is findByName {&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;name&quot; : &quot;?&quot;}}}} Not findByNameNot {&quot;bool&quot; : {&quot;must_not&quot; : {&quot;field&quot; : {&quot;name&quot; : &quot;?&quot;}}}} Between findByPriceBetween {&quot;bool&quot; : {&quot;must&quot; : {&quot;range&quot; : {&quot;price&quot; : {&quot;from&quot; : ?,&quot;to&quot; : ?,&quot;include_lower&quot; : true,&quot;include_upper&quot; : true}}}}} LessThanEqual findByPriceLessThan {&quot;bool&quot; : {&quot;must&quot; : {&quot;range&quot; : {&quot;price&quot; : {&quot;from&quot; : null,&quot;to&quot; : ?,&quot;include_lower&quot; : true,&quot;include_upper&quot; : true}}}}} GreaterThanEqual findByPriceGreaterThan {&quot;bool&quot; : {&quot;must&quot; : {&quot;range&quot; : {&quot;price&quot; : {&quot;from&quot; : ?,&quot;to&quot; : null,&quot;include_lower&quot; : true,&quot;include_upper&quot; : true}}}}} Before findByPriceBefore {&quot;bool&quot; : {&quot;must&quot; : {&quot;range&quot; : {&quot;price&quot; : {&quot;from&quot; : null,&quot;to&quot; : ?,&quot;include_lower&quot; : true,&quot;include_upper&quot; : true}}}}} After findByPriceAfter {&quot;bool&quot; : {&quot;must&quot; : {&quot;range&quot; : {&quot;price&quot; : {&quot;from&quot; : ?,&quot;to&quot; : null,&quot;include_lower&quot; : true,&quot;include_upper&quot; : true}}}}} Like findByNameLike {&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;name&quot; : {&quot;query&quot; : &quot;?*&quot;,&quot;analyze_wildcard&quot; : true}}}}} StartingWith findByNameStartingWith {&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;name&quot; : {&quot;query&quot; : &quot;?*&quot;,&quot;analyze_wildcard&quot; : true}}}}} EndingWith findByNameEndingWith {&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;name&quot; : {&quot;query&quot; : &quot;*?&quot;,&quot;analyze_wildcard&quot; : true}}}}} Contains/Containing findByNameContaining {&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;name&quot; : {&quot;query&quot; : &quot;**?**&quot;,&quot;analyze_wildcard&quot; : true}}}}} In findByNameIn(Collection&lt;String&gt;names) {&quot;bool&quot; : {&quot;must&quot; : {&quot;bool&quot; : {&quot;should&quot; : [ {&quot;field&quot; : {&quot;name&quot; : &quot;?&quot;}}, {&quot;field&quot; : {&quot;name&quot; : &quot;?&quot;}} ]}}}} NotIn findByNameNotIn(Collection&lt;String&gt;names) {&quot;bool&quot; : {&quot;must_not&quot; : {&quot;bool&quot; : {&quot;should&quot; : {&quot;field&quot; : {&quot;name&quot; : &quot;?&quot;}}}}}} Near findByStoreNear Not Supported Yet ! True findByAvailableTrue {&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;available&quot; : true}}}} False findByAvailableFalse {&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;available&quot; : false}}}} OrderBy findByAvailableTrueOrderByNameDesc {&quot;sort&quot; : [{ &quot;name&quot; : {&quot;order&quot; : &quot;desc&quot;} }],&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;available&quot; : true}}}} 例如，我们来按照价格区间查询，定义这样的一个方法： 12345678910public interface ItemRepository extends ElasticsearchRepository&lt;Item,Long&gt; &#123; /** * 根据价格区间查询 * @param price1 * @param price2 * @return */ List&lt;Item&gt; findByPriceBetween(double price1, double price2);&#125; 然后添加一些测试数据： 1234567891011@Testpublic void indexList() &#123; List&lt;Item&gt; list = new ArrayList&lt;&gt;(); list.add(new Item(1L, "小米手机7", "手机", "小米", 3299.00, "http://image.mage.com/13123.jpg")); list.add(new Item(2L, "坚果手机R1", "手机", "锤子", 3699.00, "http://image.mage.com/13123.jpg")); list.add(new Item(3L, "华为META10", "手机", "华为", 4499.00, "http://image.mage.com/13123.jpg")); list.add(new Item(4L, "小米Mix2S", "手机", "小米", 4299.00, "http://image.mage.com/13123.jpg")); list.add(new Item(5L, "荣耀V10", "手机", "华为", 2799.00, "http://image.mage.com/13123.jpg")); // 接收对象集合，实现批量新增 itemRepository.saveAll(list);&#125; 不需要写实现类，然后我们直接去运行： 1234567@Testpublic void queryByPriceBetween()&#123; List&lt;Item&gt; list = this.itemRepository.findByPriceBetween(2000.00, 3500.00); for (Item item : list) &#123; System.out.println("item = " + item); &#125;&#125; 结果： 虽然基本查询和自定义方法已经很强大了，但是如果是复杂查询（模糊、通配符、词条查询等）就显得力不从心了。此时，我们只能使用原生查询。 5.5.高级查询5.5.1.基本查询先看看基本玩法 12345678@Testpublic void testQuery()&#123; // 词条查询 MatchQueryBuilder queryBuilder = QueryBuilders.matchQuery("title", "小米"); // 执行查询 Iterable&lt;Item&gt; items = this.itemRepository.search(queryBuilder); items.forEach(System.out::println);&#125; Repository的search方法需要QueryBuilder参数，elasticSearch为我们提供了一个对象QueryBuilders： QueryBuilders提供了大量的静态方法，用于生成各种不同类型的查询对象，例如：词条、模糊、通配符等QueryBuilder对象。 结果： elasticsearch提供很多可用的查询方式，但是不够灵活。如果想玩过滤或者聚合查询等就很难了。 5.5.2.自定义查询先看最基本的match query： 1234567891011121314@Testpublic void testNativeQuery()&#123; // 构建查询条件 NativeSearchQueryBuilder queryBuilder = new NativeSearchQueryBuilder(); // 添加基本的分词查询 queryBuilder.withQuery(QueryBuilders.matchQuery("title", "小米")); // 执行搜索，获取结果 Page&lt;Item&gt; items = this.itemRepository.search(queryBuilder.build()); // 打印总条数 System.out.println(items.getTotalElements()); // 打印总页数 System.out.println(items.getTotalPages()); items.forEach(System.out::println);&#125; NativeSearchQueryBuilder：Spring提供的一个查询条件构建器，帮助构建json格式的请求体 Page&lt;item&gt;：默认是分页查询，因此返回的是一个分页的结果对象，包含属性： totalElements：总条数 totalPages：总页数 Iterator：迭代器，本身实现了Iterator接口，因此可直接迭代得到当前页的数据 5.5.4.分页查询利用NativeSearchQueryBuilder可以方便的实现分页： 12345678910111213141516171819202122232425@Testpublic void testNativeQuery()&#123; // 构建查询条件 NativeSearchQueryBuilder queryBuilder = new NativeSearchQueryBuilder(); // 添加基本的分词查询 queryBuilder.withQuery(QueryBuilders.termQuery("category", "手机")); // 初始化分页参数 int page = 0; int size = 3; // 设置分页参数 queryBuilder.withPageable(PageRequest.of(page, size)); // 执行搜索，获取结果 Page&lt;Item&gt; items = this.itemRepository.search(queryBuilder.build()); // 打印总条数 System.out.println(items.getTotalElements()); // 打印总页数 System.out.println(items.getTotalPages()); // 每页大小 System.out.println(items.getSize()); // 当前页 System.out.println(items.getNumber()); items.forEach(System.out::println);&#125; 结果： 可以发现，Elasticsearch中的分页是从第0页开始。 5.5.5.排序排序也通用通过NativeSearchQueryBuilder完成： 12345678910111213141516@Testpublic void testSort()&#123; // 构建查询条件 NativeSearchQueryBuilder queryBuilder = new NativeSearchQueryBuilder(); // 添加基本的分词查询 queryBuilder.withQuery(QueryBuilders.termQuery("category", "手机")); // 排序 queryBuilder.withSort(SortBuilders.fieldSort("price").order(SortOrder.DESC)); // 执行搜索，获取结果 Page&lt;Item&gt; items = this.itemRepository.search(queryBuilder.build()); // 打印总条数 System.out.println(items.getTotalElements()); items.forEach(System.out::println);&#125; 结果： 5.6.聚合5.6.1.聚合为桶桶就是分组，比如这里我们按照品牌brand进行分组： 12345678910111213141516171819202122232425@Testpublic void testAgg()&#123; NativeSearchQueryBuilder queryBuilder = new NativeSearchQueryBuilder(); // 不查询任何结果 queryBuilder.withSourceFilter(new FetchSourceFilter(new String[]&#123;""&#125;, null)); // 1、添加一个新的聚合，聚合类型为terms，聚合名称为brands，聚合字段为brand queryBuilder.addAggregation( AggregationBuilders.terms("brands").field("brand")); // 2、查询,需要把结果强转为AggregatedPage类型 AggregatedPage&lt;Item&gt; aggPage = (AggregatedPage&lt;Item&gt;) this.itemRepository.search(queryBuilder.build()); // 3、解析 // 3.1、从结果中取出名为brands的那个聚合， // 因为是利用String类型字段来进行的term聚合，所以结果要强转为StringTerm类型 StringTerms agg = (StringTerms) aggPage.getAggregation("brands"); // 3.2、获取桶 List&lt;StringTerms.Bucket&gt; buckets = agg.getBuckets(); // 3.3、遍历 for (StringTerms.Bucket bucket : buckets) &#123; // 3.4、获取桶中的key，即品牌名称 System.out.println(bucket.getKeyAsString()); // 3.5、获取桶中的文档数量 System.out.println(bucket.getDocCount()); &#125;&#125; 显示的结果： 关键API： AggregationBuilders：聚合的构建工厂类。所有聚合都由这个类来构建，看看他的静态方法： AggregatedPage：聚合查询的结果类。它是Page&lt;T&gt;的子接口： AggregatedPage在Page功能的基础上，拓展了与聚合相关的功能，它其实就是对聚合结果的一种封装，大家可以对照聚合结果的JSON结构来看。 而返回的结果都是Aggregation类型对象，不过根据字段类型不同，又有不同的子类表示 我们看下页面的查询的JSON结果与Java类的对照关系： 5.6.2.嵌套聚合，求平均值代码： 1234567891011121314151617181920212223242526272829@Testpublic void testSubAgg()&#123; NativeSearchQueryBuilder queryBuilder = new NativeSearchQueryBuilder(); // 不查询任何结果 queryBuilder.withSourceFilter(new FetchSourceFilter(new String[]&#123;""&#125;, null)); // 1、添加一个新的聚合，聚合类型为terms，聚合名称为brands，聚合字段为brand queryBuilder.addAggregation( AggregationBuilders.terms("brands").field("brand") .subAggregation(AggregationBuilders.avg("priceAvg").field("price")) // 在品牌聚合桶内进行嵌套聚合，求平均值 ); // 2、查询,需要把结果强转为AggregatedPage类型 AggregatedPage&lt;Item&gt; aggPage = (AggregatedPage&lt;Item&gt;) this.itemRepository.search(queryBuilder.build()); // 3、解析 // 3.1、从结果中取出名为brands的那个聚合， // 因为是利用String类型字段来进行的term聚合，所以结果要强转为StringTerm类型 StringTerms agg = (StringTerms) aggPage.getAggregation("brands"); // 3.2、获取桶 List&lt;StringTerms.Bucket&gt; buckets = agg.getBuckets(); // 3.3、遍历 for (StringTerms.Bucket bucket : buckets) &#123; // 3.4、获取桶中的key，即品牌名称 3.5、获取桶中的文档数量 System.out.println(bucket.getKeyAsString() + "，共" + bucket.getDocCount() + "台"); // 3.6.获取子聚合结果： InternalAvg avg = (InternalAvg) bucket.getAggregations().asMap().get("priceAvg"); System.out.println("平均售价：" + avg.getValue()); &#125;&#125; 结果：]]></content>
      <categories>
        <category>javaEE</category>
      </categories>
      <tags>
        <tag>项目实战</tag>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BlockChain]]></title>
    <url>%2Fblog4%2F2019%2F10%2F18%2FBlockChain%2F</url>
    <content type="text"><![CDATA[1 区块链技术比特币 莱特币 比特币的市场份额 计算机专业 前提： — 数组 链表 二叉树 哈希函数 BitCoin 以太坊 白皮书 黄皮书 源代码]]></content>
  </entry>
  <entry>
    <title><![CDATA[sublime之旅]]></title>
    <url>%2Fblog4%2F2019%2F10%2F17%2Fsublime%2F</url>
    <content type="text"><![CDATA[1 sublime的安装sublime的安装非常简单 只需要官网下载直接安装即可 sublime Txet可谓是一个现在非常流行的编辑器 闲来无事 配置配置 首先要聊的是sublime的主题 主题的安装方式有很多种 最方便的是在先安装之间 crtl +shift+p 快捷键 我们在命令行输入Install package 会出现 各种各样的插件 主题之类的东西 我们可以选择自己喜欢的主题 经过一些简单的配置完全可以充当ide来使用 但是一般我们下载了sublime之后并不能够使用 我们需要进入sublime插件的官方的地址 https://packagecontrol.io/ https://packagecontrol.io/installation#st3 需要注意的是 我们可能不太好用此时我们可以找到view 执行这一步的操作 然后将在哪个网站的链接copy过来 前提一定要有python的安装环境 这个网上不了没关系 我们可以离线安装 2 sublime插件接下来介绍一下sublime的插件 介绍一下SublimeCodeIntel自动补齐插件 这款插件能够实现python代码的自动提示功能但是需要配置相应插件的settings文件实现路径的配置 而我更喜欢的是使用anacona插件来进行python代码的编写，功能不比Pycharm差，但是需要自己相应的设置settiings文件 首先安装anaconda 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695/* Anaconda default settings*/&#123; /* Anaconda Tooltip Options Sublime Text 3 supports tooltips since build 3070; if anaconda runs in a ST3 installation equal to or newer than build 3070, it can be configured to show tooltips for documentation and signatures. The options below set how the tooltip looks. */ "anaconda_tooltip_theme": "popup", /* If you set the following option as true, anaconda will display the signatures of the methods in a floating popup. *note*: this only has an effect if "display_signatures" is true. */ "enable_signatures_tooltip": true, /* If you set the following option as true, anaconda will display the documentation of modules, classes or methods in a floating popup. */ "enable_docstrings_tooltip": true, /* If the option below is set as true, anaconda will merge the outputs of display_signatures and show documentation. *note*: This only has an effect if tooltips are enabled. */ "merge_signatures_and_doc": true, /* If you set the following option as true, anaconda will display the signatures of the methods you are using while typing */ "display_signatures": true, /* Auto project switching If you set this to true, anaconda will detect project switches and will reconnect a new JsonServer with the switched project configuration. */ "auto_project_switch": true, /* Python Builder Options If enabled, anaconda will add your configured interpreter (if any) into your per-project `build_systems` list if any, or create a new one in case you don't define one yourself. This is enabled by default; disable it if you experience slowdowns when you switch between projects, and file an issue in GitHub. */ "auto_python_builder_enabled": true, /* Debug Mode: If you need to debug the jsonserver.py service for any reason, you can configure the following options to make the plugin to connect always to the same port in your local machine. Procedure: 1. Set the debug_mode as true 2. Set the debug_port to your desired port 3. Close Sublime Text 3 4. cd to your Packages/Anaconda directory 5. Start the jsonserver manually: python -B anaconda_server/jsonserver.py -p test 9999 DEBUG 6. Start your Sublime Text 3 and Then your anaconda plugin will use the configured server, and you will get debug messages in the terminal where you ran it. */ "jsonserver_debug": false, "jsonserver_debug_port": 9999, /* Default python interpreter This can (and should) be overridden by project settings. NOTE: if you want anaconda to lint and complete using a remote python interpreter that runs the anaconda's minserver.py script in a remote machine just use it's address:port as interpreter for example: "python_interpreter": "tcp://my_remote.machine.com:19360" */ "python_interpreter": "D:\\java\\anaconda\\python.exe", /* Disable anaconda completion WARNING: setting this as true will totally disable anaconda completion. */ "disable_anaconda_completion": false, /* Set those as true if you don't want Sublime Text regular completions */ "suppress_word_completions":true, "suppress_explicit_completions": true, /* If complete_parameters is true, anaconda will add function and class parameters to its completions. If complete_all_parameters is true, it will add all the possible parameters, if it's false, it will add only required parameters */ "complete_parameters": true, "complete_all_parameters": false, /* If you set the following option as true, anaconda will complete function parameters on keypress, when cursor is between function brackets, independently from any other setting. It also works with disabled tooltips. Moreover you'll be able to choose at the moment of insertion, whether inserting all parameters or just the required parameters. Default keybindings (they only work between empty function brackets): Tab : insert required parameters Ctrl+Tab : insert all parameters */ "parameters_completion_on_keypress": true, /* The following set of options controls the autopep autoformatting behaviour. The full list of errors that can be fixed are: E101 - Reindent all lines. E111 - Reindent all lines. E121 - Fix indentation to be a multiple of four. E122 - Add absent indentation for hanging indentation. E123 - Align closing bracket to match opening bracket. E124 - Align closing bracket to match visual indentation. E125 - Indent to distinguish line from next logical line. E126 - Fix over-indented hanging indentation. E127 - Fix visual indentation. E128 - Fix visual indentation. E129 - Indent to distinguish line from next logical line. E201 - Remove extraneous whitespace. E202 - Remove extraneous whitespace. E203 - Remove extraneous whitespace. E211 - Remove extraneous whitespace. E221 - Fix extraneous whitespace around keywords. E222 - Fix extraneous whitespace around keywords. E223 - Fix extraneous whitespace around keywords. E224 - Remove extraneous whitespace around operator. E225 - Fix missing whitespace around operator. E226 - Fix missing whitespace around operator. E227 - Fix missing whitespace around operator. E228 - Fix missing whitespace around operator. E231 - Add missing whitespace. E241 - Fix extraneous whitespace around keywords. E242 - Remove extraneous whitespace around operator. E251 - Remove whitespace around parameter '=' sign. E261 - Fix spacing before comment hash. E262 - Fix spacing after comment hash. E271 - Fix extraneous whitespace around keywords. E272 - Fix extraneous whitespace around keywords. E273 - Fix extraneous whitespace around keywords. E274 - Fix extraneous whitespace around keywords. E301 - Add missing blank line. E302 - Add missing 2 blank lines. E303 - Remove extra blank lines. E304 - Remove blank line following function decorator. E401 - Put imports on separate lines. E501 - Try to make lines fit within --max-line-length characters. E502 - Remove extraneous escape of newline. E701 - Put colon-separated compound statement on separate lines. E702 - Put semicolon-separated compound statement on separate lines. E703 - Put semicolon-separated compound statement on separate lines. E711 - Fix comparison with None. E712 - Fix comparison with boolean. W191 - Reindent all lines. W291 - Remove trailing whitespace. W293 - Remove trailing whitespace on blank line. W391 - Remove trailing blank lines. E26 - Format block comments. W6 - Fix various deprecated code (via lib2to3). W602 - Fix deprecated form of raising exception. */ /* Autoformat files on save This option is disabled by default, AutoPEP8 is really slow and it get the file buffer read only while its working in the background. Use this at your own risk. */ "auto_formatting": true, /* Timeout for the autoformatting tool before fail Note: Take into account that depending on the size of the file you want to autoformat, this can take a while. */ "auto_formatting_timeout": 1, // seconds /* Enable unsafe changes: https://github.com/hhatto/autopep8#more-advanced-usage Set it as 0 if you are unsure here. */ "aggressive": 0, /* Do not fix the errors listed here: Note: By default we ignore E309 as this is not pep8 but it does conflict with pep257 D211. */ "autoformat_ignore": [ "E309" ], /* Fix only the errors listed here: */ "autoformat_select": [ ], /* Set the threshold limit for McCabe complexity checker. */ "mccabe_threshold": 7, /* Set to false to disable Anaconda linting entirely. */ "anaconda_linting": true, /* Sets the linting behaviour for anaconda: "always" - Linting works always even while you are writing (in the background) "load-save" - Linting works in file load and save only "save-only" - Linting works in file save only */ "anaconda_linting_behaviour": "save only", /* The minimum delay in seconds (fractional seconds are okay) before a linter is run when the "anaconda_linting" setting is true. This allows you to have background linting active, but defer the actual linting until you are idle. When this value is greater than the built in linting delay, errors are erased when the file is modified, since the assumption is you don't want to see errors while you type (unless the option anaconda_linter_persistent is true). */ "anaconda_linter_delay": 0.5, /* If true, anaconda does not remove lint marks while you type. */ "anaconda_linter_persistent": false, // If true, anaconda draws gutter marks on line with errors. "anaconda_gutter_marks": true, /* If anaconda_gutter_marks is true, this determines what theme is used. Theme 'basic' only adds dots and circles to gutter. Other available themes are 'alpha', 'bright', 'dark', 'hard', "retina" (for retina displays) and 'simple'. To see icons that will be used for each theme check gutter_icon_themes folder in Anaconda package. */ "anaconda_gutter_theme": "basic", /* If 'outline' (default), anaconda will outline error lines. If 'fill', anaconda will fill the lines. If 'solid_underline', anaconda will draw a solid underline below regions. If 'stippled_underline', anaconda will draw a stippled underline below regions. If 'squiggly_underline', anaconda will draw a squiggly underline below regions. If 'none', anaconda will not draw anything on error lines. */ "anaconda_linter_mark_style": "outline", /* If this is set to false, anaconda will not underline errors. */ "anaconda_linter_underlines": true, /* If this is set to true, anaconda will show errors inline. */ "anaconda_linter_phantoms": true, /* This determines what theme is phantoms used. */ "anaconda_linter_phantoms_theme": "phantom", /* This determines what template is phantoms used. */ "anaconda_linter_phantoms_template": "default", /* If anaconda_linter_show_errors_on_save is set to true, anaconda will show a list of errors when you save the file. This is disabled by default. */ "anaconda_linter_show_errors_on_save": false, /* Use PyLint instead of PyFlakes and PEP-8 **** WARNING **** - If you set this value as true, PyFlakes and pep8 will not be used. - PyLint does *NOT* support linting buffers that are not already saved in the file system. **** WARNING **** */ "use_pylint": false, // Set this to false to turn pep8 checking off completely. "pep8": true, /* If set, the given file will be used as configuration for pep8. **** WARNING **** - If this option is set to something other than false, pep8_ignore and pep8_max_line_length will be silently ignored. **** WARNING **** */ "pep8_rcfile": false, /* A list of pep8 error numbers to ignore. The list of error codes is in this file: https://pycodestyle.readthedocs.io/en/latest/intro.html#error-codes. Search for "Ennn:", where nnn is a 3-digit number. E309 is ignored by default as it conflicts with pep257 E211 */ "pep8_ignore": [ "E309" ], // Maximum line length for pep8 "pep8_max_line_length": 79, /* You can override the level of PEP8 errors altering this mapping. WARNING: Be careful with this option, the only valid error level values are: * E for errors (reduced to Warning by default) * W for warnings (reduced to Violation by default) * V for convention violations If you set a value that is not listed here, anaconda will ignore your setting */ "pep8_error_levels": &#123;"E": "W", "W": "V", "V": "V"&#125;, // Set this to true to turn pep257 checking on. "pep257": false, /* A list of pep257 error numbers to ignore. The list can be found here: http://www.pydocstyle.org/en/2.1.1/error_codes.html D209: Multi-line docstring should end with 1 blank line is ignored by default as this rule has been deprecated. D203: 1 blank line before class docstring is ignored by default, as this rule has been deprecated. */ "pep257_ignore": [ "D203", "D209" ], /* You can ignore some of the "undefined name xxx" errors (comes in handy if you work with post-processors, globals/builtins available only at runtime, etc.). You can control what names will be ignored with the user setting "pyflakes_ignore". Example: "pyflakes_ignore": [ "some_custom_builtin_o_mine", "A_GLOBAL_CONSTANT" ], */ "pyflakes_ignore": [ ], /* Specific error types to ignore for pyflakes. (Uncomment to ignore the below error types.) */ "pyflakes_explicit_ignore": [ // "Redefined", "UnusedImport", // "UndefinedName", // "UndefinedLocal", // "UnusedVariable,", // "UndefinedExport", // "DuplicateArgument", // "RedefinedWhileUnused" ], /* If set, the given file will be used as configuration for PyLint. **** WARNING **** - If this option is set to something different than false, pylint_ignore will be silently ignored **** WARNING **** */ "pylint_rcfile": false, /* You can ignore specific PyLint error codes using this configuration. We strongly suggest that you better configure your pylintrc file to determine which type of error is important to you. */ "pylint_ignore": [ // "0201" ], /* Ordinarily pyflakes will issue a warning when 'from foo import *' is used; this is ignored by default. If you want to see this warning, set this option to false. */ "pyflakes_ignore_import_*": true, /* Set the following option to true if you want anaconda to check the validity of your imports when the linting process is fired. WARNING: take into account that anaconda compiles and import the modules in the JsonServer memory segment in order to check this */ "validate_imports": false, /* MyPy Set the following option to true to enable MyPy checker. */ "mypy": false, /* Set the following variable to set MyPy MYPYPATH. */ "mypy_mypypath": "", /* MyPy Silent Imports If true, mypy will not follow imports to .py files. */ "mypy_silent_imports": false, /* MyPy Almost Silent Same as Silent Imports, but report the imports as errors. */ "mypy_almost_silent": false, /* MyPy suppress stub warnings Suppress stub warnings when silent_imports is not used. */ "mypy_suppress_stub_warnings": true, /* MyPy py2 Use Python 2 mode. */ "mypy_py2": false, /* MyPy Disallow Untyped Calls Disallow calling functions without type annotations from functions with type annotations. */ "mypy_disallow_untyped_calls": false, /* MyPy Disallow Untyped Defs Disallow defining functions without type annotations or with incomplete type annotations. */ "mypy_disallow_untyped_defs": false, /* MyPy Check Untyped Defs Type check the interior of functions without type annotations. */ "mypy_check_untyped_defs": false, /* MyPy Fast Parser Enable the experimental fast parser module of MyPy. In practice this seems to be slower, however is required for support of some recent syntax changes in Python. */ "mypy_fast_parser": false, /* MyPy Custom Typing Use a custom typing module. Uncomment to enable. */ // "mypy_custom_typing": "", /* Debug and development options Use those options only for debugging and development purposes, they can and will slowdown your Sublime Text anconda_debug can be set as: false or profiler If this is set as profiler, Anaconda will run a profiler on autocomplete and print its results in the Sublime Text console. NOTE: cProfile can't be imported in all the platforms, at least not in Linux, this is an already reported bug: http://www.sublimetext.com/forum/viewtopic.php?f=3&amp;t=13698 https://github.com/SublimeText/Issues/issues/127 */ "anaconda_debug": false, /* Anaconda test runner related configuration. The anaconda's test runner is originally a contribution by NorthIsUp https://github.com/NorthIsUp Run Twisted's trial based test suite under a virtualenv project: // this is your &lt;ProjectName&gt;.sublime-project file "settings": &#123; "python_interpreter": "~/.virtualenvs/project_name", "test_command": "trial", "test_virtualenv": "~/.virtualenvs/project_name", "test_project_path": "project_src" &#125; Run Django project tests using nose2 with a virtualenv: // this is your &lt;ProjectName&gt;.sublime-project file "settings": &#123; "python_interpreter": "~/.virtualenvs/django_project_name", "test_command": "./manage.py test --settings=tests.settings --noinput" &#125; */ /* Theme to use in the output panel. Uncomment the line below to override the default tests runner output, by default the theme is PythonConsoleDark.hidden-tmTheme NOTE: The file specified here **MUST** exists in `Packages/Anaconda` */ //"test_runner_theme": "PythonConsoleDark.hidden-tmTheme", /* Uncomment the line below to execute any command before try to execute your test suite. (if you need to run more than one command, just use a list of commands ["cmd1", "cmd2", ...]) */ // "test_before_command": "", /* Command to execute tests with. nosetests by default */ "test_command": "nosetests", /* Uncomment the line below to execute any command after execute your test suite. */ //"test_after_command": "", /* This is the delimiter between the module and the class. */ "test_delimeter": ":", /* This is the delimiter between the class and the method. */ "test_method_delimeter": ".", /* Whether to include full path of the test file/method. */ "test_include_full_path": true, /* Uncomment the line below to enable filepath-based test runner patterns like those required for py.test. */ // "test_filepath_patterns": true, /* Uncomment the line below to use a virtualenv for your tests */ //"test_virtualenv": "" /* Uncomment the line below to add a top level path when you ask anaconda to run all your project tests. Note: this is need for example to can run tests with `twisted's trial` */ // "test_project_path": "./" /* Set this option to true if you get warnings about anaconda not being able to connect to your localhost on ST3 startup but it works normally after closing the error message. NOTE: Ignoring errors may be dangerous. */ /* Parameters to include after test_command specific to testing scope */ "test_params": &#123; "current_file_tests": "", "current_test": "", "project_tests": "" &#125;, "swallow_startup_errors": false, /* If this is set to true, anaconda will unload plugins that make it malfunction interfering with it */ "auto_unload_conflictive_plugins": true, /* If this is set to true, anaconda will search for environment hook files in any directory level down to the one the current file is in as opposed to the default where it will only search down to the root of your working directory. */ "anaconda_allow_project_environment_hooks": false&#125;// vim: set ft=javascript: 这是相应的插件的设置，我们可以尝试 但是建议将这个设置关闭 否则会影响anaconda的自动提示 （sublime的自动提示和anaconda的提示出现冲突） 两个都设置成true 12"suppress_word_completions":true, "suppress_explicit_completions": true python.exe 的路径 1"python_interpreter": "D:\\java\\anaconda\\python.exe" 代码检查的配置也有我们大家可以进行搜索进行相应的设置 当然了sublime几乎支持当下所有热门的编程语言 插件的安装还有一种安装方式 直接下载github上的主题或者插件进行文件的安装 打开文件位置 将我们需要安装的东西放到里面去就行了 ， 主题的话点击theme 安装完成之后就会进行自动的切换 Theme- Afterglow这个插件就是我在github下载然后拖进去的 当然了，插件很多选择自己喜欢的就好了，自己也就业余的研究一下，但是我想用这款编译器写go，python，C++，htmlo应该挺爽的 其配置插件之后，简单易用。]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>编辑器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[马哥商城续]]></title>
    <url>%2Fblog4%2F2019%2F10%2F17%2FmageMall2%2F</url>
    <content type="text"><![CDATA[10 商品的规格数据结构10.1 spu和skuSPU：Standard Product Unit （标准产品单位） ，一组具有共同属性的商品集 SKU：Stock Keeping Unit（库存量单位），SPU商品集因具体特性不同而细分的每个商品 SPU是一个抽象的商品集概念，为了方便后台的管理。 SKU才是具体要销售的商品，每一个SKU的价格、库存可能会不一样，用户购买的是SKU而不是SPU 1.2 数据库设计1.2.1 发现问题spu的字段 123456789id:主键title：标题description：描述specification：规格packaging_list：包装after_service：售后服务comment：评价category_id：商品分类brand_id：品牌 sku的字段 12345678id：主键spu_id：关联的spuprice：价格images：图片stock：库存颜色？内存？硬盘？ 不同的商品分类，可能属性是不一样的，比如手机有内存，衣服有尺码，我们是全品类的电商网站，这些不同的商品的不同属性，那内存，磁盘等属性都是规格参数 1.2.2.分析规格参数虽然商品规格千变万化，但是同一类商品（如手机）的规格是统一的，有图为证： 1.2.3.SKU的特有属性SPU中会有一些特殊属性，用来区分不同的SKU，我们称为SKU特有属性。如华为META10的颜色、内存属性。 不同种类的商品，一个手机，一个衣服，其SKU属性不相同。 同一种类的商品，比如都是衣服，SKU属性基本是一样的，都是颜色、尺码等。 这样说起来，似乎SKU的特有属性也是与分类相关的？事实上，仔细观察你会发现，SKU的特有属性是商品规格参数的一部分： 1.2.4.搜索属性你会发现，过滤条件中的屏幕尺寸、运行内存、网路、机身内存、电池容量、CPU核数等，在规格参数中都能找到： 也就是说，规格参数中的数据，将来会有一部分作为搜索条件来使用。我们可以在设计时，将这部分属性标记出来，将来做搜索的时候，作为过滤条件。要注意的是，无论是SPU的全局属性，还是SKU的特有属性，都有可能作为搜索过滤条件的，并不冲突，而是有一个交集： 1.3.规格参数表1.3.1.表结构 可以看到规格参数是分组的，每一组都有多个参数键值对。不过对于规格参数的模板而言，其值现在是不确定的，不同的商品值肯定不同，模板中只要保存组信息、组内参数信息即可。 因此我设计了两张表 tb_spec_group：组，与商品分类关联 tb_spec_param：参数名，与组关联，一对多 1.3.2.规格组规格参数分组表：tb_spec_group 1234567CREATE TABLE `tb_spec_group` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT '主键', `cid` bigint(20) NOT NULL COMMENT '商品分类id，一个分类下有多个规格组', `name` varchar(50) NOT NULL COMMENT '规格组的名称', PRIMARY KEY (`id`), KEY `key_category` (`cid`)) ENGINE=InnoDB AUTO_INCREMENT=14 DEFAULT CHARSET=utf8 COMMENT='规格参数的分组表，每个商品分类下有多个规格参数组'; 规格组有3个字段： id：主键 cid：商品分类id，一个分类下有多个模板 name：该规格组的名称。 1.3.2.规格参数规格参数表：tb_spec_param 1234567891011121314CREATE TABLE `tb_spec_param` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT '主键', `cid` bigint(20) NOT NULL COMMENT '商品分类id', `group_id` bigint(20) NOT NULL, `name` varchar(255) NOT NULL COMMENT '参数名', `numeric` tinyint(1) NOT NULL COMMENT '是否是数字类型参数，true或false', `unit` varchar(255) DEFAULT '' COMMENT '数字类型参数的单位，非数字类型可以为空', `generic` tinyint(1) NOT NULL COMMENT '是否是sku通用属性，true或false', `searching` tinyint(1) NOT NULL COMMENT '是否用于搜索过滤，true或false', `segments` varchar(1000) DEFAULT '' COMMENT '数值类型参数，如果需要搜索，则添加分段间隔值，如CPU频率间隔：0.5-1.0', PRIMARY KEY (`id`), KEY `key_group` (`group_id`), KEY `key_category` (`cid`)) ENGINE=InnoDB AUTO_INCREMENT=24 DEFAULT CHARSET=utf8 COMMENT='规格参数组下的参数名'; 按道理来说，我们的规格参数就只需要记录参数名、组id、商品分类id即可。但是这里却多出了很多字段，为什么？ 还记得我们之前的分析吧，规格参数中有一部分是 SKU的通用属性，一部分是SKU的特有属性，而且其中会有一些将来用作搜索过滤，这些信息都需要标记出来。 通用属性 用一个布尔类型字段来标记是否为通用： generic来标记是否为通用属性： true：代表通用属性 false：代表sku特有属性 搜索过滤 与搜索相关的有两个字段： searching：标记是否用作过滤 true：用于过滤搜索 false：不用于过滤 segments：某些数值类型的参数，在搜索时需要按区间划分，这里提前确定好划分区间 比如电池容量，02000mAh，2000mAh3000mAh，3000mAh~4000mAh 数值类型 某些规格参数可能为数值类型，这样的数据才需要划分区间，我们有两个字段来描述： numberic：是否为数值类型 true：数值类型 false：不是数值类型 unit：参数的单位 2.商品规格2.1.页面布局 因为规格是跟商品分类绑定的，因此首先会展现商品分类树，并且提示你要选择商品分类，才能看到规格参数的模板。一起了解下页面的实现： 这里使用了v-layout来完成页面布局，并且添加了row属性，代表接下来的内容是行布局（左右）。 可以看出页面分成2个部分： &lt;v-flex xs3&gt;：左侧，内部又分上下两部分：商品分类树及标题 v-card-title：标题部分，这里是提示信息，告诉用户要先选择分类，才能看到模板 v-tree：这里用到的是我们之前讲过的树组件，展示商品分类树， &lt;v-flex xs9 class=&quot;px-1&quot;&gt;：右侧：内部是规格参数展示 2.1.2.右侧规格2.2.后端代码 实体类 在mage-item-interface中添加实体类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package com.mage.item.pojo;import javax.persistence.*;import java.util.List;/* * @创建人: MaLingZhao * @创建时间: 2019/4/17 * @描述： */@Table(name = "tb_spec_group")public class SpecGroup &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private Long cid; private String name; @Transient private List&lt;SpecParam&gt; params; public Long getId() &#123; return id; &#125; public void setId(Long id) &#123; this.id = id; &#125; public Long getCid() &#123; return cid; &#125; public void setCid(Long cid) &#123; this.cid = cid; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public List&lt;SpecParam&gt; getParams() &#123; return params; &#125; public void setParams(List&lt;SpecParam&gt; params) &#123; this.params = params; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100package com.mage.item.pojo;/* * @创建人: MaLingZhao * @创建时间: 2019/4/17 * @描述： */import javax.persistence.*;@Table(name = "tb_spec_param")public class SpecParam &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private Long cid; private Long groupId; private String name; @Column(name = "`numeric`") private Boolean numeric; private String unit; private Boolean generic; private Boolean searching; private String segments; public Long getId() &#123; return id; &#125; public void setId(Long id) &#123; this.id = id; &#125; public Long getCid() &#123; return cid; &#125; public void setCid(Long cid) &#123; this.cid = cid; &#125; public Long getGroupId() &#123; return groupId; &#125; public void setGroupId(Long groupId) &#123; this.groupId = groupId; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Boolean getNumeric() &#123; return numeric; &#125; public void setNumeric(Boolean numeric) &#123; this.numeric = numeric; &#125; public String getUnit() &#123; return unit; &#125; public void setUnit(String unit) &#123; this.unit = unit; &#125; public Boolean getGeneric() &#123; return generic; &#125; public void setGeneric(Boolean generic) &#123; this.generic = generic; &#125; public Boolean getSearching() &#123; return searching; &#125; public void setSearching(Boolean searching) &#123; this.searching = searching; &#125; public String getSegments() &#123; return segments; &#125; public void setSegments(String segments) &#123; this.segments = segments; &#125;&#125; 在mage-item-service中编写业务： mapper 12345678910111213141516package com.mage.item.mapper;/* * @创建人: MaLingZhao * @创建时间: 2019/4/17 * @描述： */import com.mage.item.pojo.SpecGroup;import tk.mybatis.mapper.common.Mapper;public interface SpecificationGroupMapper extends Mapper&lt;SpecGroup&gt; &#123;&#125; controller 1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.mage.item.controller;/* * @创建人: MaLingZhao * @创建时间: 2019/4/17 * @描述： */import com.mage.item.pojo.SpecGroup;import com.mage.item.service.SpecificationService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.http.ResponseEntity;import org.springframework.util.CollectionUtils;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;import java.util.List;@RestController@RequestMapping("spec")public class SpecificationController &#123; @Autowired private SpecificationService specificationService; /** * 根据分类id查询分组 * @param cid * @return */ @GetMapping("groups/&#123;cid&#125;") public ResponseEntity&lt;List&lt;SpecGroup&gt;&gt; queryGroupsByCid(@PathVariable("cid")Long cid)&#123; List&lt;SpecGroup&gt; groups = this.specificationService.queryGroupsByCid(cid); if (CollectionUtils.isEmpty(groups))&#123; return ResponseEntity.notFound().build(); &#125; return ResponseEntity.ok(groups); &#125;&#125; service 123456789101112131415161718192021222324252627282930313233package com.mage.item.service;/* * @创建人: MaLingZhao * @创建时间: 2019/4/17 * @描述： */import com.mage.item.mapper.SpecificationGroupMapper;import com.mage.item.pojo.SpecGroup;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import java.util.List;@Servicepublic class SpecificationService &#123; @Autowired private SpecificationGroupMapper groupMapper; public List&lt;SpecGroup&gt; queryGroupsByCid(Long cid) &#123; SpecGroup specGroup=new SpecGroup(); specGroup.setCid(cid); return this.groupMapper.select(specGroup); &#125;&#125; 3.SPU和SKU数据结构3.1.SPU表SPU表： 1234567891011121314CREATE TABLE `tb_spu` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &apos;spu id&apos;, `title` varchar(255) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;标题&apos;, `sub_title` varchar(255) DEFAULT &apos;&apos; COMMENT &apos;子标题&apos;, `cid1` bigint(20) NOT NULL COMMENT &apos;1级类目id&apos;, `cid2` bigint(20) NOT NULL COMMENT &apos;2级类目id&apos;, `cid3` bigint(20) NOT NULL COMMENT &apos;3级类目id&apos;, `brand_id` bigint(20) NOT NULL COMMENT &apos;商品所属品牌id&apos;, `saleable` tinyint(1) NOT NULL DEFAULT &apos;1&apos; COMMENT &apos;是否上架，0下架，1上架&apos;, `valid` tinyint(1) NOT NULL DEFAULT &apos;1&apos; COMMENT &apos;是否有效，0已删除，1有效&apos;, `create_time` datetime DEFAULT NULL COMMENT &apos;添加时间&apos;, `last_update_time` datetime DEFAULT NULL COMMENT &apos;最后修改时间&apos;, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=208 DEFAULT CHARSET=utf8 COMMENT=&apos;spu表，该表描述的是一个抽象的商品，比如 iphone8&apos;; 我做了表的垂直拆分，将SPU的详情放到了另一张表：tb_spu_detai 123456789CREATE TABLE `tb_spu_detail` ( `spu_id` bigint(20) NOT NULL, `description` text COMMENT '商品描述信息', `generic_spec` varchar(10000) NOT NULL DEFAULT '' COMMENT '通用规格参数数据', `special_spec` varchar(1000) NOT NULL COMMENT '特有规格参数及可选值信息，json格式', `packing_list` varchar(3000) DEFAULT '' COMMENT '包装清单', `after_service` varchar(3000) DEFAULT '' COMMENT '售后服务', PRIMARY KEY (`spu_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 这张表中的数据都比较大，为了不影响主表的查询效率我们拆分出这张表。 需要注意的是这两个字段：generic_spec和special_spec。 前面讲过规格参数与商品分类绑定，一个分类下的所有SPU具有类似的规格参数。SPU下的SKU可能会有不同的规格参数信息，因此我们计划是这样： SPUDetail中保存通用的规格参数信息。 SKU中保存特有规格参数。 来看下我们的表如何存储这些信息。 3.1.1.generic_spec字段首先是generic_spec，其中保存通用规格参数信息的值，这里为了方便查询，使用了json格式： json结构，其中都是键值对： key：对应的规格参数的spec_param的id value：对应规格参数的值 3.1.2.special_spec字段以手机为例，品牌、操作系统等肯定是全局通用属性，内存、颜色等肯定是特有属性。 当你确定了一个SPU，比如小米的：红米4X 全局属性值都是固定的了： 12品牌：小米型号：红米4X 特有属性举例： 123颜色：[香槟金, 樱花粉, 磨砂黑]内存：[2G, 3G]机身存储：[16GB, 32GB] 颜色、内存、机身存储，作为SKU特有属性，key虽然一样，但是SPU下的每一个SKU，其值都不一样，所以值会有很多，形成数组。 我们在SPU中，会把特有属性的所有值都记录下来，形成一个数组： 刚好符合我们的结构，这样页面渲染就非常方便了。 3.2 sku表3.2.1 indexes字段在SPU表中，已经对特有规格参数及可选项进行了保存，结构如下： 123456789101112131415&#123; "4": [ "香槟金", "樱花粉", "磨砂黑" ], "12": [ "2GB", "3GB" ], "13": [ "16GB", "32GB" ]&#125; 这些特有属性如果排列组合，会产生12个不同的SKU，而不同的SKU，其属性就是上面备选项中的一个 比如： 红米4X，香槟金，2GB内存，16GB存储 红米4X，磨砂黑，2GB内存，32GB存储 你会发现，每一个属性值，对应于SPUoptions数组的一个选项，如果我们记录下角标，就是这样： 红米4X，0,0,0 红米4X，2,0,1 既然如此，我们是不是可以将不同角标串联起来，作为SPU下不同SKU的标示。这就是我们的indexes字段。 这个设计在商品详情页会特别有用： 3.2.2.own_spec字段看结构： 1&#123;"4":"香槟金","12":"2GB","13":"16GB"&#125; 保存的是特有属性的键值对。 SPU中保存的是可选项，但不确定具体的值，而SKU中的保存的就是具体的值。 3.3 导入图片信息现在商品表中虽然有数据，但是所有的图片信息都是无法访问的，我们需要把图片导入到虚拟机： 首先，把资料提供的数据上传到虚拟机下：/mage/static目录：在mage下创建static目录 然后，使用命令解压缩： 1unzip images.zip 修改Nginx配置，使nginx反向代理这些图片地址： 1vim /opt/nginx/conf/nginx.conf 修改成如下配置： 12345678910111213141516171819server &#123; listen 80; server_name image.mage.com; # 监听域名中带有group的，交给FastDFS模块处理 location ~/group([0-9])/ &#123; ngx_fastdfs_module; &#125; # 将其它图片代理指向本地的/leyou/static目录 location / &#123; root /mage/static/; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125;&#125; 不要忘记重新加载nginx配置 1nginx -s reload 4.商品查询4.1 效果 可以看出整体是一个table，然后有新增按钮。 4.2 后台接口 4.2.1.实体类在mage-item-interface工程中添加实体类： SPU 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119package com.mage.item.pojo;/* * @创建人: MaLingZhao * @创建时间: 2019/4/18 * @描述： */import javax.persistence.GeneratedValue;import javax.persistence.GenerationType;import javax.persistence.Id;import javax.persistence.Table;import java.util.Date;@Table(name = "tb_spu")public class Spu &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private Long brandId; private Long cid1;// 1级类目 private Long cid2;// 2级类目 private Long cid3;// 3级类目 private String title;// 标题 private String subTitle;// 子标题 private Boolean saleable;// 是否上架 private Boolean valid;// 是否有效，逻辑删除用 private Date createTime;// 创建时间 private Date lastUpdateTime;// 最后修改时间 public Long getId() &#123; return id; &#125; public void setId(Long id) &#123; this.id = id; &#125; public Long getBrandId() &#123; return brandId; &#125; public void setBrandId(Long brandId) &#123; this.brandId = brandId; &#125; public Long getCid1() &#123; return cid1; &#125; public void setCid1(Long cid1) &#123; this.cid1 = cid1; &#125; public Long getCid2() &#123; return cid2; &#125; public void setCid2(Long cid2) &#123; this.cid2 = cid2; &#125; public Long getCid3() &#123; return cid3; &#125; public void setCid3(Long cid3) &#123; this.cid3 = cid3; &#125; public String getTitle() &#123; return title; &#125; public void setTitle(String title) &#123; this.title = title; &#125; public String getSubTitle() &#123; return subTitle; &#125; public void setSubTitle(String subTitle) &#123; this.subTitle = subTitle; &#125; public Boolean getSaleable() &#123; return saleable; &#125; public void setSaleable(Boolean saleable) &#123; this.saleable = saleable; &#125; public Boolean getValid() &#123; return valid; &#125; public void setValid(Boolean valid) &#123; this.valid = valid; &#125; public Date getCreateTime() &#123; return createTime; &#125; public void setCreateTime(Date createTime) &#123; this.createTime = createTime; &#125; public Date getLastUpdateTime() &#123; return lastUpdateTime; &#125; public void setLastUpdateTime(Date lastUpdateTime) &#123; this.lastUpdateTime = lastUpdateTime; &#125;&#125; spu详情 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273package com.mage.item.pojo;/* * @创建人: MaLingZhao * @创建时间: 2019/4/18 * @描述： */import javax.persistence.Id;import javax.persistence.Table; @Table(name="tb_spu_detail") public class SpuDetail &#123; @Id private Long spuId;// 对应的SPU的id private String description;// 商品描述 private String specialSpec;// 商品特殊规格的名称及可选值模板 private String genericSpec;// 商品的全局规格属性 private String packingList;// 包装清单 private String afterService;// 售后服务 public Long getSpuId() &#123; return spuId; &#125; public void setSpuId(Long spuId) &#123; this.spuId = spuId; &#125; public String getDescription() &#123; return description; &#125; public void setDescription(String description) &#123; this.description = description; &#125; public String getSpecialSpec() &#123; return specialSpec; &#125; public void setSpecialSpec(String specialSpec) &#123; this.specialSpec = specialSpec; &#125; public String getGenericSpec() &#123; return genericSpec; &#125; public void setGenericSpec(String genericSpec) &#123; this.genericSpec = genericSpec; &#125; public String getPackingList() &#123; return packingList; &#125; public void setPackingList(String packingList) &#123; this.packingList = packingList; &#125; public String getAfterService() &#123; return afterService; &#125; public void setAfterService(String afterService) &#123; this.afterService = afterService; &#125; &#125; 4.2.2.mapper12345678910111213141516package com.mage.item.controller;/* * @创建人: MaLingZhao * @创建时间: 2019/4/18 * @描述： */import com.mage.item.pojo.Spu;import tk.mybatis.mapper.common.Mapper;public interface GoodsController extends Mapper&lt;Spu&gt; &#123;&#125; 4.2.3.controller先分析： 请求方式：GET 请求路径：/spu/page 请求参数： page：当前页 rows：每页大小 key：过滤条件 saleable：上架或下架 返回结果：商品SPU的分页信息。 要注意，页面展示的是商品分类和品牌名称，而数据库中保存的是id，怎么办？ 们可以新建一个类，继承SPU，并且拓展cname和bname属性，写到mage-item-interface 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.mage.item.controller;/* * @创建人: MaLingZhao * @创建时间: 2019/4/18 * @描述： */import com.mage.common.pojo.PageResult;import com.mage.item.bo.SpuBo;import com.mage.item.pojo.Spu;import com.mage.item.service.GoodsService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.http.ResponseEntity;import org.springframework.stereotype.Controller;import org.springframework.util.CollectionUtils;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestParam;import tk.mybatis.mapper.common.Mapper;@Controllerpublic class GoodsController&#123; @Autowired private GoodsService goodsService; @GetMapping("spu/page") public ResponseEntity&lt;PageResult&lt;SpuBo&gt;&gt; querySpuBoByPage( @RequestParam(value = "key", required = false)String key, @RequestParam(value = "saleable", required = false)Boolean saleable, @RequestParam(value = "page", defaultValue = "1")Integer page, @RequestParam(value = "rows", defaultValue = "5")Integer rows ) &#123; PageResult&lt;SpuBo&gt; pageResult = this.goodsService.querySpuBoByPage(key, saleable, page, rows); if (CollectionUtils.isEmpty(pageResult.getItems())) &#123; return ResponseEntity.notFound().build(); &#125; return ResponseEntity.ok(pageResult); &#125;&#125; 4.2.4 service123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081package com.mage.item.service;/* * @创建人: MaLingZhao * @创建时间: 2019/4/18 * @描述： */import com.github.pagehelper.PageHelper;import com.github.pagehelper.PageInfo;import com.mage.common.pojo.PageResult;import com.mage.item.bo.SpuBo;import com.mage.item.mapper.BrandMapper;import com.mage.item.mapper.SpuMapper;import com.mage.item.pojo.Spu;import org.apache.commons.lang.StringUtils;import org.springframework.beans.BeanUtils;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import tk.mybatis.mapper.entity.Example;import java.util.ArrayList;import java.util.Arrays;import java.util.List;@Servicepublic class GoodsService &#123; @Autowired private SpuMapper spuMapper; @Autowired private CategoryService categoryService; @Autowired private BrandMapper brandMapper; public PageResult&lt;SpuBo&gt; querySpuBoByPage(String key, Boolean saleable, Integer page, Integer rows) &#123; Example example = new Example(Spu.class); Example.Criteria criteria = example.createCriteria(); // 搜索条件 if (StringUtils.isNotBlank(key)) &#123; criteria.andLike("title", "%" + key + "%"); &#125; if (saleable != null) &#123; criteria.andEqualTo("saleable", saleable); &#125; // 分页条件 PageHelper.startPage(page, rows); // 执行查询 List&lt;Spu&gt; spus = this.spuMapper.selectByExample(example); PageInfo&lt;Spu&gt; pageInfo = new PageInfo&lt;&gt;(spus); List&lt;SpuBo&gt; spuBos = new ArrayList&lt;&gt;(); spus.forEach(spu-&gt;&#123; SpuBo spuBo = new SpuBo(); // copy共同属性的值到新的对象 BeanUtils.copyProperties(spu, spuBo); // 查询分类名称 List&lt;String&gt; names = this.categoryService.queryNamesByIds(Arrays.asList(spu.getCid1(), spu.getCid2(), spu.getCid3())); spuBo.setCname(StringUtils.join(names, "/")); // 查询品牌的名称 spuBo.setBname(this.brandMapper.selectByPrimaryKey(spu.getBrandId()).getName()); spuBos.add(spuBo); &#125;); return new PageResult&lt;&gt;(pageInfo.getTotal(), spuBos); &#125; &#125; 4.2.5.Category中拓展查询名称的功能页面需要商品的分类名称需要在这里查询，因此要额外提供查询分类名称的功能， 在CategoryService中添加功能： 123456789public List&lt;String&gt; queryNamesByIds(List&lt;Long&gt; ids) &#123; List&lt;Category&gt; list = this.categoryMapper.selectByIdList(ids); List&lt;String&gt; names = new ArrayList&lt;&gt;(); for (Category category : list) &#123; names.add(category.getName()); &#125; return names; // return list.stream().map(category -&gt; category.getName()).collect(Collectors.toList());&#125; mapper的selectByIdList方法是来自于通用mapper。不过需要我们在mapper上继承一个通用mapper接口： 123public interface CategoryMapper extends Mapper&lt;Category&gt;, SelectByIdListMapper&lt;Category, Long&gt; &#123; &#125; 4.3 测试 11 搭建前端工程1 导入已经写好的项目 2 安装live-server安装，使用npm命令即可，这里建议全局安装，以后任意位置可用 1npm install -g live-server 运行时，直接输入命令： 1live-server 另外，你可以在运行命令后，跟上一些参数以配置： --port=NUMBER - 选择要使用的端口，默认值：PORT env var或8080 --host=ADDRESS - 选择要绑定的主机地址，默认值：IP env var或0.0.0.0（“任意地址”） --no-browser - 禁止自动Web浏览器启动 --browser=BROWSER - 指定使用浏览器而不是系统默认值 --quiet | -q - 禁止记录 --verbose | -V - 更多日志记录（记录所有请求，显示所有侦听的IPv4接口等） --open=PATH - 启动浏览器到PATH而不是服务器root --watch=PATH - 用逗号分隔的路径来专门监视变化（默认值：观看所有内容） --ignore=PATH- 要忽略的逗号分隔的路径字符串（anymatch -compatible definition） --ignorePattern=RGXP-文件的正则表达式忽略（即.*\.jade）（不推荐使用赞成--ignore） --middleware=PATH - 导出要添加的中间件功能的.js文件的路径; 可以是没有路径的名称，也可以是引用middleware文件夹中捆绑的中间件的扩展名 --entry-file=PATH - 提供此文件（服务器根目录）代替丢失的文件（对单页应用程序有用） --mount=ROUTE:PATH - 在定义的路线下提供路径内容（可能有多个定义） --spa - 将请求从/ abc转换为/＃/ abc（方便单页应用） --wait=MILLISECONDS - （默认100ms）等待所有更改，然后重新加载 --htpasswd=PATH - 启用期待位于PATH的htpasswd文件的http-auth --cors - 为任何来源启用CORS（反映请求源，支持凭证的请求） --https=PATH - 到HTTPS配置模块的路径 --proxy=ROUTE:URL - 代理ROUTE到URL的所有请求 --help | -h - 显示简洁的使用提示并退出 --version | -v - 显示版本并退出 3 进行测试我们进入leyou-portal目录，输入命令： 1live-server --port=9002 4 配置nginx 直接访问域名 nginx的配置文件 1234567891011121314server &#123; listen 80; server_name www.leyou.com; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Server $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; location / &#123; proxy_pass http://127.0.0.1:9002; proxy_connect_timeout 600; proxy_read_timeout 600; &#125;&#125; 2 hosts文件添加域名的映射1127.0.0.1 www.mage.com 3 重启nginxnginx -s reload 4 测试 关于马哥商城的下一章节详解 ElasticSearch]]></content>
      <categories>
        <category>javaEE</category>
      </categories>
      <tags>
        <tag>项目实战</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MallAnalysisPlatform]]></title>
    <url>%2Fblog4%2F2019%2F10%2F16%2FMallAnalysisPlatform%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[DeepLearning]]></title>
    <url>%2Fblog4%2F2019%2F10%2F16%2FDeepLearning%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[springcloud]]></title>
    <url>%2Fblog4%2F2019%2F10%2F14%2Fspringcloud%2F</url>
    <content type="text"><![CDATA[springcloud学习总结1 eureka1.1 基础搭建搭建环境的注意的问题 1 application.yml文件配置请求参数的时候注意的问题 @Requestparam请求的路径是http://localhost/consumer/user?id=1 注意 RestTemplate的使用 如何使用实现服务的远程调用 @GetMapping @ResponseBody public User queryUserById(@RequestParam(“id”) Integer id) { User user= this.restTemplate.getForObject(“http://localhost:8081/user/&quot;+id, User.class); return user; }调用远程的服务 @SpringBootApplicationpublic class MlzServiceConsumerApplication { @Bean public RestTemplate restTemplate() { return new RestTemplate(); } public static void main(String[] args) { SpringApplication.run(MlzServiceConsumerApplication.class, args); }构建RestTemplate 两个工程 服务的提供方 服务的消费方服务注册与发现 如何实现自动注册与发现 如何实现统一的配置 Eureka服务注册中心的配置构建eurekaserver spring脚手架构建注意到springcloud的版本 Finchley.SR2springboot的版本2.0.6.RELEASE 引入springcloud组件的启动器 覆盖默认配置 在引导类上添加注解，开启相关的组件 spring: application: name: mlz-eureka 微服务的名称注入到Eureka容器 eureka: client: service-url: eureka: client: service-url: defaultZone: htp://localhost:${server.port}/eureka添加注解@EnableEurekaServer status 计算机名+xxx 服务端 eureka-server 有了 eureka-client 呢 服务的提供方注册为service-provider添加springcloud版本号配置application.yml添加spring的应用名称配置eureka-client eureka: client: service-url: defaultZone: http://localhost:10086/eureka 添加注解@EnableDiscoveryClient刷新eureka的界面发现服务提供方的服务添加依赖3步 1 2 3 注册中心 服务的提供方 服务的注册方 完毕心跳机制 renew 看看服务的提供方是否还活着用户端拉取服务列表 确认这个服务的状态之后 对这个服务调用 当我们使用了eureka之后 我们就要想如何才能得到我们拉取的服务 在我们的服务的消费端 1234567891011121314151617181920212223242526272829303132@Controller@RequestMapping("consumer/user")public class UserController &#123; @Autowired private RestTemplate restTemplate; @Autowired private DiscoveryClient discoveryClient; @GetMapping @ResponseBody public User queryUserById(@RequestParam("id") Integer id) &#123; List&lt;ServiceInstance&gt; instances = discoveryClient.getInstances("service-provider"); ServiceInstance instance = instances.get(0); User user= this.restTemplate.getForObject("http://"+instance.getHost()+":"+instance.getPort()+"/user/"+id, User.class); return user; &#125;&#125; 主程序 1添加允许eureka的客户端的注解 1.2 eureka的高可用10086 10087 两个eureka 相互注册 程序并行模拟 10086 10087 互换操作 修改配置文件applicaiton.yml 两个eureka的操作相互注册 1234567891011121314151617181920212223242526server: port: 10087spring: application: name: mlz-eurekaeureka: client: service-url: defaultZone: http://localhost:10086/eurekaserver: port: 10086spring: application: name: mlz-eurekaeureka: client: service-url: defaultZone: http://localhost:10087/eureka 内容的回顾 架构的演变传统架构—》》 水平拆分 web service dao—》》 垂直拆分（最早的分布式）—》》 soa（面向服务dubbo）—》》微服务（springcloud）—》》服务网格（谷歌） 耦合度该高 无法拆分 维护简单 水平 一个tomcat垂直拆分 多个tomcatdubbo 服务注册中心微服务 springcloud 远程调用技术： rpc http rpc协议： 自定义数据格式，一般限定技术 传速速度快 效率高（基于TCP传输的） dubbo http协议： 统一的数据格式，不限定技术 rest接口 tcp springcloud基于http一些 springcloud 微服务架构的解决方案 很多组件的集合 eureka 注册中心 服务的注册与发现 zuul 网关组件 路由请求 过滤器 具体到哪个个网关 ribbon： 负载均衡组件 服务间的相互调用 hystrix： 熔断组件 电路的保险丝 feign： 远程调用组件（ribbon）负载均衡 2 Ribbon负载均衡的使用2.1 步骤 eureka的依赖中本身就有ribbon的依赖 修改provider的yml文件 改成端口是8082 这样的话两个服务提供方 但是他的引导类在RestTemplate上写 1234567891011121314151617181920@SpringBootApplication@EnableDiscoveryClientpublic class MlzServiceConsumerApplication &#123; @Bean @LoadBalanced //开启负载均衡 public RestTemplate restTemplate() &#123; return new RestTemplate(); &#125; public static void main(String[] args) &#123; SpringApplication.run(MlzServiceConsumerApplication.class, args); &#125;&#125; Url的调用的方式 12345678910111213141516171819202122232425262728293031323334@Controller@RequestMapping("consumer/user")public class UserController &#123; @Autowired private RestTemplate restTemplate;// @Autowired// private DiscoveryClient discoveryClient; @GetMapping @ResponseBody public User queryUserById(@RequestParam("id") Integer id) &#123;// List&lt;ServiceInstance&gt; instances = discoveryClient.getInstances("service-provider");//// ServiceInstance instance = instances.get(0); // User user= this.restTemplate.getForObject("http://"+instance.getHost()+":"+instance.getPort()+"/user/"+id, User.class); User user= this.restTemplate.getForObject("http://service-provider/user/"+id, User.class); return user; &#125;&#125; 测试负载均衡算法 123456789101112131415161718192021@SpringBootTest@RunWith(SpringRunner.class)public class RibbonLoadBalanceTest &#123; @Autowired private RibbonLoadBalancerClient client; @Test public void test() &#123; for (int i = 0; i &lt;50 ; i++) &#123; ServiceInstance instance=this.client.choose("service-provider"); System.out.println(instance.getHost()+":"+instance.getPort()); &#125; &#125;&#125; 负载均衡算法的策略 修改 改成随机的负载均衡策略 1234service-provider: ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule 3 Hystrix3.1 理论feign 远程调用 网关 微服务的前面 Hystrix微服务的 spring的组件大部分来自于netflix 延迟和容错库 雪崩问题 复杂的调用联络 一个微服务卡住 卡到一定的程度 服务器的资源耗尽，其他的服务就不能用了 触发服务的降级 优先保证核心服务 不会阻塞 3.2 实践服务的调用方添加hystrix的依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;&lt;/dependency&gt; 熔断器的使用方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980package com.mlz.service.controller;/* * @创建人: MaLingZhao * @创建时间: 2019/10/14 * @描述： */import com.mlz.service.pojo.User;import com.netflix.hystrix.contrib.javanica.annotation.DefaultProperties;import com.netflix.hystrix.contrib.javanica.annotation.HystrixCommand;import com.netflix.ribbon.proxy.annotation.Hystrix;import org.springframework.beans.factory.annotation.Autowired; import org.springframework.beans.factory.annotation.Autowired;import org.springframework.cloud.client.ServiceInstance;import org.springframework.cloud.client.discovery.DiscoveryClient;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.bind.annotation.ResponseBody;import org.springframework.web.client.RestTemplate;import java.util.List;@Controller@RequestMapping("consumer/user")@DefaultProperties(defaultFallback = "fallBackMethod") // 指定一个类的全局熔断方法public class UserController &#123; @Autowired private RestTemplate restTemplate; @Autowired private DiscoveryClient discoveryClient;// @GetMapping// @ResponseBody// public User queryUserById(@RequestParam("id") Integer id)// &#123;//// List&lt;ServiceInstance&gt; instances = discoveryClient.getInstances("service-provider");//////// ServiceInstance instance = instances.get(0);// // User user= this.restTemplate.getForObject("http://"+instance.getHost()+":"+instance.getPort()+"/user/"+id, User.class);// User user= this.restTemplate.getForObject("http://service-provider/user/"+id, User.class);//////// return user;// &#125; @GetMapping @ResponseBody @HystrixCommand // 标记该方法需要熔断 public String queryUserById(@RequestParam("id") Long id) &#123; String user = this.restTemplate.getForObject("http://service-provider/user/" + id, String.class); return user; &#125; /** * 熔断方法 * 返回值要和被熔断的方法的返回值一致 * 熔断方法不需要参数 * @return */ public String fallBackMethod()&#123; return "请求繁忙，请稍后再试！"; &#125;&#125; 主方法 123456789101112131415161718192021222324252627282930package com.mlz.service;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.circuitbreaker.EnableCircuitBreaker;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;import org.springframework.cloud.client.loadbalancer.LoadBalanced;import org.springframework.context.annotation.Bean;import org.springframework.web.client.RestTemplate;@SpringBootApplication@EnableDiscoveryClient@EnableCircuitBreaker //开启熔断器public class MlzServiceConsumerApplication &#123; @Bean @LoadBalanced //开启负载均衡 public RestTemplate restTemplate() &#123; return new RestTemplate(); &#125; public static void main(String[] args) &#123; SpringApplication.run(MlzServiceConsumerApplication.class, args); &#125;&#125; 3.3总结降级 引入hystrix的启动器， 熔断时间 默认的熔断时间是1s 在引导类添加了一个注解 @EnableCircuitBreaker @SpringcloudApplicaiton 定义熔断方法 局部（要和熔断器的方法的方返回值和参数列表一致） 全局 返回值和被熔断的方法一致（参数列表必须为空） HystrixCommond (fallbackMethod=” 局部的熔断方法名”)声明熔断方法 DefauleProperties(defaultFallbcak=”全局的熔断方法名”) 3.4 服务熔断理论熔断的原理很简单 发生短路能立刻熔断电路 三个 状态 Close Open 服务降级 Half Open 健康 关闭 否则继续打开 3.5总结 close 闭合状态 所有请求正常方法 open 打开状态 所有的请求无法访问，如果在一定的时间内失败的比例不小于50%，或者不少于20次 half open 半开状态，默认5s的休眠期，在休眠期所有的请求无法正常访问过了休眠期会进入半开状态放部分请求通过，如果请求是健康的， 5 springcloud的思维导图 https://github.com/malingzhao/tuchaung]]></content>
      <categories>
        <category>javaEE</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[马哥商城]]></title>
    <url>%2Fblog4%2F2019%2F10%2F13%2FmageMall%2F</url>
    <content type="text"><![CDATA[1 马哥商城项目介绍1.1 项目介绍 马哥商城是一个全品类的电商购物网站（B2C）。 用户可以在线购买商品、加入购物车、下单 可以评论已购买商品 管理员可以在后台管理商品的上下架、促销活动 管理员可以监控商品销售状况 客服可以在后台处理退款操作 希望未来3到5年可以支持千万用户的使用 2.2 系统架构1 架构图 2 架构解读整个马哥商城可以分为两部分：后台管理系统、前台门户系统。 后台管理： 后台系统主要包含以下功能： 商品管理，包括商品分类、品牌、商品规格等信息的管理 销售管理，包括订单统计、订单退款处理、促销活动生成等 用户管理，包括用户控制、冻结、解锁等 权限管理，整个网站的权限控制，采用JWT鉴权方案，对用户及API进行权限控制 统计，各种数据的统计分析展示 后台系统会采用前后端分离开发，而且整个后台管理系统会使用Vue.js框架搭建出单页应用（SPA）。 前台门户 前台门户面向的是客户，包含与客户交互的一切功能。例如： 搜索商品 加入购物车 下单 评价商品等等 前台系统我们会使用Thymeleaf模板引擎技术来完成页面开发。出于SEO优化的考虑，我们将不采用单页应用。 无论是前台还是后台系统，都共享相同的微服务集群，包括： 商品微服务：商品及商品分类、品牌、库存等的服务 搜索微服务：实现搜索功能 订单微服务：实现订单相关 购物车微服务：实现购物车相关功能 用户中心：用户的登录注册等功能 Eureka注册中心 Zuul网关服务 … 3.项目搭建3.1.技术选型前端技术： 基础的HTML、CSS、JavaScript（基于ES6标准） JQuery Vue.js 2.0以及基于Vue的框架：Vuetify（UI框架） 前端构建工具：WebPack 前端安装包工具：NPM Vue脚手架：Vue-cli Vue路由：vue-router ajax框架：axios 基于Vue的富文本框架：quill-editor 后端技术： 基础的SpringMVC、Spring 5.x和MyBatis3 Spring Boot 2.0.7版本 Spring Cloud 最新版 Finchley.SR2 Redis-4.0 RabbitMQ-3.4 Elasticsearch-6.3 nginx-1.14.2 FastDFS - 5.0.8 MyCat Thymeleaf mysql 5.6 3.2.开发环境 IDE：我们使用Idea 2017.3 版本 JDK：统一使用JDK1.8 项目构建：maven3.3.9以上版本即可（3.5.2） 版本控制工具：git 3.3.域名我们在开发的过程中，为了保证以后的生产、测试环境统一。尽量都采用域名来访问项目。 一级域名：www.leyou.com，leyou.com leyou.cn 二级域名：manage.leyou.com/item , api.leyou.com 我们可以通过switchhost工具来修改自己的host对应的地址，只要把这些域名指向127.0.0.1，那么跟你用localhost的效果是完全一样的。 switchhost可以去课前资料寻找。 3.3.域名我们在开发的过程中，为了保证以后的生产、测试环境统一。尽量都采用域名来访问项目。 一级域名：www.mage.com，mage.com l mage.cn 二级域名：manage.mage.com/item , api.mage.com 通过switchhost工具来修改自己的host对应的地址，只要把这些域名指向127.0.0.1，那么跟你用localhost的效果是完全一样的。 3.4创建父工程创建统一的maven父工程：，用来管理依赖及其版本 pom.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.mage.parent&lt;/groupId&gt; &lt;artifactId&gt;mageMall&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;name&gt;mageMall&lt;/name&gt;&lt;description&gt;Demo project for springBoot&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.7.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;spring-cloud.version&gt;Finchley.SR2&lt;/spring-cloud.version&gt; &lt;mybatis.starter.version&gt;1.3.2&lt;/mybatis.starter.version&gt; &lt;mapper.starter.version&gt;2.0.2&lt;/mapper.starter.version&gt; &lt;druid.starter.version&gt;1.1.9&lt;/druid.starter.version&gt; &lt;mysql.version&gt;5.1.32&lt;/mysql.version&gt; &lt;pageHelper.starter.version&gt;1.2.3&lt;/pageHelper.starter.version&gt; &lt;leyou.latest.version&gt;1.0.0-SNAPSHOT&lt;/leyou.latest.version&gt; &lt;fastDFS.client.version&gt;1.26.1-RELEASE&lt;/fastDFS.client.version&gt; &lt;/properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- springCloud --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-cloud.version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- mybatis启动器 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;$&#123;mybatis.starter.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 通用Mapper启动器 --&gt; &lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;$&#123;mapper.starter.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 分页助手启动器 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;$&#123;pageHelper.starter.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- mysql驱动 --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;$&#123;mysql.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!--FastDFS客户端--&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.tobato&lt;/groupId&gt; &lt;artifactId&gt;fastdfs-client&lt;/artifactId&gt; &lt;version&gt;$&#123;fastDFS.client.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; 3.5.创建EurekaServer3.5.1.创建工程我们的注册中心，起名为：mage-registry 选择新建module： mage-registry 聚合项目 选择目录在下面 3.5.2 导入依赖pom.xml 1234567891011121314151617181920212223&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;parent&gt; &lt;artifactId&gt;mageMall&lt;/artifactId&gt; &lt;groupId&gt;com.mage.parent&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.mage.common&lt;/groupId&gt; &lt;artifactId&gt;mage-registry&lt;/artifactId&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 3.5.3.编写启动类12345678910111213141516171819202122package com.mage;mageMall/* * @创建人: MaLingZhao * @创建时间: 2019/4/13 * @描述： *mageMall/import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;@SpringBootApplication@EnableEurekaServerpublic class MageRegistryApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(MageRegistryApplication.class, args); &#125;&#125; 3.5.4.配置文件1234567891011121314server: port: 10086spring: application: name: mage-registryeureka: client: service-url: defaultZone: http:/mageMall/127.0.0.1:$&#123;server.port&#125;/eureka register-with-eureka: false # 把自己注册到eureka服务列表 fetch-registry: false # 拉取eureka服务信息 server: enable-self-preservation: false # 关闭自我保护 eviction-interval-timer-in-ms: 5000 # 每隔5秒钟，进行一次服务列表的清理 3.5.5.项目的结构目前的架构 3.6.创建Zuul网关3.6.1.创建工程 3.6.2.添加依赖这里我们需要添加Zuul和EurekaClient的依赖： 1234567891011121314151617181920212223242526272829303132&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;parent&gt; &lt;artifactId&gt;mageMall&lt;/artifactId&gt; &lt;groupId&gt;com.mage.parent&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.mage.common&lt;/groupId&gt; &lt;artifactId&gt;mage-gateway&lt;/artifactId&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-zuul&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- springboot提供微服务检测接口，默认对外提供几个接口 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 3.6.3.编写启动类12345678910111213141516171819202122232425package com.mage;mageMall/* * @创建人: MaLingZhao * @创建时间: 2019/4/13 * @描述： *mageMall/import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;import org.springframework.cloud.netflix.zuul.EnableZuulProxy;@SpringBootApplication@EnableDiscoveryClient@EnableZuulProxypublic class MageGatewayApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(MageGatewayApplication.class, args); &#125;&#125; 3.6.4.配置文件123456789101112server: port: 10010spring: application: name: mage-gatewayeureka: client: registry-fetch-interval-seconds: 5 service-url: defaultZone: http:/mageMall/127.0.0.1:10086/eurekazuul: prefix: /api # 路由路径前缀 3.6.5.项目结构目前，mage下有两个子模块： mage-registry：服务的注册中心（EurekaServer） mage-gateway：服务网关（Zuul） 目前，服务的结构如图所示： 截止到这里，我们已经把基础服务搭建完毕，为了便于开发，统一配置中心（ConfigServer）我们留待以后添加。 3.7.创建商品微服务既然是一个全品类的电商购物平台，那么核心自然就是商品。因此我们要搭建的第一个服务，就是商品微服务。其中会包含对于商品相关的一系列内容的管理，包括： 商品分类管理 品牌管理 商品规格参数管理 商品管理 库存管理 3.7.1.微服务的结构因为与商品的品类相关，我们的工程命名为mage-item. 需要注意的是，我们的mage-item是一个微服务，那么将来肯定会有其它系统需要来调用服务中提供的接口，获取的接口数据，也需要对应的实体类来封装，因此肯定也会使用到接口中关联的实体类。 因此这里我们需要使用聚合工程，将要提供的接口及相关实体类放到独立子工程中，以后别人引用的时候，只需要知道坐标即可。 我们会在mage-item中创建两个子工程： leyou-mage-interface：主要是对外暴露的接口及相关实体类 leyou-mage-service：所有业务逻辑及内部使用接口 调用关系如图所示： 3.7.2.mage-itemmaven搭建 因为是聚合工程，所以把项目打包方式设置为pom 1234567891011121314151617&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;parent&gt; &lt;artifactId&gt;mageMall&lt;/artifactId&gt; &lt;groupId&gt;com.mage.parent&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.mage.item&lt;/groupId&gt; &lt;artifactId&gt;mage-item&lt;/artifactId&gt; &lt;packaging&gt;pom&lt;/packaging&gt;&lt;/project&gt; 3.7.3.mage-item-interface 3.7.4.mage-item-service搭建过程与上一个相同 3.7.5.添加依赖接下来我们给mage-item-service中添加依赖： 思考一下我们需要什么？ Eureka客户端 web启动器 mybatis启动器 通用mapper启动器 分页助手启动器 连接池，我们用默认的Hykira mysql驱动 千万不能忘了，我们自己也需要mage-item-interface中的实体类 这些依赖，我们在顶级父工程：mage中已经添加好了。所以直接引入即可： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;parent&gt; &lt;artifactId&gt;mage-item&lt;/artifactId&gt; &lt;groupId&gt;com.mage.item&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.mage.item&lt;/groupId&gt; &lt;artifactId&gt;mage-item-service&lt;/artifactId&gt; &lt;dependencies&gt; &lt;!-- web启动器 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- eureka客户端 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- mybatis的启动器 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 通用mapper启动器 --&gt; &lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 分页助手启动器 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- jdbc启动器 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- mysql驱动 --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.mage.item&lt;/groupId&gt; &lt;artifactId&gt;mage-item-interface&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;!-- springboot检测服务启动器 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; mage-item-interface中需要什么我们暂时不清楚，所以先不管。以后需要什么依赖，再引入。 3.7.7.编写启动和配置在整个mage-item工程中，只有leyou-item-service是需要启动的。因此在其中编写启动类即可： 12345678910111213141516171819202122232425262728package com.mage;mageMall/* * @创建人: MaLingZhao * @创建时间: 2019/4/13 * @描述： *mageMall/import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;@SpringBootApplication@EnableDiscoveryClientpublic class MageItemServiceApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(MageItemServiceApplication.class, args); &#125;&#125;@SpringBootApplication@EnableDiscoveryClientpublic class LeyouItemServiceApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(LeyouItemServiceApplication.class, args); &#125;&#125; 然后是全局属性文件： 12345678910111213141516171819server: port: 8081spring: application: name: item-service datasource: url: jdbc:mysql://localhost:3306/mage username: root password: root hikari: max-lifetime: 28830000 # 一个连接的生命时长（毫秒），超时而且没被使用则被释放（retired），缺省:30分钟，建议设置比数据库超时时长少30秒，参考MySQL wait_timeout参数（show variables like '%timeout%';） maximum-pool-size: 9 # 连接池中允许的最大连接数。缺省值：10；推荐的公式：((core_count * 2) + effective_spindle_count)eureka: client: service-url: defaultZone: http:/mageMall/127.0.0.1:10086/eureka instance: lease-renewal-interval-in-seconds: 5 # 5秒钟发送一次心跳 lease-expiration-duration-in-seconds: 10 # 10秒不发送就过期 3.8.添加商品微服务的路由规则既然商品微服务已经创建，接下来肯定要添加路由规则到Zuul中，我们不使用默认的路由规则。 修改leyou-gateway工程的application.yml配置文件： 1234zuul: prefix: /api # 路由路径前缀 routes: item-service: /item/** # 商品微服务的映射路径 3.9.启动测试我们分别启动：leyou-registry，leyou-gateway，leyou-item-service 4.ES6语法指南后端项目搭建完毕，接下来就是前端页面了。 需要了解ES6的语法标准 4.1.什么是ECMAScript？ECMAScript是浏览器脚本语言的规范，而各种我们熟知的js语言，如JavaScript则是规范的具体实现。 4.2.ECMAScript的快速发展2015年6月，ECMAScript 6，也就是 ECMAScript 2015 发布了。 并且从 ECMAScript 6 开始，开始采用年号来做版本。即 ECMAScript 2015，就是ECMAScript6。 它的目标，是使得 JavaScript 语言可以用来编写复杂的大型应用程序，成为企业级开发语言。 4.3.ES5和6的一些新特性常用练习 创建一个空的html页面： 123456789101112131415&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt; &lt;meta http-equiv="X-UA-Compatible" content="ie=edge"&gt; &lt;title&gt;Document&lt;/title&gt;&lt;/head&gt;&lt;script&gt;&lt;/script&gt;&lt;body&gt; &lt;/body&gt;&lt;/html&gt; 4.3.1 idea创建静态的web工程创建空的工程 mage-es6 添加静态moudle 4.3.2.let 和 const 命令1234567891011121314151617181920&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;script&gt; for (let i=0;i&lt;5;i++) &#123; console.log(i); &#125; mageMall//console.log("我在循环外:"+ i);&lt;/script&gt;&lt;/html&gt; 测试var和let的区别 var的变量在循环外仍然生效 const`声明的变量是常量，不能被修改 4.3.3.字符串扩展 新的APIES6为字符串扩展 includes()：返回布尔值，表示是否找到了参数字符串。 startsWith()：返回布尔值，表示参数字符串是否在原字符串的头部。 endsWith()：返回布尔值，表示参数字符串是否在原字符串的尾部。 字符串模板 ES6中提供了`来作为字符串模板标记。我们可以这么玩： 在两个`之间的部分都会被作为字符串的值，不管你任意换行，甚至加入js脚本 4.3.3.解构表达式 4.3.3.解构表达式 12345let arr = [1,2,3]我想获取其中的值，只能通过角标。ES6可以这样：const [x,y,z] = arr;/mageMall/ x，y，z将与arr中的每个位置对应来取值mageMall/mageMall/ 然后打印console.log(x,y,z); 对象结构 1234567891011121314例如有个person对象：const person = &#123; name:"jack", age:21, language: ['java','js','css']&#125;mageMall/mageMall/ 解构表达式获取值const &#123;name,age,language&#125; = person;mageMall/mageMall/ 打印console.log(name);console.log(age);console.log(language); 4.3.4.函数优化 函数参数默认值 在ES6以前，我们无法给一个函数参数设置默认值，只能采用变通写法： 1234567function add(a , b) &#123; mageMall/mageMall/ 判断b是否为空，为空就给默认值1 b = b || 1; return a + b; &#125; mageMall/mageMall/ 传一个参数 console.log(add(10)); 现在可以这么写 12345function add(a , b = 1) &#123; return a + b;&#125;mageMall/mageMall/ 传一个参数console.log(add(10)); 箭头函数 ES6中定义函数的简写方式： 一个参数时： 12345var print = function (obj) &#123; console.log(obj);&#125;mageMall/mageMall/ 简写为：var print2 = obj =&gt; console.log(obj); 多个参数： 123456mageMall/mageMall/ 两个参数的情况：var sum = function (a , b) &#123; return a + b;&#125;mageMall/mageMall/ 简写为：var sum2 = (a,b) =&gt; a+b; 代码不止一行，可以用{}括起来 123var sum3 = (a,b) =&gt; &#123; return a + b;&#125; 对象的函数属性简写 比如一个Person对象，里面有eat方法： 12345678910111213let person = &#123; name: "jack", mageMall/mageMall/ 以前： eat: function (food) &#123; console.log(this.name + "在吃" + food); &#125;, mageMall/mageMall/ 箭头函数版： eat2: food =&gt; console.log(person.name + "在吃" + food),mageMall/mageMall/ 这里拿不到this mageMall/mageMall/ 简写版： eat3(food)&#123; console.log(this.name + "在吃" + food); &#125;&#125; 箭头函数结合解构表达式 比如有一个函数： 123456789const person = &#123; name:"jack", age:21, language: ['java','js','css']&#125;function hello(person) &#123; console.log("hello," + person.name)&#125; 如果用箭头函数和解构表达式 1var hi = (&#123;name&#125;) =&gt; console.log("hello," + name); 4.3.5.map和reduce数组中新增了map和reduce方法。 map map()：接收一个函数，将原数组中的所有元素用这个函数处理后放入新数组返回。 举例：有一个字符串数组，我们希望转为int数组 12345let arr = ['1','20','-5','3'];console.log(arr)arr = arr.map(s =&gt; parseInt(s));console.log(arr) reduce reduce()：接收一个函数（必须）和一个初始值（可选）。 第一个参数（函数）接收两个参数： 第一个参数是上一次reduce处理的结果 第二个参数是数组中要处理的下一个元素 reduce()会从左到右依次把数组中的元素用reduce处理，并把处理的结果作为下次reduce的第一个参数。如果是第一次，会把前两个元素作为计算参数，或者把用户指定的初始值作为起始参数 举例： 1const arr = [1,20,-5,3] 没有初始值： 指定初始值： 4.3.6.对象扩展ES6给Object拓展了许多新的方法，如： keys(obj)：获取对象的所有key形成的数组 values(obj)：获取对象的所有value形成的数组 entries(obj)：获取对象的所有key和value形成的二维数组。格式：[[k1,v1],[k2,v2],...] assign(dest, …src) ：将多个src对象的值 拷贝到 dest中（浅拷贝）。 4.3.7.数组扩展ES6给数组新增了许多方法： find(callback)：数组实例的find方法，用于找出第一个符合条件的数组成员。它的参数是一个回调函数，所有数组成员依次执行该回调函数，直到找出第一个返回值为true的成员，然后返回该成员。如果没有符合条件的成员，则返回undefined。 findIndex(callback)：数组实例的findIndex方法的用法与find方法非常类似，返回第一个符合条件的数组成员的位置，如果所有成员都不符合条件，则返回-1。 includes(数组元素)：与find类似，如果匹配到元素，则返回true，代表找到了。 5 搭建后台管理前端5.1 导入已有资源利用已经准备好的资源 然后在Intellij idea中导入新的工程： 5.2 安装依赖package.json中依然定义了我们所需的一切依赖： npm install命令，即可安装这些依赖。 大概需要几分钟。 安装成功 5.3 运行输入命令：npm run dev或者npm start localhost:9001 5.4 目录结构config webpack的配置文件 dist 打包输出口 webpack：是一个现代 JavaScript 应用程序的静态模块打包器(module bundler)。并且提供了前端项目的热部署插件。 5.5 调用关系我们最主要理清index.html、main.js、App.vue之间的关系： index.html：html模板文件。定义了空的div，其id为app。 main.js：实例化vue对象，并且通过id选择器绑定到index.html的div中，因此main.js的内容都将在index.html的div中显示。main.js中使用了App组件，即App.vue，也就是说index.html中最终展现的是App.vue中的内容。index.html引用它之后，就拥有了vue的内容（包括组件、样式等），所以，main.js也是webpack打包的入口。 index.js：定义请求路径和组件的映射关系。相当于之前的&lt;vue-router&gt; App.vue中也没有内容，而是定义了vue-router的锚点：&lt;router-view&gt;,我们之前讲过，vue-router路由后的组件将会在锚点展示。 最终结论：一切路由后的内容都将通过App.vue在index.html中显示。 访问流程：用户在浏览器输入路径，例如：http://localhost:9001/#/item/brand –&gt;index.js(/item/brand路径对应pages/item/Brand.vue组件) –&gt; 该组件显示在App.vue的锚点位置 –&gt; main.js使用了App.vue组件，并把该组件渲染在index.html文件中（id为“app”的div中） 6.使用域名访问本地项目6.1.统一环境现在访问页面使用的是：http://localhost:9001 实际开发中，会有不同的环境： 开发环境：自己的电脑 测试环境：提供给测试人员使用的环境 预发布环境：数据是和生成环境的数据一致，运行最新的项目代码进去测试 生产环境：项目最终发布上线的环境 如果不同环境使用不同的ip去访问，可能会出现一些问题。为了保证所有环境的一致，我们会在各种环境下都使用域名来访问。 我们将使用以下域名： 主域名是：www.mage.com，mage.com 管理系统域名：mage.leyou.com 网关域名：api.mage.com … 最终，我们希望这些域名指向的是我们本机的某个端口。 那么，当我们在浏览器输入一个域名时，浏览器是如何找到对应服务的ip和端口的呢？ 6.2.域名解析一个域名一定会被解析为一个或多个ip。这一般会包含两步： 本地域名解析 浏览器会首先在本机的hosts文件中查找域名映射的IP地址，如果查找到就返回IP ，没找到则进行域名服务器解析，一般本地解析都会失败，因为默认这个文件是空的。 Windows下的hosts文件地址：C:/Windows/System32/drivers/etc/hosts Linux下的hosts文件所在路径： /etc/hosts 样式： 12# My hosts127.0.0.1 localhost 域名服务器解析 本地解析失败，才会进行域名服务器解析，域名服务器就是网络中的一台计算机，里面记录了所有注册备案的域名和ip映射关系，一般只要域名是正确的，并且备案通过，一定能找到。 6.3.解决域名解析问题可以伪造本地的hosts文件，实现对域名的解析。修改本地的host为： 12127.0.0.1 api.mage.com127.0.0.1 manage.mage.com 这样就实现了域名的关系映射了。 switchHost的使用 ping测试 ping api.mage.com 访问出现 原因：我们配置了项目访问的路径，虽然manage.leyou.com映射的ip也是127.0.0.1，但是webpack会验证host是否符合配置。 解决 index.js的修改 】 在webpack.dev.conf.js中取消host验证：`disableHostCheck: true 后面不要忘了加逗号 重新执行npm run dev，刷新浏览器： 成功 6.4.nginx解决端口问题域名问题解决了，但是现在要访问后台页面，还得自己加上端口： http:/mageMall/manage.mage.com:9001`。 这就不够优雅了。我们希望的是直接域名访问：http://manage.taotao.com。这种情况下端口默认是80，如何才能把请求转移到9001端口呢？ 这里就要用到反向代理工具：Nginx 6.4.1.什么是Nginx nginx可以作为web服务器，但更多的时候，我们把它作为网关，因为它具备网关必备的功能： 反向代理 负载均衡 动态路由 请求过滤 6.4.2.nginx作为web服务器Web服务器分2类： web应用服务器，如： tomcat resin jetty web服务器，如： Apache 服务器 Nginx IIS 区分：web服务器不能解析jsp等页面，只能处理js、css、html等静态资源。并发：web服务器的并发能力远高于web应用服务器。 6.4.3.nginx作为反向代理什么是反向代理？ 代理：通过客户机的配置，实现让一台服务器代理客户机，客户的所有请求都交给代理服务器处理。 反向代理：用一台服务器，代理真实服务器，用户访问时，不再是访问真实服务器，而是代理服务器。 nginx可以当做反向代理服务器来使用： 我们需要提前在nginx中配置好反向代理的规则，不同的请求，交给不同的真实服务器处理 当请求到达nginx，nginx会根据已经定义的规则进行请求的转发，从而实现路由功能 利用反向代理，就可以解决我们前面所说的端口问题，如图 6.4.4.安装和使用 安装 直接解压 即可安装 1.14 版本的nginx conf：配置目录 contrib：第三方依赖 html：默认的静态资源目录，类似于tomcat的webapps logs：日志目录 nginx.exe：启动程序。可双击运行，但不建议这么做。 反向代理配置 示例： nginx中的每个server就是一个反向代理配置，可以有多个server 1234567891011121314151617181920212223242526272829303132333435363738394041424344#user nobody;worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; gzip on; server &#123; listen 80; server_name manage.leyou.com; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Server $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; location mageMall/ &#123; proxy_pass http:/mageMall/127.0.0.1:9001; proxy_connect_timeout 600; proxy_read_timeout 600; &#125; &#125; server &#123; listen 80; server_name api.leyou.com; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Server $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; location mageMall/ &#123; proxy_pass http:/mageMall/127.0.0.1:10010; proxy_connect_timeout 600; proxy_read_timeout 600; &#125; &#125;&#125; 使用 start nginx.exe 启动 nginx.exe -s stop 停止 nginx.exe -s reload 重新加载 启动过程会闪烁一下，启动成功后，任务管理器中会有两个nginx进程： 6.5.测试 成功 直接输入api.mage.com 7 实现商品的分类查询商城的核心自然是商品，而商品多了以后，肯定要进行分类，并且不同的商品会有不同的品牌信息，我们需要依次去完成：商品分类、品牌、商品的开发。 7.1.导入数据 导入表 7.2实现功能在浏览器页面点击“分类管理”菜单： 根据这个路由路径到路由文件（src/route/index.js），可以定位到分类管理页面： 由路由文件知，页面是src/pages/item/Category.vue 商品分类使用了树状结构，而这种结构的组件vuetify并没有为我们提供，这里自定义了一个树状组件 7.2.1.url异步请求点击商品管理下的分类管理子菜单，在浏览器控制台可以看到： 页面中没有，只是发起了一条请求：http://api.leyou.com/api/item/category/list?pid=0 觉得很奇怪，明明是使用的相对路径：/item/category/list，讲道理发起的请求地址应该是： http://manage.leyou.com/item/category/list 但实际却是： http://api.leyou.com/api/item/category/list?pid=0 这是因为，有一个全局的配置文件，对所有的请求路径进行了约定： 路径是http://api.leyou.com，并且默认加上了/api的前缀，这恰好与我们的网关设置匹配，我们只需要把地址改成网关的地址即可,因为我们使用了nginx反向代理，这里可以写域名。 接下来，我们要做的事情就是编写后台接口，返回对应的数据即可。 7.2.2.实体类在mage-item-interface中添加category实体类： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package com.mage.item.pojo;mageMall/* * @创建人: MaLingZhao * @创建时间: 2019/4/15 * @描述： *mageMall/@Table(name="tb_category")public class Category &#123; @Id @GeneratedValue(strategy=GenerationType.IDENTITY) private Long id; private String name; private Long parentId; private Boolean isParent; mageMall/mageMall/ 注意isParent生成的getter和setter方法需要手动加上Is private Integer sort; public Long getId() &#123; return id; &#125; public void setId(Long id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Long getParentId() &#123; return parentId; &#125; public void setParentId(Long parentId) &#123; this.parentId = parentId; &#125; public Boolean getParent() &#123; return isParent; &#125; public void setParent(Boolean parent) &#123; isParent = parent; &#125; public Integer getSort() &#123; return sort; &#125; public void setSort(Integer sort) &#123; this.sort = sort; &#125;&#125; 需要注意的是，这里要用到jpa的注解，因此我们在leyou-item-iterface中添加jpa依赖 12345&lt;dependency&gt; &lt;groupId&gt;javax.persistence&lt;/groupId&gt; &lt;artifactId&gt;persistence-api&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt;&lt;/dependency&gt; 7.2.3.controller编写一个controller一般需要知道四个内容： 请求方式：决定我们用GetMapping还是PostMapping 请求路径：决定映射路径 请求参数：决定方法的参数 返回值结果：决定方法的返回值 在刚才页面发起的请求中，我们就能得到绝大多数信息： 请求方式：Get，插叙肯定是get请求 请求路径：/api/item/category/list。其中/api是网关前缀，/item是网关的路由映射，真实的路径应该是/category/list 请求参数：pid=0，根据tree组件的说明，应该是父节点的id，第一次查询为0，那就是查询一级类目 返回结果：？？ 根据前面tree组件的用法我们知道，返回的应该是json数组： 12345678910111213141516[ &#123; "id": 74, "name": "手机", "parentId": 0, "isParent": true, "sort": 2 &#125;, &#123; "id": 75, "name": "家用电器", "parentId": 0, "isParent": true, "sort": 3 &#125;] 对应的java类型可以是List集合，里面的元素就是类目对象了。也就是`List 添加Controller： controller代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package com.mage.item.controller;mageMall/* * @创建人: MaLingZhao * @创建时间: 2019/4/15 * @描述： *mageMall/import com.mage.item.pojo.Category;import com.mage.item.service.CategoryService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.http.HttpStatus;import org.springframework.http.ResponseEntity;import org.springframework.stereotype.Controller;import org.springframework.util.CollectionUtils;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestParam;import java.util.Collections;import java.util.List;@Controller@RequestMapping("category")public class CategoryController &#123; @Autowired private CategoryService categoryService; mageMall/* * 根据父节点id查询子节点 * *mageMall/ @GetMapping("list") public ResponseEntity&lt;List&lt;Category&gt;&gt; queryCategoriesByPid(@RequestParam(value = "pid", defaultValue = "0") Long pid) &#123; try &#123; if (pid == null || pid.longValue() &lt; 0) &#123; mageMall/mageMall/响应等于400相当于ResponseEntity.status(HttpStatus.BAD_REQUEST).build(); return ResponseEntity.badRequest().build(); &#125; List&lt;Category&gt; categories = this.categoryService.queryCategoriesByPid(pid); if (CollectionUtils.isEmpty(categories)) &#123; mageMall/mageMall/ 响应404 资源未找到 return ResponseEntity.notFound().build(); &#125; mageMall/mageMall/200 查询成功 return ResponseEntity.ok(categories); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; mageMall/mageMall/服务器内部错误 return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).build(); &#125;&#125; 7.2.4.service一般service层我们会定义接口和实现类，不过这里我们就偷懒一下，直接写实现类了： 123456789101112131415161718192021222324252627282930313233343536package com.mage.item.service;mageMall/* * @创建人: MaLingZhao * @创建时间: 2019/4/15 * @描述： *mageMall/import com.mage.item.mapper.CategoryMapper;import com.mage.item.pojo.Category;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import java.util.List;@Servicepublic class CategoryService &#123; mageMall/mageMall/ controller 调service service 调dao @Autowired private CategoryMapper categoryMapper; mageMall/** * 根据parentId查询子类目 * @param pid * @return *mageMall/ public List&lt;Category&gt; queryCategoriesByPid(Long pid) &#123; Category record = new Category(); record.setParentId(pid); return this.categoryMapper.select(record); &#125; &#125; 7.2.5.mapper使用通用mapper来简化开发： 12public interface CategoryMapper extends Mapper&lt;Category&gt; &#123;&#125; 要注意，我们并没有在mapper接口上声明@Mapper注解，那么mybatis如何才能找到接口呢？ 我们在启动类上添加一个扫描包功能： 12345678910111213141516171819202122package com.mage;mageMall/* * @创建人: MaLingZhao * @创建时间: 2019/4/13 * @描述： *mageMall/import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;import tk.mybatis.spring.annotation.MapperScan;@SpringBootApplication@EnableDiscoveryClient@MapperScan("com.mage.item.mapper") mageMall/mageMall/ mapper接口的包扫描public class MageItemServiceApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(MageItemServiceApplication.class, args); &#125;&#125; 7.2.6.启动并测试我们不经过网关，直接访问：http://localhost:8081/category/list 访问成功 8 跨域问题跨域：浏览器对于javascript的同源策略的限制 。 以下情况都属于跨域： 跨域原因说明 示例 域名不同 www.jd.com 与 www.taobao.com 域名相同，端口不同 www.jd.com:8080 与 www.jd.com:8081 二级域名不同 item.jd.com 与 miaosha.jd.com 如果域名和端口都相同，但是请求路径不同，不属于跨域，如： www.jd.com/item www.jd.com/goods http和https也属于跨域 而我们刚才是从manage.mage.com去访问api.mage.com，这属于二级域名不同，跨域了。 8.1.为什么有跨域问题？跨域不一定都会有跨域问题。 因为跨域问题是浏览器对于ajax请求的一种安全限制：一个页面发起的ajax请求，只能是与当前页域名相同的路径，这能有效的阻止跨站攻击。 因此：跨域问题 是针对ajax的一种限制。 但是这却给我们的开发带来了不便，而且在实际生产环境中，肯定会有很多台服务器之间交互，地址和端口都可能不同，怎么办？ 8.2.解决跨域问题的方案目前比较常用的跨域解决方案有3种： Jsonp 最早的解决方案，利用script标签可以跨域的原理实现。 限制： 需要服务的支持 只能发起GET请求 nginx反向代理 思路是：利用nginx把跨域反向代理为不跨域，支持各种请求方式 缺点：需要在nginx进行额外配置，语义不清晰 CORS 规范化的跨域请求解决方案，安全可靠。 优势： 在服务端进行控制是否允许跨域，可自定义规则 支持各种请求方式 缺点： 会产生额外的请求 我们这里会采用cors的跨域方案。 8.3.cors解决跨域8.3.1.什么是corsCORS是一个W3C标准，全称是”跨域资源共享”（Cross-origin resource sharing）。 它允许浏览器向跨源服务器，发出XMLHttpRequest请求，从而克服了AJAX只能同源使用的限制。 CORS需要浏览器和服务器同时支持。目前，所有浏览器都支持该功能，IE浏览器不能低于IE10。 浏览器端： 目前，所有浏览器都支持该功能（IE10以下不行）。整个CORS通信过程，都是浏览器自动完成，不需要用户参与。 服务端： CORS通信与AJAX没有任何差别，因此你不需要改变以前的业务逻辑。只不过，浏览器会在请求中携带一些头信息，我们需要以此判断是否允许其跨域，然后在响应头中加入一些信息即可。这一般通过过滤器完成即可。 8.3.2.原理有点复杂浏览器将ajax的请求分成两类 8.3.2.1 简单请求要同时满足以下两大条件，就属于简单请求。： （1) 请求方法是以下三种方法之一： HEAD GET POST （2）HTTP的头信息不超出以下几种字段： Accept Accept-Language Content-Language Last-Event-ID Content-Type：只限于三个值application/x-www-form-urlencoded、multipart/form-data、text/plain 当浏览器发现ajax发现是简单请求的时候，会在前面加一个字段Origin Origin中会指出当前请求属于哪个域（协议+域名+端口）。服务会根据这个值决定是否允许其跨域。 如果服务器允许跨域，需要在返回的响应头中携带下面信息： 有关cookie 要想操作cookie，需要满足3个条件： 服务的响应头中需要携带Access-Control-Allow-Credentials并且为true。 浏览器发起ajax需要指定withCredentials 为true 响应头中的Access-Control-Allow-Origin一定不能为*，必须是指定的域名 8.3.2.2 特殊请求一个“预检”请求的样板： 12345678OPTIONS /cors HTTP/1.1Origin: http://manage.leyou.comAccess-Control-Request-Method: PUTAccess-Control-Request-Headers: X-Custom-HeaderHost: api.leyou.comAccept-Language: en-USConnection: keep-aliveUser-Agent: Mozilla/5.0... 与简单请求相比，除了Origin以外，多了两个头： Access-Control-Request-Method：接下来会用到的请求方式，比如PUT Access-Control-Request-Headers：会额外用到的头信息 预检请求的响应 服务的收到预检请求，如果许可跨域，会发出响应： 1234567891011121314HTTP/1.1 200 OKDate: Mon, 01 Dec 2008 01:15:39 GMTServer: Apache/2.0.61 (Unix)Access-Control-Allow-Origin: http://manage.leyou.comAccess-Control-Allow-Credentials: trueAccess-Control-Allow-Methods: GET, POST, PUTAccess-Control-Allow-Headers: X-Custom-HeaderAccess-Control-Max-Age: 1728000Content-Type: text/html; charset=utf-8Content-Encoding: gzipContent-Length: 0Keep-Alive: timeout=2, max=100Connection: Keep-AliveContent-Type: text/plain 除了Access-Control-Allow-Origin和Access-Control-Allow-Credentials以外，这里又额外多出3个头： Access-Control-Allow-Methods：允许访问的方式 Access-Control-Allow-Headers：允许携带的头 Access-Control-Max-Age：本次许可的有效时长，单位是秒，过期之前的ajax请求就无需再次进行预检了 如果浏览器得到上述响应，则认定为可以跨域，后续就跟简单请求的处理是一样的了。 8.3.3.3 实现非常简单虽然原理比较复杂，但是前面说过： 浏览器端都有浏览器自动完成，我们无需操心 服务端可以通过拦截器统一实现，不必每次都去进行跨域判定的编写。 事实上，SpringMVC已经帮我们写好了CORS的跨域过滤器：CorsFilter ,内部已经实现了刚才所讲的判定逻辑，我们直接用就好了。 在mage-gateway中编写一个配置类，并且注册CorsFilter： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.mage.config;mageMall/* * @创建人: MaLingZhao * @创建时间: 2019/10/14 * @描述： *mageMall/import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.web.cors.CorsConfiguration;import org.springframework.web.cors.CorsConfigurationSource;import org.springframework.web.cors.UrlBasedCorsConfigurationSource;import org.springframework.web.filter.CorsFilter;@Configurationpublic class MageCorsConfig &#123; @Bean public CorsFilter corsFilter() &#123; mageMall/mageMall/1.添加CORS配置信息 CorsConfiguration config = new CorsConfiguration(); mageMall/mageMall/1) 允许的域,不要写*，否则cookie就无法使用了 config.addAllowedOrigin("http://manage.mage.com"); mageMall/mageMall/2) 是否发送Cookie信息 config.setAllowCredentials(true); mageMall/mageMall/3) 允许的请求方式 config.addAllowedMethod("OPTIONS"); config.addAllowedMethod("HEAD"); config.addAllowedMethod("GET"); config.addAllowedMethod("PUT"); config.addAllowedMethod("POST"); config.addAllowedMethod("DELETE"); config.addAllowedMethod("PATCH"); mageMall/mageMall/ 4）允许的头信息 config.addAllowedHeader("*"); mageMall/mageMall/2.添加映射路径，我们拦截一切请求 UrlBasedCorsConfigurationSource configSource = new UrlBasedCorsConfigurationSource(); configSource.registerCorsConfiguration("mageMall/**", config); mageMall/mageMall/3.返回新的CorsFilter. return new CorsFilter( configSource); &#125;&#125; 结构 重启测试 访问 出现品牌 出现分类的页面 9 fastDFS9.1 restclient chrome 测试插件的安装 9.2 fastDFS文件系统 分布式文件系统（Distributed File System）是指文件系统管理的物理存储资源不一定直接连接在本地节点上，而是通过计算机网络与节点相连。 通俗来讲： 传统文件系统管理的文件就存储在本机。 分布式文件系统管理的文件存储在很多机器，这些机器通过网络连接，要被统一管理。无论是上传或者访问文件，都需要通过管理中心来访问 9.2.什么是FastDFSFastDFS是由淘宝的余庆先生所开发的一个轻量级、高性能的开源分布式文件系统。用纯C语言开发，功能丰富： 文件存储 文件同步 文件访问（上传、下载） 存取负载均衡 在线扩容 适合有大容量存储需求的应用或系统。同类的分布式文件系统有谷歌的GFS、HDFS（Hadoop）、TFS（淘宝）等。 9.3 配置nginx的rewrite重定向Nginx提供了rewrite指令，用于对地址进行重写，语法规则： 1rewrite "用来匹配路径的正则" 重写后的路径 [指令]; 我们的案例： 1234567891011121314151617181920212223server &#123; listen 80; server_name api.leyou.com; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Server $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # 上传路径的映射 location /api/upload &#123; proxy_pass http:/mageMall/127.0.0.1:8082; proxy_connect_timeout 600; proxy_read_timeout 600; rewrite "^/api/(.*)$" mageMall/$1 break; &#125; location mageMall/ &#123; proxy_pass http:/mageMall/127.0.0.1:10010; proxy_connect_timeout 600; proxy_read_timeout 600; &#125; &#125; 首先，我们映射路径是/api/upload，而下面一个映射路径是 mageMall/ ，根据最长路径匹配原则，/api/upload优先级更高。也就是说，凡是以/api/upload开头的路径，都会被第一个配置处理 proxy_pass：反向代理，这次我们代理到8082端口，也就是upload-service服务 rewrite &quot;^/api/(.*)$&quot; mageMall/$1 break，路径重写： &quot;^/api/(.*)$&quot;：匹配路径的正则表达式，用了分组语法，把/api/以后的所有部分当做1组 mageMall/$1：重写的目标路径，这里用$1引用前面正则表达式匹配到的分组（组编号从1开始），即/api/后面的所有。这样新的路径就是除去/api/以外的所有，就达到了去除/api前缀的目的 break：指令，常用的有2个，分别是：last、break last：重写路径结束后，将得到的路径重新进行一次路径匹配 break：重写路径结束后，不再重新匹配路径。 我们这里不能选择last，否则以新的路径/upload/image来匹配，就不会被正确的匹配到8082端口了 修改完成，输入nginx -s reload命令重新加载配置。然后再次上传试试。 9.4 绕过zuul网关的配置123zuul: ignored-services: - upload-service # 忽略upload-service服务 图片上传是文件的传输，如果也经过Zuul网关的代理，文件就会经过多次网路传输，造成不必要的网络负担。在高并发时，可能导致网络阻塞，Zuul网关不可用。这样我们的整个系统就瘫痪了。 所以，我们上传文件的请求就不经过网关来处理了。 9.5 什么是分布式文件系统文件的上传 Client通过Tracker server查找可用的Storage server。 Tracker server向Client返回一台可用的Storage server的IP地址和端口号。 Client直接通过Tracker server返回的IP地址和端口与其中一台Storage server建立连接并进行文件上传。 上传完成，Storage server返回Client一个文件ID，文件上传结束。 文件的下载 Client通过Tracker server查找要下载文件所在的的Storage server。 Tracker server向Client返回包含指定文件的某个Storage server的IP地址和端口号。 Client直接通过Tracker server返回的IP地址和端口与其中一台Storage server建立连接并指定要下载文件。 下载文件成功。 9.6 上传图片的代码代码的组织结构 9.6.1 controller1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.mage.upload.controller;mageMall/* * @创建人: MaLingZhao * @创建时间: 2019/4/16 * @描述： *mageMall/import com.mage.upload.service.UploadService;import org.apache.commons.lang.StringUtils;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.http.HttpStatus;import org.springframework.http.ResponseEntity;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.PostMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.multipart.MultipartFile;@Controller@RequestMapping("upload")public class UploadController &#123; @Autowired private UploadService uploadService; mageMall/* * 图片上传 * *mageMall/ @PostMapping("image") public ResponseEntity&lt;String&gt; uploadImage(@RequestParam("file") MultipartFile file)&#123; String url = this.uploadService.upload(file); if (StringUtils.isBlank(url)) &#123; return ResponseEntity.badRequest().build(); &#125; return ResponseEntity.status(HttpStatus.CREATED).body(url); &#125;&#125; 9.6.2 service1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package com.mage.upload.service;mageMall/* * @创建人: MaLingZhao * @创建时间: 2019/4/16 * @描述： *mageMall/import com.github.tobato.fastdfs.domain.StorePath;import com.github.tobato.fastdfs.service.FastFileStorageClient;import org.apache.commons.lang.StringUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import org.springframework.web.multipart.MultipartFile;import javax.imageio.ImageIO;import java.awt.image.BufferedImage;import java.io.File;import java.io.IOException;import java.util.Arrays;import java.util.List;@Servicepublic class UploadService &#123; @Autowired private FastFileStorageClient storageClient; private static final List&lt;String&gt; CONTENT_TYPES = Arrays.asList("image/jpeg", "image/gif"); private static final Logger LOGGER = LoggerFactory.getLogger(UploadService.class); public String upload(MultipartFile file) &#123; String originalFilename = file.getOriginalFilename(); mageMall/mageMall/ 校验文件的类型 String contentType = file.getContentType(); if (!CONTENT_TYPES.contains(contentType))&#123; mageMall/mageMall/ 文件类型不合法，直接返回null LOGGER.info("文件类型不合法：&#123;&#125;", originalFilename); return null; &#125; try &#123; mageMall/mageMall/ 校验文件的内容 BufferedImage bufferedImage = ImageIO.read(file.getInputStream()); if (bufferedImage == null)&#123; LOGGER.info("文件内容不合法：&#123;&#125;", originalFilename); return null; &#125; mageMall/mageMall/ 保存到服务器 mageMall/mageMall/ file.transferTo(new File("C:\\leyou\\images\\" + originalFilename)); String ext = StringUtils.substringAfterLast(originalFilename, "."); StorePath storePath = this.storageClient.uploadFile(file.getInputStream(), file.getSize(), ext, null); mageMall/mageMall/ 生成url地址，返回 return "http://image.mage.com/" + storePath.getFullPath(); &#125; catch (IOException e) &#123; LOGGER.info("服务器内部错误：&#123;&#125;", originalFilename); e.printStackTrace(); &#125; return null; &#125; &#125; 9.6.3 config1MageCorsFilter解决Cors跨域问题 123456789101112131415161718192021222324252627282930313233343536373839404142package com.mage.upload.config;mageMall/* * @创建人: MaLingZhao * @创建时间: 2019/4/16 * @描述： *mageMall/import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.web.cors.CorsConfiguration;import org.springframework.web.cors.UrlBasedCorsConfigurationSource;import org.springframework.web.filter.CorsFilter;@Configurationpublic class MageCorsFilter &#123; @Bean public CorsFilter corsFilter() &#123; mageMall/mageMall/1.添加CORS配置信息 CorsConfiguration config = new CorsConfiguration(); mageMall/mageMall/1) 允许的域,不要写*，否则cookie就无法使用了 config.addAllowedOrigin("http://manage.mage.com"); mageMall/mageMall/3) 允许的请求方式 config.addAllowedMethod("OPTIONS"); config.addAllowedMethod("POST"); mageMall/mageMall/ 4）允许的头信息 config.addAllowedHeader("*"); mageMall/mageMall/2.添加映射路径，我们拦截一切请求 UrlBasedCorsConfigurationSource configSource = new UrlBasedCorsConfigurationSource(); configSource.registerCorsConfiguration("mageMall/**", config); mageMall/mageMall/3.返回新的CorsFilter. return new CorsFilter(configSource); &#125;&#125; 解决fdfs文件上传问题 123456789101112131415161718192021package com.mage.upload.config;mageMall/* * @创建人: MaLingZhao * @创建时间: 2019/4/16 * @描述： *mageMall/import com.github.tobato.fastdfs.FdfsClientConfig;import org.springframework.context.annotation.Configuration;import org.springframework.context.annotation.EnableMBeanExport;import org.springframework.context.annotation.Import;import org.springframework.jmx.support.RegistrationPolicy;@Configuration@Import(FdfsClientConfig.class)mageMall/mageMall/ 解决jmx重复注册bean的问题@EnableMBeanExport(registration = RegistrationPolicy.IGNORE_EXISTING)public class FastClientImporter &#123;&#125; 引导类 12345678910111213141516171819mageMall/* * @创建人: MaLingZhao * @创建时间: 2019/4/16 * @描述： *mageMall/import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;@SpringBootApplication@EnableDiscoveryClientpublic class MageUploadApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(MageUploadApplication.class); &#125;&#125; 测试服务fdfs服务器配置的代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package com.mage.upload.Test;mageMall/* * @创建人: MaLingZhao * @创建时间: 2019/10/16 * @描述： *mageMall/import com.github.tobato.fastdfs.domain.StorePath;import com.github.tobato.fastdfs.domain.ThumbImageConfig;import com.github.tobato.fastdfs.service.FastFileStorageClient;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.test.context.junit4.SpringRunner;import java.io.File;import java.io.FileInputStream;import java.io.FileNotFoundException;@SpringBootTest@RunWith(SpringRunner.class)public class FastDFSTest &#123; @Autowired private FastFileStorageClient storageClient; @Autowired private ThumbImageConfig thumbImageConfig; @Test public void testUpload() throws FileNotFoundException &#123; mageMall/mageMall/ 要上传的文件 File file = new File("G:\\javaEELearning\\image\\ma.jpg"); mageMall/mageMall/ 上传并保存图片，参数：1-上传的文件流 2-文件的大小 3-文件的后缀 4-可以不管他 StorePath storePath = this.storageClient.uploadFile( new FileInputStream(file), file.length(), "jpg", null); mageMall/mageMall/ 带分组的路径 System.out.println(storePath.getFullPath()); mageMall/mageMall/ 不带分组的路径 System.out.println(storePath.getPath()); &#125; @Test public void testUploadAndCreateThumb() throws FileNotFoundException &#123; File file = new File("G:\\javaEELearning\\image\\ma.jpg"); mageMall/mageMall/ 上传并且生成缩略图 StorePath storePath = this.storageClient.uploadImageAndCrtThumbImage( new FileInputStream(file), file.length(), "png", null); mageMall/mageMall/ 带分组的路径 System.out.println(storePath.getFullPath()); mageMall/mageMall/ 不带分组的路径 System.out.println(storePath.getPath()); mageMall/mageMall/ 获取缩略图路径 String path = thumbImageConfig.getThumbImagePath(storePath.getPath()); System.out.println(path); &#125;&#125; 9.7 商品分类的回显controller 12345678@GetMapping("bid/&#123;bid&#125;") public ResponseEntity&lt;List&lt;Category&gt;&gt; queryByBrandId(@PathVariable("bid") Long bid) &#123; List&lt;Category&gt; list = this.categoryService.queryByBrandId(bid); if (list == null || list.size() &lt; 1) &#123; return new ResponseEntity&lt;&gt;(HttpStatus.NOT_FOUND); &#125; return ResponseEntity.ok(list); &#125; service 123456public List&lt;Category&gt; queryByBrandId(Long bid) &#123; return this.categoryMapper.queryByBrandId(bid);&#125; mapper 123@Select("SELECT * FROM tb_category WHERE id IN " + "(SELECT category_id FROM tb_category_brand WHERE brand_id = #&#123;bid&#125;)") List&lt;Category&gt; queryByBrandId(Long bid);]]></content>
      <categories>
        <category>javaEE</category>
      </categories>
      <tags>
        <tag>项目实战</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[牛客网刷题]]></title>
    <url>%2Fblog4%2F2019%2F10%2F12%2Fnowcoder%2F</url>
    <content type="text"><![CDATA[1 Linux运维第一次练习Linux的进程有哪些 交互进程 批处理进程 守护进程 Linux的文件权限10位长度 - rw- r– r– 也就是说文件类型 普通文件 文件所有者进行读写 文件所属组只能读 非同组用户只能读 文件类型 + 属主权限 + 属组权限+ 其他用户权限 文件类型 d 目录 普通文件 l 链接文件 b 普通块设备文件 c字符设备文件 p pipe管道类型文件 s socket管道文件 属主权限 文件所有者 属组权限 文件所属组 其他用户权限 非同组用户权限 OGO Owner Group Owner mkidr -p mkdir -m 设置权限 压缩 compress 相当古老的unix档案压缩指令 后缀.Z uncompress 解压缩 Linux的压缩格式 .zip .gz .bz2 .tar.gz .tar.bz2 zip zip -r 到指定的目录 unzip gzip .gz gzip 源文件 gzip -c 源文件&gt;压缩文件 gzip -r 目录 不能压缩目录 压缩目录下的所有子文件 bzip2 源文件 不保留源文件 bzip2 -k 源文件 保留源文件 bzip2 -d 解压缩 -k 保留压缩文件 bunzip2 u-g-o 用于-用户组-其他用户 a+x 增加所有用户的执行权限 g+w rw- r– r– rwx rwx r-x 775 4-2-1 r w x 关机命令 half init 0 poweroff shutdown -h 时间最安全 重启命令 reboot init 6 shutdown -r 只要cpu没有满 性能的瓶颈就不在cpu这里 如何设置交换空间 内存&lt;2G 交换空间为内存两倍 内存大于2G 在内存的基础上加上2G 虚拟内存是交换空间 虚拟内存的速度比真实的内存的速度小 因此设置的越小越好 HTTp的3个端口80 8080 443（ssl加密） 主动连接20 被动连接21 Linux的文件权限10位长度 less光标上下移动 mysqldump的备份命令 mysqldump -p ip -u root -p DBNAME &gt; block.sql -h 主机 host tar zcvf z压缩 x解压缩 c 创建这个压缩包 v可视 hosts文件补充 扩充 Dns的功能 Linux 缺省 默认 bash 建立动态路由用到的文件 /etc./gateways www端口和ftp端口 linux下的mysqldump的备份命令 归档压缩的命令 Linux缺省的shell是 没有条件简历dns服务器，不想ip访问网站 存放系统需要的配置文件和子目录 /etc dev bin sbin opt usr home etc lost+found var 2 大数据第一次练习菲波那切数列的计算 12345678int fib(int n)&#123; if(n==0) return 1; else if(n==1) return 2; else return fib(n-1)+fib(n-2);&#125; 计算f(9) f(2)调用了3个次 f(1)调用了1次 f(3)=(f2)+f(1)+1 以此类推 得到最终的结果 109 HDFS 数据存储 DataNode Hive 数据仓库 ResourceManager为应用程序分配 Container 每个节点上的资源不是RM负责的 NM 管理节点的资源 Applicaitonmaster对应用进行管理 表的组成内容 行和列组成 行是一个记录 列是一个字段 三次握手 传输层的建立 在文件的索引节点中存放直接索引指针10个，一级和二级索引指针各1个。磁盘块大小为1KB，每个索引指针占4个字节。若某文件的索引节点已在内存中， 则把该文件偏移量（按字节编址）为12345和987654处所在的磁盘块读入内存，需访问的磁盘块个数分别是（ ） 明白一级索引二级索引 以及他们取数据的流程是什么样的 10240&lt; 12345&lt;256 *1024 一级索引 取数据 2 二级索引 取数据 3 2级索引 256256 kb = 256 **256/1024 64MB 12345 引用函数和普通函数的区别 SCAN调度（电梯调度) 算法得到的磁盘访问序列 假设磁头当前位于第99道，正在向磁道序号增加的方向移动。现有一个磁道访问请求序列为33，59，13，77，123，170，160，185， 采用SCAN调度(电梯调度)算法得到的磁道访问序列是( ) 电梯调度的思想：从移动臂当前位置开始沿着臂的移动方向去选择离当前移动臂最近的那个柱面的访问者，如果沿臂的移动方向无请求访问时，就改变臂的移动方向再选择。但在本题中，磁头正在向磁道序号增加的方向移动。首先，磁头选择与当前磁头所在磁道距离最近的请求作为首次服务的对象(123)，当磁头沿途相应访问请求序列直到达到一端末(123，160，170，185)，再反向移动响应另一端的访问请求(77，59，33，13)。 二叉树的前序遍历 中序遍历和后序遍历 Spark的Task的数量由什么决定 task是stage下的一个任务执行单元，一般来说，一个rdd中有多少个partition就有多少个task，因为一个task知识处理一个partition上的数据 task 是stage的下一个的执行单元 有多少partition就有多少个task spark广播变量 如果是broadcast广播变量的话，没记错应该是通过设置的吧。每个函数都可以调用，且是只读的，如果是cache好像是MEMORY_AND_DISK吧，也就是说会优先存放于内存，内存不够会溢写到磁盘上，我记得还有MERORY_ONLY模式，DISK_ONLY模式，好几种模式，但是不会存放在hdfs中 测得某个采用按需调页策略的计算机系统部分状态数据为：CPU利用率5%，用于交换空间的磁盘利用率95%，其他I/O设备利用率5%。 试问，这种情况下（ ）能提高CPU的利用率。 CPU使用率越高 说明在这个期间运行了多道程序 反之 越小 CPU一次读太多程序放入内存中，因此要降低多道程序的次数 交换空间利用率高 增大内存的容量 对以下各搜索树进行删除操作，哪些树在最坏情况下时间复杂度不超过O(log(n))?其中n为关键码的数量。 AVL树是最先发明的自平衡二叉查找树。在AVL树中任何节点的两个子树的高度最大差别为一，所以它也被称为高度平衡树。查找、插入和删除在平均和最坏情况下都是O(log n)。 B.伸展树(Splay Tree)，也叫分裂树，是一种二叉排序树，它能在O(log n)内完成插入、查找和删除操作。伸展树支持所有的二叉树操作。伸展树不保证最坏情况下的时间复杂度为O(logN)。伸展树的时间复杂度边界是均摊的。 C.红黑树(Red Black Tree) 是一种自平衡二叉查找树，它虽然是复杂的，但它的最坏情况运行时间也是非常良好的，并且在实践中是高效的: 它可以在O(log n)时间内做查找，插入和删除，这里的 是树中元素的数目。 D.在二叉查找树中查询元素的最优时间复杂度是O(logN)即在满二叉树的情况下,最坏时间复杂度是O(n)即除叶子节点外每个节点只有一个子节点, 搜索树进行删除操作 那些树在最坏的情况下复杂度不超过O(log(n)) AVL树 红黑树 10亿个unnsigned int的数中 如何判断一个给定的数是不是在其中 unsigned int 32进制 把10亿个数中的每一个用32位的二进制来表示。假设这10亿个数开始放在一个文件中。 然后将这10亿个数分成两类: 1.最高位为0 2.最高位为1 并将这两类分别写入到两个文件中，其中一个文件中数的个数&lt;=5亿，而另一个&gt;=5亿（这相当于折半了）；与要查找的数的最高位比较并接着进入相应的文件再查找 再然后把这个文件为又分成两类: 1.次最高位为0 2.次最高位为1 并将这两类分别写入到两个文件中，其中一个文件中数的个数&lt;=2.5亿，而另一个&gt;=2.5亿（这相当于折半了）； 与要查找的数的次最高位比较并接着进入相应的文件再查找。 描述一下hadoop的shuffle的过程 环形缓冲区的处理 key进行分区 按照partition的数量 ，分区之后进行排序 如果有combiner 会将分区的数据进行聚合，每个map任务产生多个spill文件，在map任务结束之前，多路归并算法讲这些spill文件归并成一个文件 reduce端的话 请求RM得到map的输出位置 复制数据输出的同时对map的输出结果进行归并，最后将整合的结果交给reudce处理 字母 空格 组成的字符串序列 序列中只包函字母，不包含空格的子序列成为一个单词 ，请输出最后一个序列中的单词的长度 3 java第一次练习封装 继承 多态 java的三大特性 passwd 修改密码 myfile rw-rw-rw- chmod o-w myfile chmod 664 myfile 6 6 4 6 rw 4 r 关系型数据库 简化查询而又不增加数据的存储空间的常用方法 视图 简化用户的操作 用户多种角度看待同一个数据 设置主键可以提高查询的性能 子查询和group by 合计字段和groupby 关于查询 子查询中不能出现order by select后只能有两种类型的表达 是 1 种是合计函数 一种是出现在group by子句后面的列名 三、进程与程序的主要区别： （1）程序是永存的；进程是暂时的，是程序在数据集上的一次执行，有创建有撤销，存在是暂时 （2）程序是静态的观念，进程是动态的观念 （3）进程具有并发性，而程序没有 （4）进程是竞争计算机资源的基本单位，程序不是 （5）进程和程序不是一一对应的： 一个程序可对应多个进程即多个进程可执行同一程序； 一个进程可以执行一个或几个程序 四、进程与程序的相同点：程序是构成进程的组成部分之一，一个进程存在的目的就是执行其所对应的程序，如果没有程序，进程就失去了其存在的意义。 一个栈的入栈顺序是A,B,C,D,E,则栈不可能的输出顺序是（ ） 进程和程序的区别是什么入栈的顺序 A，B，C，D，E想出栈顺序 堆栈讲究先进后出，后进先出。选项AD错。 选项B是a入栈，然后a出栈；b再入栈，b出栈……依此类推。 选项C是a先入栈，然后a出栈，b入栈然后b出栈，然后是cde入栈，再出栈变为edc 考虑方向 9/2 和 9/2.0的区别 9/2 =4 9/2.0= 4.5 坑爹 父类方法 构造函数 析构函数 的执行顺序 123456789101112131415161718192021222324252627282930#include &lt;stdio.h&gt;class CPerson&#123;public:virtual void Whoami() = 0;&#125;;class CStudent : public CPerson&#123;public:CStudent()&#123;printf("student is created!\r\n");&#125;virtual ~CStudent()&#123;printf("student is destroy!\r\n");&#125;virtual void Whoami()&#123;printf("student!\r\n");&#125;&#125;;class CTeacher : public CPerson&#123;public:CTeacher()&#123;printf("teacher is created!\r\n");&#125;virtual ~CTeacher()&#123;printf("teacher is destroy!\r\n");&#125;virtual void Whoami()&#123;printf("teacher!\r\n");&#125;&#125;;int main()&#123;CPerson *pLily = new CTeacher();pLily-&gt;Whoami();delete (CTeacher *)pLily;CPerson *pLucy = new CStudent();pLucy-&gt;Whoami();delete pLucy;return 0;&#125; 注意着两个位置 一个指针 一个非指针 先构造函数再内部方法 先父类后子类 纯虚函数 12含有纯虚函数的类不能被声明对像，这些类被称为抽象类继承抽象类的派生类可以被声明对像，但要在派生类中完全实现基类中所有的纯虚函数 进程和进程B在临界段上互斥，那么在xianchengA处于临界段的时候 它不能被进程B中断 XXX 1虚拟存储管理中的抖动(thrashing)现象是指页面置换(page replacement)时用于换页的时间远多于执行程序的时间 纯虚函数是指被表明为不具体实现的虚拟成员函数。它用于这样的情况： 定义一个基类时，会遇到无法定义基类中虚函数的具体实现，其实现依赖于不同的派生类。 含有纯虚函数的基类是不可以定义对象的。纯虚函数无实现部分，不能产生对象，所以含有虚函数的类是抽象类。 short a[200] sizeof(a)=2 垃圾回收 1.检测垃圾对象 2.回收垃圾对象所占用的堆空间 Map不继承Collection接口 java中的swap的分析 基本类型数值 引用类型指针 垃圾收集的 4 大数据第二次练习 什么是均值 什么是标准差 2 为表示某地区男性年龄与薪水的关系，最佳的方式是通过绘制（）来表达 年龄 薪水 年龄 是整数 既然是整数 我们无法回执连续图形 所以我们选择散点图 3 主成分分析（PCA）中各因子的关系是（） 互相独立的 4 说说项目 5数据库试题5.1事务的四大特性原子性 要么执行 要么不执行 隔离性 所有的操作全部执行 完以前其他的操作不能看到过程 一致性 十五前后 数据总额一致 持久性 一旦失误提交 对数据的改变是永久的 5.2 数据库隔离级别12345脏读 ：事务B读取事务A还没有提交的数据不可重复读：两次事务读的数据不一致幻读:事务A修改了数据，事务B也修改了数据，这时在事务A看来，明明修改了数据，咋不一样 5. 3mysql的两种存储引擎的区别（事务 锁的级别） 引擎 特性 MYISAM 不支持外键 表锁 插入数据的时候锁定整个表 查表的总行数的时候不需要 全表扫描 INNODB 支持外键 行锁 查表总行数时候，全表扫描 5.4 索引有B+和hash+索引 索引 hash hash索引 等待查询效率高，不能排序 不能进行范围查询 B+ 数据有序 范围查询 5.5 聚集索引和非聚集索引 索引 聚焦索引 数据按索引的顺序存储 中子节点存储的是真实的物理数据 非聚焦索引 5.6 索引的优缺点 什么时候使用索引什么时候不使用索引最大的优点 提高查询速度 缺点 更新数据的效率低 因为需要同时更新索引 对数据进行频繁的查询的时候建立索引。如果需要频繁的更改数据的话 不建议使用索引 5.7 InnoDB和MyIsam的区别 主索引的区别 Innodb的数据文件本身就是索引文件 ​ myisam的索引和数据是开的 二是辅助索引的区别 innodb的辅助索引data域存储相应记录主 5.8 索引的底层实现（B+树 为什么不使用红黑树 B树）重点 树 红黑树 增加 删除 红黑树会进行频繁的调整，来保证红黑树的性质 浪费时间 B树也就是B+树 B树的查询性能不稳定 查询结果高度不一致 每个节点爆粗执行真实数据的指针，相比B+树每一层 ，存储的元素更多显得更高一点 B+树 B+树相比较于另外两种书 先的更矮更宽查询的速度更宽 查询的层次更浅 5.9 B+树的实现个m阶的B+树具有如下几个特征：1.有k个子树的中间节点包含有k个元素（B树中是k-1个元素），每个元素不保存数据，只用来索引，所有数据都保存在叶子节点。2.所有的叶子结点中包含了全部元素的信息，及指向含这些元素记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。3.所有的中间节点元素都同时存在于子节点，在子节点元素中是最大（或最小）元素 5.10 为什么用B+Tree索引查找过程中就要产生磁盘I/O消耗,主要看IO次数，和磁盘存取原理有关。根据B-Tree的定义，可知检索一次最多需要访问h个节点。数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入局部性原理与磁盘预读 5.11 sql的优化1 sql尽量使用索引 而且查询要走索引 2 对sql语句优化 子查询变成left joinlimit 分布优化，先利用ID定位，再分页or条件优化，多个or条件可以用union all对结果进行合并（union all结果可能重复）不必要的排序where代替having,having 检索完所有记录，才进行过滤避免嵌套查询对多个字段进行等值查询时，联合索引 5.12 索引的最左前缀问题如果对三个字段建立联合索引，如果第二个字段没有使用索引，第三个字段也使用不到索引了 5.13 索引分类 索引失效条件 索引类型 普通索引 最基本的索引，没有任何限制 唯一索引 与”普通索引”类似，不同的就是：索引列的值必须唯一，但允许有空值。 主键索引 它是一种特殊的唯一索引，不允许有空值。 全文索引 针对较大的数据，生成全文索引很耗时好空间。 组合索引 为了更多的提高mysql效率可建立组合索引，遵循”最左前缀“原则 失效条件条件是or,如果还想让or条件生效，给or每个字段加个索引like查询，以%开发内部函数对索引列进行计算is null不会用，is not null 会用 5.14 数据库的主从复制 复制方式 概念 异步复制 默认异步复制 容易造成主数据库和从数据库不一致，一个数据库为Master 一个数据库为slave，通过binlog日志 ，slave两个线程，一个线程去读master binlog日志，写到自己的中继日志，一个线程解析日志，执行sql,master启动一个线程,给slave传递binlog日志 半同步复制 只有把master发送的binlog日志写到slave的中继日志，这时主库,才返回操作完成的反馈，性能有一定降低 并行操作 slave 多个线程去请求binlog日志 5.15 long_query如何解决设置参数，开启慢日志功能，得到耗时超过一定时间的sql 5.16 varchar和char的使用的场景 类型 varchar 字符长度经常变的 char 5.17数据库连接池的作用 维护一定数量的链接，减少创建连接的时间 更快响应时间 统一的管理 分库分表，主从复制，读写分离 数据库的三范式 5.18数据库三范式 级别 概念 1NF 属性不可分 2NF 非主键属性，完全依赖于主键属性 3NF 非主键属性无传递依赖 5.19 关系型数据库和非关系型数据库的区别优点 1、容易理解：二维表结构是非常贴近逻辑世界一个概念，关系模型相对网状、层次等其他模型来说更容易理解； 2、使用方便：通用的SQL语言使得操作关系型数据库非常方便；3、易于维护：丰富的完整性(实体完整性、参照完整性和用户定义的完整性)大大减低了数据冗余和数据不一致的概率；4、支持SQL，可用于复杂的查询。5.支持事务 缺点1、为了维护一致性所付出的巨大代价就是其读写性能比较差；2、固定的表结构；3、不支持高并发读写需求；4、不支持海量数据的高效率读写 非关系型数据库 1、使用键值对存储数据；2、分布式；优点无需经过sql层的解析，读写性能很高基于键值对，数据没有耦合性，容易扩展存储数据的格式：nosql的存储格式是key,value形式缺点不提供sql支持 5.20 数据库的三范式 级别 概念 1NF 属性不可分 2NF 非主键属性，完全依赖于主键属性 3NF 非主键属性无传递依赖 5.22 数据库中join的inner join outer join cross join1.以A，B两张表为例A left join B选出A的所有记录，B表中没有的以null 代替right join 同理 2.inner joinA,B的所有记录都选出，没有的记录以null代替 3.cross join (笛卡尔积)A中的每一条记录和B中的每一条记录生成一条记录例如A中有4条，B中有4条，cross join 就有16条记录 5.23 有哪些锁 select 怎么加锁 锁 概念 乐观锁 自己实现，通过版本号 悲观锁 共享锁，多个事务，只能读不能写，加 lock in share mode 排它锁 一个事务，只能写，for update 行锁 作用于数据行 表锁 作于用表 5.24 死锁怎么解决找到进程号kill掉 5.25 最左匹配原则最左匹配原则是针对索引的举例来说：两个字段（name,age）建立联合索引，如果where age=12这样的话，是没有利用到索引的，这里我们可以简单的理解为先是对name字段的值排序，然后对age的数据排序，如果直接查age的话，这时就没有利用到索引了，查询条件where name=‘xxx’ and age=xx 这时的话，就利用到索引了，再来思考下where age=xx and name=’xxx‘ 这个sql会利用索引吗，按照正常的原则来讲是不会利用到的，但是优化器会进行优化，把位置交换下。这个sql也能利用到索引了 大数据练习第三次]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MATLAB]]></title>
    <url>%2Fblog4%2F2019%2F10%2F12%2FMATLAB-0%2F</url>
    <content type="text"><![CDATA[机器学习及MatLab实现1 MATLAB入门基础1.1 MATLAB的简介1.1.1 matlab的教程地址https://ww2.mathworks.cn/help/matlab/ matlab**是什么？** 主要用于科学的计算 用的人 工程师 科学家 量化分析 可视化 计算 1.1.2 matlab的应用 matlab的图像的信号处理 工具包 每年两个版本 matlab matlab2014a matlab2014b 对于版本的选择 如果用途 常规的运算版本之间的差别不是特别大 1.2 安装路径的选择 预览窗口 1.3 MATLAB的变量的命名规则 区分大小写 变量的长度不超过63位 变量名以字母开头 可以由字母 、数字和下划线组成 但是不能使用标点 变联名应该更加简洁，通过变量 名k暗处所表示的物理意义 1.4 MATLAB的数据类型数字 字符与字符串 矩阵 元胞数组 结构体 1.5MATLAB的矩阵操作矩阵的定义和构造 矩阵的四则运算 矩阵的下标 1.6 MATLAB逻辑与流程控制if…. else …end for end while …end switch…case …end 1.7 MATLAB脚本与函数文件脚本文件 函数文件 1.8 MATLAB的基本绘图操作二维平面图 三维立体图 图形的保存与退出 1.9 MATLAB的文件导入mat load txt load xls xlsread xlsread csv csvwrite scvread 2 MATLAB的进阶人工神经网络 什么是人工神经网络]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>MATLAB</tag>
        <tag>业余学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark的学习]]></title>
    <url>%2Fblog4%2F2019%2F10%2F12%2Fspark%2F</url>
    <content type="text"><![CDATA[Spark day011 Spark初始1.1 什么是spark1.2 总体的技术栈的讲解1.3 spark的演变历史 1.4 Spark与MapReduce的区别 1.5 Spark的运行模式 2 spark Java-Scala 混编Maven开发idea创建maven工程 pom文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172&lt;!-- 配置以下可以解决 在jdk1.8环境下打包时报错 “-source 1.5 中不支持 lambda 表达式” --&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- Spark-core --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;2.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- SparkSQL --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;2.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- SparkSQL ON Hive--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-hive_2.11&lt;/artifactId&gt; &lt;version&gt;2.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;!--mysql依赖的jar包--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;/dependency&gt; &lt;!--SparkStreaming--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;2.3.1&lt;/version&gt; &lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt; &lt;/dependency&gt; &lt;!-- SparkStreaming + Kafka --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt; &lt;version&gt;2.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 向kafka 生产数据需要包 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.10.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;!--连接 Redis 需要的包--&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.6.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Scala 包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;2.11.7&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-compiler&lt;/artifactId&gt; &lt;version&gt;2.11.7&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-reflect&lt;/artifactId&gt; &lt;version&gt;2.11.7&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.collections&lt;/groupId&gt; &lt;artifactId&gt;google-collections&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;!-- 在maven项目中既有java又有scala代码时配置 maven-scala-plugin 插件打包时可以将两类代码一起打包 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.scala-tools&lt;/groupId&gt; &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt; &lt;version&gt;2.15.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!-- maven 打jar包需要插件 --&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;2.4&lt;/version&gt; &lt;configuration&gt; &lt;!-- 设置false后是去掉 MySpark-1.0-SNAPSHOT-jar-with-dependencies.jar 后的 “-jar-with-dependencies” --&gt; &lt;!--&lt;appendAssemblyId&gt;false&lt;/appendAssemblyId&gt;--&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.bjsxt.scalaspark.sql.windows.OverFunctionOnHive&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;assembly&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!-- 以上assembly可以将依赖的包打入到一个jar包中，下面这种方式是使用maven原生的方式打jar包，不将依赖的包打入到最终的jar包中 --&gt; &lt;!--&lt;plugin&gt;--&gt; &lt;!--&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;--&gt; &lt;!--&lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;--&gt; &lt;!--&lt;version&gt;2.4&lt;/version&gt;--&gt; &lt;!--&lt;configuration&gt;--&gt; &lt;!--&lt;archive&gt;--&gt; &lt;!--&lt;manifest&gt;--&gt; &lt;!--&lt;addClasspath&gt;true&lt;/addClasspath&gt;--&gt; &lt;!--&amp;lt;!&amp;ndash; 指定当前主类运行时找依赖的jar包时 所有依赖的jar包存放路径的前缀 &amp;ndash;&amp;gt;--&gt; &lt;!--&lt;classpathPrefix&gt;/alljars/lib&lt;/classpathPrefix&gt;--&gt; &lt;!--&lt;mainClass&gt;com.bjsxt.javaspark.sql.CreateDataSetFromHive&lt;/mainClass&gt;--&gt; &lt;!--&lt;/manifest&gt;--&gt; &lt;!--&lt;/archive&gt;--&gt; &lt;!--&lt;/configuration&gt;--&gt; &lt;!--&lt;/plugin&gt;--&gt; &lt;!-- 拷贝依赖的jar包到lib目录 --&gt; &lt;!--&lt;plugin&gt;--&gt; &lt;!--&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;--&gt; &lt;!--&lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;--&gt; &lt;!--&lt;executions&gt;--&gt; &lt;!--&lt;execution&gt;--&gt; &lt;!--&lt;id&gt;copy&lt;/id&gt;--&gt; &lt;!--&lt;phase&gt;package&lt;/phase&gt;--&gt; &lt;!--&lt;goals&gt;--&gt; &lt;!--&lt;goal&gt;copy-dependencies&lt;/goal&gt;--&gt; &lt;!--&lt;/goals&gt;--&gt; &lt;!--&lt;configuration&gt;--&gt; &lt;!--&lt;outputDirectory&gt;--&gt; &lt;!--&amp;lt;!&amp;ndash; 将依赖的jar 包复制到target/lib下&amp;ndash;&amp;gt;--&gt; &lt;!--$&#123;project.build.directory&#125;/lib--&gt; &lt;!--&lt;/outputDirectory&gt;--&gt; &lt;!--&lt;/configuration&gt;--&gt; &lt;!--&lt;/execution&gt;--&gt; &lt;!--&lt;/executions&gt;--&gt; &lt;!--&lt;/plugin&gt;--&gt; &lt;/plugins&gt; &lt;/build&gt; 3 SparkCore3.1 RDD*概念 * 弹性分布式数据集 五大特性 partition 函数作用在每一个partition RDD之间有一系列的依赖关系 分区器是作用在K，V格式的RDD上的 RDD提供一系列的最佳的计算位置 3.2 Spark任务执行的原理3.3 Spark的代码流程 3.4 Transformation转换算子filter map flatMap smaple reduceByKey sortBy/sortByKey 3.5 Action行动算子count take first foreach collect 3.6 控制算子cache persist checkpoint 三种都可以将RDD持久化 cache和persist都是懒执行的 checkpoint 不仅可以 持久化到磁盘 还能切断RDD之间的依赖关系 4 集群的搭建和测试4.1 搭建slaves文件的配置 saprk-env.sh文件的配置 12 SPARK_MASTER_IP:master的ip SPARK_MASTER_PORT:提交任务的端口，默认是7077 4.2Local模式123456bin/spark-submit \--class org.apache.spark.examples.SparkPi \--executor-memory 1G \--total-executor-cores 2 \./examples/jars/spark-examples_2.11-2.1.1.jar \100 解压缩tar包直接运行 不需要配置 Pi的官方群里 4.3 StandAlone模式Master + slaves 构成的集群 conf 文件 配置slaves文件123hadoop102hadoop103hadoop104 12 sprk-env.sh 文件的配置配置JobHistoryServerspark-default.conf 12spark.eventLog.enabled truespark.eventLog.dir hdfs://hadoop102:9000/directory 在hdfs上创建目录 spark-env,sh 123export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=30 -Dspark.history.fs.logDirectory=hdfs://hadoop102:9000/directory" HA的配置 12345678注释掉如下内容：#SPARK_MASTER_HOST=hadoop102#SPARK_MASTER_PORT=7077添加上如下内容：export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=hadoop102,hadoop103,hadoop104 -Dspark.deploy.zookeeper.dir=/spark" saprkHA的访问 1234/opt/module/spark/bin/spark-shell \--master spark://hadoop102:7077,hadoop103:7077 \--executor-memory 2g \--total-executor-cores 2 4.1. Yarn模式4.4 Mesos模式4.5 几种模式的对比4.6 测试PI的计算的案例 yarn上的测试 ./spark-submit –master yarn –class xxxx jar包 100000 Standalone ./spark-submit –master spark://hadoop102:7077 –class jar包 参数10000 Sparkday021 StandAlone的两种提交任务的方式2 Yarn的两种提交任务的方式、3 补充部分算子4 术语解释5宽依赖和窄依赖6Stage7 Spark的资源调度和任务调度广播变量123456789101112131415 不使用广播变量 有很多个driver端的变量副本 使用广播变量只有一分变量副本 大大的减少了内存 广播变量只能在Driver端定义 在Executor中使用 不能再Executor中改变广播量的值BlockManager100个task 只有driver的一份的变量副本 广播zhangsan 再来一个lisi 线程是不安全的 广播变量的代码 1234567891011121314151617181920212223242526272829303132333435363738394041object BrodCastTest &#123; def main(args: Array[String]): Unit = &#123; val conf=new SparkConf() conf.setMaster("local[*]") conf.setAppName("test") val sc = new SparkContext(conf) val list: List[String] = List[String]("zhangsan","lisi") //使用广播变量 速度会提高 val bcList: Broadcast[List[String]] = sc.broadcast(list) //每个executor用了一个broadcast 就会有一个task val nameRDD: RDD[String] = sc.parallelize(List[String]("zhangsan","lisi","wangwu")) val result: RDD[String] = nameRDD.filter(name =&gt; &#123; val innerList: List[String] = bcList.value !list.contains(name) &#125;) result.foreach(println) &#125;&#125;Driver i=0ExecutorExecutor累加器在Driver端定义初始化 累加器伪代码 12345678910111213141516171819val rdd=sc.textfilerdd.count()//统计数据var i=0val rdd2=rdd.map(one=&gt;&#123;i+=1one&#125;)rdd2.collect()println("i="+i) UDFuser defined function 用户自定义函数 12345678910111213141516171819202122232425bject UDF &#123; def main(args: Array[String]): Unit = &#123; val spark: SparkSession = SparkSession.builder().master("local[*]").appName("UDF").getOrCreate() val nameList: List[String] = List[String]("zhangsan","lisi","wangwu","zhaoliu","tianqi") //不要忘记隐式转换 import spark.implicits._ //简历一个列 val nameDF: DataFrame = nameList.toDF("name") //注册表的名称 nameDF.createOrReplaceTempView("students") //注册 去名称 注册udf的函数的名称 //注册一个列 spark.udf.register("STRLEN",(name :String) =&gt;&#123; name.length &#125;) spark.sql("select name,STRLEN(name) as length from students sort by length desc" ) .show() &#125; UDAFuser defined aggregate function 用户自定义聚合函数 继承UeserDefinedAggregateFuntion 实现8个方法 最重要的initialize update merge方法 map端按照groupby 每个RDD分区内按照groupby的字符安分组 initialize方法 初始化设置 update****方法**** Reduce端 merge方法 开窗函数 spark的题目 spark运行模式 spark核心RDD 注意点 spark的代码流程 spark的技术栈相关的技术 介绍联系 spark资源调度 基于standalone和yarn提交任务两种方式的区别 什么是sparkRDD spark 计算模式【术语–&gt;RDD的宽窄依赖 —&gt; Stage 】 spakr资源调度和任务调度流程 粗粒度 细粒度 资源申请 spark的源码看懂 一部分 整体的流程 sparkSubmit —-&gt;&gt;Driver —&gt;&gt;向Master注册Application–&gt;&gt;Application 申请资源 —-&gt;&gt; 任务调度源码 spark Shuffle 两种shuffleManager 发展过程hash shuffleManager sparkShuffle优化 11.内存管理 调内存 8G的内存]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>大数据框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java面试突击]]></title>
    <url>%2Fblog4%2F2019%2F10%2F12%2FjavaInterview%2F</url>
    <content type="text"><![CDATA[1 面试前的准备简介秋招 春招 秋招的难度比春招大 秋招的含金量更高 内推 需要优秀的简历 github有比较好的开源项目 大厂邀请你参加你面试 2 面试须知1 两份简历hr 主要讲自己的经历 技术能力一语带过 技术面试官 讲自己的技术 突出自己的项目经历 技术细节 项目经验 2 着装简单大方 3 简历携带4 提前刷题5 准备项目项目架构 主要承担了什么角色 你负责了什么 用了哪些技术 你解决了 你协助团队解决了哪些问题 3 重点简历 数据结构与算法 自学 好 2 java1 java基础知识重载 重写方法里的参数 重写父类的方法 （返回值 父类private修饰 不能重写） StringBuilder StringBuffer StringString 1private final char value[]; final 修饰 Striing类型不可变 StringBuilder StringBuffer 和String 12public final class StringBuilder extends AbstractStringBuilder 12345*/ public final class StringBuffer extends AbstractStringBuilder implements java.io.Serializable, CharSequence&#123; 继续看源码 1234567891011/abstract class AbstractStringBuilder implements Appendable, CharSequence &#123; /** * The value is used for character storage. */ char[] value; /** * The count is the number of characters used. */ int count; char[] 没有final修饰所以StringBuilder和StringBuffer都是可变的 继续比较StringBuilder和StringBuffer StringBuilder 1234567891011121314public StringBuffer(CharSequence seq) &#123; this(seq.length() + 16); append(seq); &#125; @Override public synchronized int length() &#123; return count; &#125; @Override public synchronized int capacity() &#123; return value.length; &#125; 我们看到了synchronized ， 因为StringBuffer使用了同步锁，所以它的线程安全性得到了提高， 但是性能却下降了 使用的场景 String 少量数据 StringBuilder 单线程 大量数据 StringBuffer 多线程的大量数据 自动拆箱和装箱基本类型和引用类型的相互转换 ==和equals的区别== 基本类型来说 比较数值 对于引用类型比较的是地址 equals String aa=”ab”; String bb=”ab”; 放在常量池中 aa和bb是相等的 String a=new String(“ab”) String b=new String(“ab”) a是一个引用 b是一个引用 a和b的地址是不一样的 String的equals 是重写过的 比较的是连个字符串的值 而Objec中的equals方法是比较的两个对象的地址 final的关键字的总结变量 基本 类型 不能改变 引用类新型 初始化之后不能指向新的对象 方法 保证方法不被改变 类 这个类不能被继承 所有的方式 被认为是final类型 object中的方法notify notifyAll wait java的集合框架ArrayList和LinkedList异同同 不保证线程的安全 java的多线程java虚拟机设计模式计算机网络 Linux Mysql Redis Spring 消息队列 Dubbo 数据结构 算法 实际场景提 BAT真实面试题System.out.println(3|9) &amp; &amp;&amp; ； | || | 和|| 的区别两者都可以是逻辑运算符 | 两边的东西都会运算 || 先判断左边 再去判断右边 了解 2进制 10进制Forward 和 Redirect的区别转发是服务器的行为 重定向是客户端的行为 重定向 是用服务器的返回的状态码来实现的 301 302 新的网址请求这个资源 1 地址显示来说 forward 是服务器请求资源 ，服务器请求地址的资源 服务器直接访问目标地址的url，然后把这些内容发送给浏览器 浏览器根本不知道内容是从哪里来的 redirect是服务端根据逻辑，发送一个状态码，高速浏览器去重新请求那个地址，所以地址显示的是新的url 2 数据共享 ​ foward 转发页面和转发到的额页面乐意共享request中的数据 3 应用 forward 用户登录 redirect 用户退出 注销登录的时候 4 效率上说 forward 高 redirect低 最后面试失败 乃是常事]]></content>
      <categories>
        <category>javaEE</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JUC并发编程]]></title>
    <url>%2Fblog4%2F2019%2F10%2F10%2FJUC%2F</url>
    <content type="text"><![CDATA[1 JUC是什么java.util.current 在并发编程中使用的工具类 进程和线程的回顾1 进程是什么 线程是什么进程是操作系统动作执行的基本单元 在传统的操作系统中，进程既是基本的分配单元，也是基本的执行单元 独立功能的程序关于某个数据集合的一次性活动 线程一个进程可以包含若干个线程，一个进程中至少有一个线程， 不然没有存在的意义 线程可以利用进程拥有的资源 进程作为分配资源的基本单位 线程作为独立运行和独立调度的基本单位，基本上不拥有系统资源， 因此对它的调度所付出的开销就小得多 2 进程和线程的例子3 线程的状态new runnable blocked waiting Timed_waiting terminated 4 wait/sleep的区别wait 线程暂停 wait 放开手去睡 放开手里的锁 sleep握紧手去睡 行了手里还有锁 5 什么是并发 什么是并行并发 同一个时刻在访问同一个 资源 并行 多项工作一起执行 之后再汇总 Lock接口复习synchronized线程操作资源类 高内聚 低耦合 实现步骤1 创建资源类 2 资源类创建同步方法，同步代码 example 买票程序 Lock Lock接口实现 可重入锁怎么用 1234567891011121314class X&#123; private final ReetrantLock lock=new ReetrantLock(); //.... public void m() &#123; lock.lock(); try&#123; //....nethod body &#125;finally&#123; lock.unlock(); &#125; &#125;&#125; synchronized和lock的区别1 首先synchronized是java的内置关键字，在jvm层面，lock是一个java类 2 synchronized无法判断是否获取锁的状态，Lock可以判断是否被获取到锁 3 首先synchronized是java的内置关键字，在jvm层面，lock是一个java类 synchronized无法判断是否获取锁的状态，Lock可以判断是否被获取到锁 4 用synchronized关键字的两个线程线程1 和线程2 ，如果线程1 获得锁 线程2 等待 。如果线程1阻塞线程2 会一致等待下去，Lock就不一定会等待下去，如果一致获取不到锁的话，线程可以不用一直等待就结束了 5 synchronized的锁可重入 不可中断，非公平 而Lock锁可重入，可判断， 可公平（两者皆可） 6Lock锁适合大量同步代码的同步问题 synchronized适合代码量少的同步问题 创建线程的方式集成Thread 实现Runnable方法 新建类实现Runnnable接口 12ckass MyThread umplements Runnablenew Thread() 匿名内部类 123456new Thread(new Runnable()&#123; @Override public void run()&#123;&#125;&#125;) lambda表达式 123new Thread(()-&gt;&#123; &#125;,"your thread name").start() Lambda表达式lambda是一个匿名函数 我们可以把lambda表达式理解为可以传递的代码 左侧 lambda表达式需要的参数 右侧 lambda题 lambda表达式要执行的功能 Runnable接口为什么可以使用lambda表达式]]></content>
      <categories>
        <category>javaEE</category>
      </categories>
      <tags>
        <tag>java高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git的学习]]></title>
    <url>%2Fblog4%2F2019%2F10%2F10%2Fgit%2F</url>
    <content type="text"><![CDATA[一、Git的基础1 认识git有了SVN 为什么还需要git svn增量式管理 GIT采取文件系统快照的方式 svn是集中式管理 git是分布式管理 其他的优势 大部分的操作在本地不需要互联网，只有pull和push时需要 2 git的安装1 Linux的环境./config make sudo make install gt config –global user.name “YourName” 我们注意到了global参数 用了这个参数，表示这台加的所有的git仓库都会使用这个配置 3 git的结构工作区 git add的暂存区 （临时存储） git commit 本地库（历史版本） 很多人喜欢直接commit到本地库 是因为用svn比较多的缘故 工作区 (.git)=.svn git的版本库里面包含了很多的东西，其中最重要的就是成为stage的暂存区， 还有Git为我们自动创建的第一个分支master 以及指向master的一个指针叫Head svn是没有暂存区的 文件git的版本库里添加的时候 分两步执行 实际上就是把文件修改添加到暂存区 4 Git和代码托管中心局域网 - GitLab服务器 外网- GitLab 、 码云 本地库与远程协作的方式 1 团队的内部协作 2 跨团队的协作 5 Git的命令行操作克隆远程仓库12git clone https://github.com/malingzhao/tuchaung.gitcd 本地仓库 本地库的初始化git add 设置签名作用 区分开发人员的身份 设置的额签名和远程 登录的仓库的账号和密码没有关系 命令 123git config user.name tom_progit config user.email [good_pro@126.com](mailto:good_pro@126.com) cd .git 观察配置信息 项目的优先于用户的优先级 二 命令行的常用的操作1 上传文件git clone cd 仓库地址 gti add 文件名 git commit -m “描述” git push orign master 第一次出现user.name user.email 配置输入即可// 查看路径echo $HOME 查看路径git config –global credential.helper store 创建此文件时一依据此路径 三 常见问题和解决方案1 当我们下载了一个仓库的时候，我们需要重新进行认证 ssh-keygen -t rsa -C “username” 然后一路enter下去 查看根目录下的.ssh文件 id_rsa.pub文件 用记事本将这个pub文件打开 进入我们github的官方地址 进入页面 将我们的记事本中的内容粘贴进去 新建sshkey的地址 https://github.com/settings/ssh/new 最后输入 仍然在bash.exe中输入ssh -T git@github.com 最后的时候输入yes 不要enter 个人实践过 出错了]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>技能</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据面试]]></title>
    <url>%2Fblog4%2F2019%2F10%2F09%2FbigdataInterview%2F</url>
    <content type="text"><![CDATA[Michael_PK 大数据团队面试 spark Hadoop 的公有云和私有云 剑指大数据面试概述互联网寒冬 简历投递的活跃度高 大数据岗位 够硬的技术功底 表现能力 拓展和提升大数据相应的能力 java Linux的基础知识 具有一定的项目经 1 开篇就业趋势 高效Offer 2 大数据处理架构总体架构 系统设计 3 小文件引发的血案篇hdfs 小文件问题 4 SQL on Hadoop架构层面的调优 语法层面调优 执行框架底层调优 sql的实战案例 5 数据倾斜篇什么是数据倾斜及产生的原因 大数据中的shuffle 产生数据倾斜的场景 数据倾斜的解决方案 6 spark的调优篇算子的合理选择给计算性能带来的深远影响 合理的序列化整合saprk使用为性能提速 如何保证流处理过程的零数据丢失 流处理数据Sink到目的地的N中错误的操作剖析 案例实战： 如何基于Spark定制外部数据源 7 java篇javaSE 多线程 生产者和消费者 jvm 8 jvm篇ClassLoader机制 内存模型 垃圾回收 垃圾回收算法 垃圾回收器 9 其他篇 zookeeper Linux 10 技巧篇2小文件引发的血案篇1 HDFS架构 学习一个新框架 ​ 百度 文档 不推荐 ​ 开源框架存在问题 推荐 官网+源码 ​ 跪在坚持 ​ hadoop ​ spark ​ flink ​ org.apache.xxxxx hadoop :HDFS/YARN/MapReduce3 HDFS ​ NameNode ​ DataNode ​ SecondaryNameNode ​ 概念 Client NN: 一个，Single Problem Of Failure ====&gt; HA metadata： 谁 权限 文件对应block的信息 文件名 副本信息（生产环境一般3个） 副本机制 ======》》》 增加容错 DN： 多个 存储数据 和NN之间是有心跳的 Block : File存入HDFS 按照block进行拆分 128M 2 HDFS的读写流程HDFS的写流程 HDFS的读流程 请求最近的NameNode 最终 3HDFS HA NameNode挂掉 SecondaryNameNode Active Standby 同步NameNode的状态 共享 快速的切换 两个独立的机器上 一个活动状态一个备份状态 Zookeeper 实现高可用调度系统调度机 zookeeper进行管理持久性的 临时性的 HA 主 和 备的 切换 各自的进程监控zookeeper状态的切换Active Standby 数据的同步 共享 。。。。很多 Yarn的架构 ResourceManager HA NN HA 也有这个问题？ 什么导致 小文件问题RM HA 遇到的问题： 起不来NN HA 也有这个问题？ 什么导致 小文件问题 RM HA 遇到的问题： 起不来 SLA 如何保证？？ 99.99% 99.95% 4 小文件是什么小文件的定义 小文件 明显小于block size的文件 80% 129M 128M + 1M 为什么产生小文件1 批处理 离线计算 小文件 特别是spark 2 数据拷贝到HDFS 没有关注 数据搬迁（手工 Flume采集）没有做很好的设置 3 MapReduce 作业 Reduce 没有做设置 Reduce 决定了输出文件的多少 shuffle合理的设置 hadoop的目录 文件 blk是以元数据的方式存储下来的 200字节 5 小文件给hadoop集群带来的瓶颈问题 100个小文件 IO n多个小文件的IO1 IO开销大2 Map task Reduce Task 的开启和销毁 （task）jvm的启动销毁3 资源有限 3 SQL on Hadoop1 SQL on Hadoop的常用框架常用的SQL on hadoop框架 Hive sql =====》》 对应的sql转换成对应的执行引擎的作业： MapReduce/Spark/Tez Impala： 内存 Presto: 京东 Drill: 跨数据源的查询 Phoenix： HBase（RowKey） 性能高 API + 命令行 sql查询hbase的东西 Spark SQL： 查询结构化数据 MetaStore 存储表的元数据信息 框架之间是共享元数据信息的 Hive on Spark: Hive社区 MR Spark TezSpark SQL： Spark社区 Spark on Hive : X 错误的说法 2 行式存储 vs 列式存储 列式存储 带来很大的性能提升 分表SQL on Hadoop的调优策略 架构层面调优 架构调优之分表 spark ETL操作 Flume ===&gt;&gt; HDFS===&gt;&gt; spark EL ===&gt;&gt; SQL==&gt;&gt;Spark SQL ==&gt; NoSQL 前提 1 行式 2 每分钟2亿条数据 3 500个作业访问这个大表 分区表系统用户日志： who done 语法调优jvm重用 MapTask/ReduceTask都是以进程存在的 ，有多少个task就有多少个jvm 推测执行1 集群中的机器负载是不同的 2 集群中机器的配置不同 3 数据倾斜 一个job100个reduce 99个很快运行完， 只有最后一个话费很撑的执行时间，那么这个job它的运行速度是取决于最慢的一个task 长尾作业 并行执行 默认没有开启 并行的前提多个task之间是没有依赖的 jvm重用MapTask 和ReduceTask 都是以进程的形式存在的，有多少个task就有多少个jvm 当task运行完成结束之后，jvm就会被销毁，jvm的启动和销毁需要开销，每个jvm可以执行多个task 本章总结框架 MetaStore 行式存储 列式存储 调优策略 4 Spark调优篇其他篇分布式锁什么是分布式锁： 公共资源： 同步、加锁 下订单 zookeeper 实现分布式锁 创建一个springboot工程 订单： id itemid 条目： id name counts 1 Spark 10 2 Hadoop 6 3 Flink 3 Spring Data 操作数据库 springdatajpa + mysql 12345jpa: hibernate: ddl-auto: update database: mysql 同一个业务的执行 两个请求一块发起 两个订单同时创建成功 案例演示 自动创建表的配置 套路 dao service controller domain 业务层 代码详情https://github.com/malingzhao/bigdata Linu的内容1 检索： 内容​ grep pk1.txt pk2.txt grep “oop” pk*|grep “Spark” 管道操作符 | 多个命令 多个指令 连接起来 前一个指令的结果作为下一个指令的输入 ps -ef ps -ef| grep zookeeper ps -ef|grep zookeeper |grep -v “grep” -v过滤东西 2 对内容的统计awk 获得第一列和第二列 tab键分割的处理 awk ‘{print $1,$2}’ file.txt 拿到头 awk “$1=8888 || NR=1” emp.txt 逗号 awk -F “,” ‘{print $1}’ sales.csv 拿出来 -F 指定分割符 awk -F “,” ‘{print $0}’一行所有的数据 3 对内容的替换sed java的格式转换成scala的 sed ‘s/String/val/‘ test.txt 第一个原来的 替换成 val sed -i ‘s/String/val’ test.txt 完成第二步的操作 sed -i ‘s/;/ /‘ test.txt 默认替换第一个 sed -i ‘s/Hadoop/hadoop/‘ test.txt 想要全局替换 sed -i ‘s/Hadoop/hadoop/g’ test.txt 技巧篇技术一票决定的权利公司的价值观 你的职业规划为什么要离职薪资问题 加班（抗压能力） 周边的同事 领导 项目 ==》》》》》》》》 个人的发展： 加分 个人的成长空间 离职原因不是个人原因 不是频繁的离职 找到更适合自己的平台 家庭的原因： 对象 你对加班的看法常态 公司基本上加班是早上11点 住的地方就在公司附近 有家 单身 为什么选择我们公司公司 海投 打电话 为什么选择你们公司 建议 对于个人特别想去的 建立一定要差异化 投不同的公司 投不一样的简历 编简历 简历的包装 行业 公司 岗位的描述 喜欢 胜任 你的优缺点目的 你这个人对自己有没有对自己的清晰的认识 自我认知 实事求是 优点： 全力以赴 适应能力 缺点： 推辞能力 在意别人的看法 问问题 自己加班加点===》》（乐于助人，人缘，关系融洽） 大数据架构图日志分析项目 电商推荐系统 电商数仓项目]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kylin的学习]]></title>
    <url>%2Fblog4%2F2019%2F10%2F09%2FKylin%2F</url>
    <content type="text"><![CDATA[1 概述1.1 Kylin定义开源 分布式 分析引擎 提供基于Hadoop/spark的sql查询和多维分析 OLAP能力（联机分析处理） 亚秒内查询巨大的Hive表 1.2 架构 1 RestServer Rsultful接口 提供查询 2 查询引擎 解析用户查询 3 路由器 在发行版是默认关闭的 体验不好 Hive的速度和Kylin的速度相差加大 4 元数据管理工具 元数据驱动应用程序 kylin的元数据存储在hbase中 5 分析引擎 处理离线任务 shell脚本 java API MapReduce任务 任务引擎需要对kylin中的任务进行协调和管理 1.3特点SQL接口 超大规模数据集 亚秒级的响应 ​ 很多复杂的计算 连接 聚合 离线的预计算过程就完成了 可伸缩性和高吞吐率 搭建集群 每秒70个查询 BI ODBC tableau JDBC RestPI Kylin 2 环境搭建官方文档即可 3 入门4 Cube构建原理5 cube构建优化6 BI集成]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper的学习]]></title>
    <url>%2Fblog4%2F2019%2F10%2F09%2Fzookeeper%2F</url>
    <content type="text"><![CDATA[1 zookeeper入门1.1概述开源 为分布式应用提供协调服 Zookeeper=文件系统+通知机制 1.2 特点Leader follower 半数机制 每个server保存的数据一致（数据备份） 实时性 更新请求顺序执行 1.3 数据结构unix文件系统 形结构 1.4 应用场景统一的配置管理和配置服务 1.5下载地址https://zookeeper.apache.org/ 2 zookeeper的安装2.1 本地模式zoo.cfg bin/zksSrver.sh start|status|stop 出现bash4.1 .bash_profile 每次登陆 .bashrc 每次进入新的bash环境 .bash_logout 每次退出登录 .bash_history 每用户注销前使用的命令 进入root用户 cp .bash_profile .bashrc .bash_logout .bash_profile /home/user 问题得到解决 2.2 配置参数的解读tickTime 2000 initLimit 10 syncLimit dataDir clientPort 3 Zookeeper的内部的原理3.1 选举机制半数机制，适合安装奇数台服务器 没有mater和slave，但是zookeeper在工作的时候 一个leader 其他的都是follower 4 zookeeper的实战分布式部署安装zoo.cfg myid zkData 12345dataDir=/opt/module/zookeeper-3.4.10/zkData#######################cluster##########################server.2=hadoop102:2888:3888server.3=hadoop103:2888:3888server.4=hadoop104:2888:3888 server.A=B:C:D A myid B 主机名称 C 与Leader交换信息 D 一但leader挂掉 选举出新的leader 4.2 API操作5 面试zookeeper的选举机制zookeeper的监听原理main方法 connect线程和listener线程 connect 将监听的事件发送给zookeeper zookeeper在监控l列表上添加事件 listener线程监听到事件变化 zookeeper调用process方法 zookeeper的部署的方式本地部署 分布式部署 zookeeper的常用命令ls create get delete set]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[电商推荐系统]]></title>
    <url>%2Fblog4%2F2019%2F10%2F09%2Fdianshangtuijianxitong%2F</url>
    <content type="text"><![CDATA[1 电商推荐系统的简介1.1 项目的架构设计1.1.2 亚马逊推荐系统的贡献 比例达到了20%到25% 在亚马逊的页面上 推荐列表占了很大的比例 真实的业务架构 构建真正的系统 基于统计的模块 实时 自定义的推荐模块 设计到机器学习 和推荐系统的相关知识 电影推荐系统 切换不同的业务场景 一通百通 对大数据的工具有一个更深刻的理解、 做的是电商推荐系统 推荐系统的具体的应用 1.1.2 分析项目框架 数据源解析 统计推荐模块+ 基于LFM的离线推荐模块 基于自定义模型的实时推荐模块 其他形式的离线相似推荐 基于内容的推荐和 ​ 基于物品的协同过滤模块 1.1.3 数据的生命周期 数据源 三大类 图片 视频 非结构化的数据 日志数据 半结构化的数据 结构化数据 关系数据 数据源 数据采集 数据存储 数据计算 数据应用 用的数据库 mongodb 运算处理 （Mahout） hadoop的 storm的流式处理 spark flink 大数据的计算框架 算完之后 存储到数据库里 分析 Echarts 等数据可视化展示 Cassandra NoSQL 1.2 大数据的处理流程 左边的实时的处理 网站 APP 前端页面 用户接口 —》》》》 http请求 —-》》》 业务系统的后台—-》》》》 调用相应的服务 响应—–》》 埋点收集日志 –》》 记录用户的行为 —》》 日志的采集 （Flume） 数据总线 —》》 KAfka （做存储） 实时消费 flume的数据 —-》》 sparkStreaming 实时计算 —-》》 数据的可视化展示 右边的离线处理的流程 业务系统 日志文件 flume 日志采集 sink 配置成hdfs存储 日志清洗ETL 做数据清洗的操作 数据仓库 最后对这些数据进行计算 对数据进行离线计算 相关的业务数据库 最后做可视化展示 推荐系统 —》》 大数据的典型应用 1.3 我们的目标 不同的地方显示不同的推荐的结果 商品的详情 评分 xxx 然后下面相似的商品推荐出来 混合推荐 典型分区混合 1.4 项目的系统架构用户可视化 Angular JS 前端 后台spring 不是我们主要考虑的地方 数据 存储 MongoDB 一些重要的数据 缓存到Redis里面 并不是一定要存MongoDB mongoDB大数据平台很主流的数据库 读写性能 支持很大的数据量 mongo是一个文档型数据库 json串 存储在里面 很多的特征 redis缓存常规操作 ES 模糊查询 条件查询 离线的推荐服务 统计服务 个性化统计服务 在线 实时 Flume kafka 缓冲 sparkStreaming 实时的推荐处理 Redis缓存的数据 写回到mongo里 用户可视化 推荐 1.4.1 离线数据加载服务 数据放到mongo里面 离线统计服务 sparkSQL 写回表 对应的查询 个性化推荐 隐语义模型的推荐模块 基于内容的推荐 最终的结果写进mongo 1.4.2 实时log Flume kafka 消息缓冲 对应的消息处理 spark Streaming对数据过滤 最后做实时的计算 redis里面拿到最近的评分数据 ‘ 商品检索 mongo es也是可以的 1.5 数据源的解析1.5.1 信息介绍 商品信息 1.用户评分信息 基于商品的信息 做基于内容推荐 用户的评分数据 隐语义模型 协同过滤的推荐 1.6主要的数据模型商品相似度 为了实时推荐做基础 实时 1.6 统计推荐模块商品相似度 1.6.1 历史热门商品统计什么样的商品是热门的 评分的多少 1select productId ,count(productId) as count from ratings group by productId order by count desc RatingMoreProducts 1.6.2近期热门商品的统计UDF函数 changeDate 时间戳 转换为年月的格式 分解成一个月的的热门商品 1.6.3商品的平均得分的统计统计的指标 相应的实现 1.7 离线推荐模块ALS算法进行隐语义模型训练 ALS.train() lambda 正则化参数 iterations 迭代次数 隐语义模型定义的隐特征的个数 涉及到了模型评估和参数调整 RMSE 考察预测评分和实际评分的误差 得到选取什么样的参数是最好的 1.7.1 计算用户推荐矩阵 user的RDD 和product的RDD做了一个笛卡尔积 物品两两匹配得到的结果 model.predict uid 聚合 sortBy（“score”.take(20) sparkSession.write 1.7.2计算商品相似度矩阵特征向量 矩阵分解 用户的特征向量矩阵 商品的特征向量矩阵 笛卡尔积 给实时推荐做基础 1.8 基于模型的实时推荐模块 1.8.1 需求计算速度快 结果可以不是特别精确 有预先设计好的推荐模型 评分数据 flume kafka log mongo 里面 redis里 结果写回mongo 1.8.2推荐优先级的计算刚看的商品 差评的物品 综合考虑相似度和评分 对剑优先级 计算公式 .推荐的基础评分项 奖励 惩罚 incount 评分里面的高分项 AB 高分 奖励 C 低分 惩罚 前面的基础项加权 lg 2 AB高分 C是低分 1.9 其他形式的离线相似推荐点开用户的商品详情页 出现相似的内容 基于用户购买了哪些商品 1.9.1 基于内容的推荐与A有相同的标签的商品 TF-IDF算法 根据UGC的特征提取item-CF算法 喜欢商品A的人还喜欢哪些商品 TF 词频 每一个标签 词语 每一个商品获得的一个标签 提取出物品的特征向量 余弦相似度 1.9.3 基于物品的协同过滤 1.10 混合推荐分区推荐 同时购买了商品i和商品j 2 环境的搭建2.1 安装monhgodb1234567891011wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel62-3.4.3.tgztar xvf 到 ./cluster然后mv /usr/local/mongodbcd /usr/local/mongodb创建 /usr/local/mongodb/data/usr/local/mongodb/data/db/usr/local/mongodb/data/logs/usr/local/mongodb/data/logs/mongodb.log/usr/local/mongodb/data/mongodb.conf mongodb.conf 1234567891011#端口号port = 27017#数据目录dbpath = /usr/local/mongodb/data/db#日志目录logpath = /usr/local/mongodb/data/logs/mongodb.log#设置后台运行fork = true#日志输出方式logappend = true#开启认证#auth = true 2.1.1 mongodb的启动1sudo /usr/local/mongodb/bin/mongod -config /usr/local/mongodb/data/mongodb.conf 2.1.2 mongodb的访问1/usr/local/mongodb/bin/mongo 2.1.3 mongodb的停止1/usr/local/mongodb/bin/mongo 2.2 安装Redis2.2.1 Redis的基本安装获得安装包 12wget http://download.redis.io/releases/redis-4.0.2.tar.gz 解压 tar xvf xxx ./clsuetr cd redis 1sudo yum install gcc 编译源代码 1make MALLOC=libc 编译安装 1sudo make install 创建配置文件 1sudo cp ./redis-4.0.2/redis.conf /etc/redis.conf 修改配置文件 12345daemonize yes #37行 #是否以后台daemon方式运行，默认不是后台运行pidfile /var/run/redis/redis.pid #41行 #redis的PID文件路径（可选）bind 0.0.0.0 #64行 #绑定主机IP，默认值为127.0.0.1，我们是跨机器运行，所以需要更改logfile /var/log/redis/redis.log #104行i #定义log文件位置，模式log信息定向到stdout，输出到/dev/null（可选）dir “/usr/local/rdbfile” #188行 #本地数据库存放路径，默认为./，编译安装默认存在在/usr/local/bin下（可选） 2.2.2 Redis的基本使用启动redis服务器 1redis-server /etc/redis.conf 连接redis服务器 1redis-cli 停止redis服务器 1redis-cli shutdown 2.3 安装Spark（单节点） 上传安装包 ./cluster 配置slaves 添加主机名 hadoop101 配置spark的主机名称和端口号 spark-env.sh 12SPARK_MASTER_HOST=linux #添加spark master的主机名SPARK_MASTER_PORT=7077 #添加spark master的端口号 14. 启动 1sbin/start-all.sh 2.4 安装 Zookeeper 单节点2.5 安装Flume-ng 单节点 上传安装包 1wget http://www.apache.org/dyn/closer.lua/flume/1.8.0/apache-flume-1.8.0-bin.tar.gz ​ 2. 解压 即可 2.6 安装kafka 单节点1 解压 2 配置 server.properties 3 kafka节点的使用 3 项目的搭建3.1 新建maven项目com.xxx 最外层pom.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889 &lt;properties&gt; &lt;log4j.version&gt;1.2.17&lt;/log4j.version&gt; &lt;slf4j.version&gt;1.7.22&lt;/slf4j.version&gt; &lt;mongodb-spark.version&gt;2.0.0&lt;/mongodb-spark.version&gt; &lt;casbah.version&gt;3.1.1&lt;/casbah.version&gt; &lt;redis.version&gt;2.9.0&lt;/redis.version&gt; &lt;kafka.version&gt;0.10.2.1&lt;/kafka.version&gt; &lt;spark.version&gt;2.1.1&lt;/spark.version&gt; &lt;scala.version&gt;2.11.8&lt;/scala.version&gt; &lt;jblas.version&gt;1.2.1&lt;/jblas.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;jcl-over-slf4j&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;!--maven项目 各种各样的声明周期编译的插件--&gt; &lt;build&gt; &lt;!--声明并引入子项目共有的插件--&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.6.1&lt;/version&gt; &lt;!--所有的编译用JDK1.8--&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;!--maven的打包插件--&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!--该插件用于将scala代码编译成class文件--&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt; &lt;executions&gt; &lt;!--绑定到maven的编译阶段--&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;/build&gt; 2.2reoommender 的pom.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- 引入Spark相关的Jar包 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-mllib_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-graphx_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;build&gt; &lt;plugins&gt; &lt;!-- 父项目已声明该plugin，子项目在引入的时候，不用声明版本和已经声明的配置 --&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 2.3DataLoader 的搭建pom.xml 123456789101112131415161718192021222324252627&lt;dependencies&gt; &lt;!-- Spark的依赖引入 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 引入Scala --&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 加入MongoDB的驱动 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;casbah-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;casbah.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt; &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;mongodb-spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; log4j.properties的配置 1234log4j.rootLogger=info, stdoutlog4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; %5p --- [%50t] %-80c(line:%5L) : %m%n 导入数据集 新建类 样例类 搭建基本的程序框架 隐式类的加入 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154package com.atguigu.recommenderimport com.mongodb.casbah.commons.MongoDBObjectimport com.mongodb.casbah.&#123;MongoClient, MongoClientURI&#125;import org.apache.spark.SparkConfimport org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;/* * @创建人: MaLingZhao * @创建时间: 2019/9/29 * @描述： 推荐系统的加载服务 *//*Producct 数据集21643^ 商品ID 0巴黎欧莱雅男士劲能醒肤露 8重功效50ml^ 商品名称 1916,502,352 商品的分类ID 不需要 2B0010MBKHO^ 亚马逊ID 不需要 3Y300_QL70_.jpg^ 商品的图URL 为了和业务系统相关联小说|文学艺术|图书音像 商品分类 5男士乳液/面霜|男士护肤| 商品的UGC标签 6*/case class Product(productId :Int,name:String, imageUrl:String,categories:String,tags:String)/*Rating数据集407423, 用户ID457976, 商品ID5.0, 评分数据1379001600 时间戳 *///Int 类型case class Rating(userId:Int, productId:Int,score:Double,timestamp:Int)/*uri mongoDb的ueldb 要操作的db *///mongo的配置封装样例类case class MongoConfig(uri:String,db:String)object DataLoader &#123; //定义数据文件路径 val PRODUCT_DATA_PATH="D:\\xiangmu\\ECommerceRecommendSystem\\recommender\\DataLoader\\src\\main\\resources\\products.csv" val RATING_DATA_PATH="D:\\xiangmu\\ECommerceRecommendSystem\\recommender\\DataLoader\\src\\main\\resources\\ratings.csv" //定义mongoDB中存储的表名 val MONGODB_PRODUCT_COLLECTION="Product" val MONGODB_RATING_COLLECTION="Rating" def main(args: Array[String]): Unit = &#123; val config=Map( "spark.cores" -&gt; "local[*]", "mongo.uri" -&gt; "mongodb://linux:27017/recommender", "mongo.db" -&gt;"recommender" ) //创建一个spark Config val sparkConf=new SparkConf().setMaster(config("spark.cores")).setAppName("DataLoader"); //创建sparkSession val spark=SparkSession.builder().config(sparkConf).getOrCreate() //隐式转换的包 import spark.implicits._ //加载数据 val productRDD= spark.sparkContext.textFile(PRODUCT_DATA_PATH) //转换成表结构 val productDF=productRDD.map(item =&gt;&#123; //第一个反斜杠 转义后面的反斜杠 数据通过^分割 val attr=item.split("\\^") //转换成product类 最后一行代表代码的返回 Product(attr(0).toInt,attr(1).trim,attr(4).trim,attr(5).trim,attr(6).trim) &#125;).toDF() val ratingRDD=spark.sparkContext.textFile(RATING_DATA_PATH) val ratingDF=ratingRDD.map(item=&gt;&#123; //逗号不用转义 val attr=item.split("," ) Rating(attr(0).toInt,attr(1).toInt,attr(2).toDouble,attr(3).toInt) &#125;).toDF() //调用多次的活 不用每次传入参数 implicit val mongoConfig=MongoConfig(config("mongo.uri"),config("mongo.db")) storeDataInMongoDB(productDF,ratingDF) //spark.stop()、 &#125; //隐式参数的使用 //以后调用的参数 def storeDataInMongoDB(productDF:DataFrame,raingDF:DataFrame)(implicit mongoConfig: MongoConfig): Unit = &#123; //新建和一个mongoDb的连接 客户端 val mongoClient = MongoClient(MongoClientURI(mongoConfig.uri)) //定义要操作的mongoDb中的表 val productCollection = mongoClient(mongoConfig.db)(MONGODB_PRODUCT_COLLECTION) val ratingCollection = mongoClient(mongoConfig.db)(MONGODB_RATING_COLLECTION) //如果表存在的话 删除 productCollection.dropCollection() ratingCollection.dropCollection() //将当前的数据存入对应的表中 productDF.write .option("uri", mongoConfig.uri) .option("collection", MONGODB_PRODUCT_COLLECTION) .mode("overwrite") .format("com.mongodb.spark.sql") .save() raingDF.write .option("uri", mongoConfig.uri) .option("collection", MONGODB_RATING_COLLECTION) .mode("overwrite") .format("com.mongodb.spark.sql") .save() //创建索引 //字段 productCollection.createIndex(MongoDBObject("productId" -&gt; 1)) ratingCollection.createIndex(MongoDBObject("productId" -&gt; 1)) ratingCollection.createIndex(MongoDBObject("userId" -&gt; 1)) mongoClient.close() &#125; &#125; 运行程序 测试代码吗 启动本地的mongo的测试 4 离线推荐服务 代码4.1 创建maven项目任务调度工具 离线统计服务 pom.xml 123456789101112131415161718192021222324252627&lt;dependencies&gt; &lt;!-- Spark的依赖引入 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 引入Scala --&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 加入MongoDB的驱动 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;casbah-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;casbah.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt; &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;mongodb-spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 统计学习模块 没有MLib log4j 复制过来 建立类 4.2 创建离线统计 服务 StatisticsRecommender代码部分 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142package com.atguigu.statisticsimport java.text.SimpleDateFormatimport java.util.Dateimport org.apache.spark.SparkConfimport org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;/* * @创建人: MaLingZhao * @创建时间: 2019/9/30 * @描述： *//*商品的信息不是特别的重要只需要评分的数据集 */case class Rating(userId:Int, productId:Int,score:Double,timestamp:Int)/*uri mongoDb的ueldb 要操作的db *///mongo的配置封装样例类case class MongoConfig(uri:String,db:String)object StatisticsRecommender &#123; val MONGODB_RATING_COLLECTION="Rating" //统计的表的名称 val RATE_MORE_PRODUCTS = "RateMoreProducts" val RATE_MORE_RECENTLY_PRODUCTS = "RateMoreRecentlyProducts" val AVERAGE_PRODUCTS = "AverageProducts" def main(args: Array[String]): Unit = &#123; val config = Map( "spark.cores" -&gt; "local[*]", "mongo.uri" -&gt; "mongodb://linux:27017/recommender", "mongo.db" -&gt; "recommender" ) //创建一个spark Config val sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("StatisticsRecommender"); //创建sparkSession val spark = SparkSession.builder().config(sparkConf).getOrCreate() //隐式转换的包 import spark.implicits._ //调用自己的包里面的 //调用多次的活 不用每次传入参数 implicit val mongoConfig=MongoConfig(config("mongo.uri"),config("mongo.db")) //加载数据 val ratingDF = spark.read .option("uri",mongoConfig.uri) .option("collection",MONGODB_RATING_COLLECTION) .format("com.mongodb.spark.sql") .load() //转换为定义的数据结构 .as[Rating] .toDF() //创建一张ratings的临时表 ratingDF.createOrReplaceTempView("ratings") //TODO 用spark sql 去做不同的统计推荐 //1. 历史热门商品 ，按照评分个数统计 //id 聚合 降序排列 productId count val rateMoreProductsDF =spark.sql("select productId,count(productId) as count from ratings group by productId order by count desc") //一一对应 storeDFInMongoDB(rateMoreProductsDF,RATE_MORE_PRODUCTS) //2. 近期热门商品，把时间戳转换成yyyyMM 年月格式进行评分个数统计 最终得到的是 productId count yearmonth // 创建一个日期的格式化工具 val simpleDateFormat=new SimpleDateFormat("yyyyMM") //注册UDF，将timestamp转化为年月格式yyyyMM //x为毫秒 x的格式为长整型 spark.udf.register("changeDate",(x:Int)=&gt;simpleDateFormat.format(new Date(x*1000L)).toInt) //把ratings 数据转换成想要的格式 productId，score yearmonth val ratingOfYearMonthDF=spark.sql("select productId,score,changeDate(timestamp) as yearmonth from ratings") ratingOfYearMonthDF.createOrReplaceTempView("ratingOfMonth") val rateMoreRecentlyProductDF =spark.sql("select productId, count(productId) as count ,yearmonth from ratingOfMonth group by yearmonth,productId order by yearmonth desc,count desc ") //把DF保存到mongoDB storeDFInMongoDB(rateMoreProductsDF,RATE_MORE_RECENTLY_PRODUCTS) //3. 优质商品统计 ，商品的平均得分 val averageProductsDF=spark.sql("select productId,avg(score) as avg from ratings group by productId order by avg desc ") storeDFInMongoDB(averageProductsDF,AVERAGE_PRODUCTS) spark.stop() &#125; def storeDFInMongoDB(df: DataFrame, collection_name: String)(implicit mongoConfig: MongoConfig): Unit = &#123; df.write .option("uri",mongoConfig.uri) .option("collection",collection_name) .mode("overwrite") .format("com.mongodb.spark.sql") .save()&#125;&#125;测试代码 4.3 基于隐语义模型的推荐评分矩阵 分解成两矩阵 用户商品推荐列表 ALS训练出来的Model 计算当前用户推荐列表 userId 和productId 产生(userId,productId)的分组 预测评分 评分排序 返回分支最大的K个商品 作为当前用户的推荐列表 UserRevcs 4.3.2 商品相似度矩阵 存储结构 4.3.4 创建模块 pom.xml 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scalanlp&lt;/groupId&gt; &lt;artifactId&gt;jblas&lt;/artifactId&gt; &lt;version&gt;$&#123;jblas.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Spark的依赖引入 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-mllib_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 引入Scala --&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 加入MongoDB的驱动 --&gt; &lt;!-- 用于代码方式连接MongoDB --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;casbah-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;casbah.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 用于Spark和MongoDB的对接 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt; &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;mongodb-spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; log4j配置文件的复制 4.3.5ALS算法代码部分 包装样例类 12345678910//定义标准推荐对象case class Recommendation(productId:Int,scre:Double) //定义用户的推荐列表case class UserRecs(userId:Int,recs:Seq[Recommendation]) //定义商品相似度列表case class ProductRecs(productId:Int,recs:Seq[Recommendation]) scala代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165case class ProductRating(userId:Int, productId:Int,score:Double,timestamp:Int)//定义标准推荐对象case class Recommendation(productId:Int,scre:Double)//定义用户的推荐列表case class UserRecs(userId:Int,recs:Seq[Recommendation])//定义商品相似度列表case class ProductRecs(productId:Int,recs:Seq[Recommendation])/*uri mongoDb的ueldb 要操作的db *///mongo的配置封装样例类case class MongoConfig(uri:String,db:String)object OfflineRecommender &#123; //定义mongodb中存储的表名 val MONGODB_RATING_COLLECTION = "Rating" //用户的推荐列表 val USER_RECS = "UserRecs" //商品相似度表 val PRODUCT_RECS = "ProductRecs" //最大的返回数量 val USER_MAX_RECOMMENDATION = 20 def main(args: Array[String]): Unit = &#123; val config = Map( "spark.cores" -&gt; "local[*]", "mongo.uri" -&gt; "mongodb://linux:27017/recommender", "mongo.db" -&gt; "recommender" ) //创建一个spark Config val sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("StatisticsRecommender"); //创建sparkSession val spark = SparkSession.builder().config(sparkConf).getOrCreate() //隐式转换的包 import spark.implicits._ //调用自己的包里面的 //调用多次的活 不用每次传入参数 implicit val mongoConfig = MongoConfig(config("mongo.uri"), config("mongo.db")) //加载数据 RDD ALS训练的时候使用 val ratingRDD = spark.read .option("uri", mongoConfig.uri) .option("collection", MONGODB_RATING_COLLECTION) .format("com.mongodb.spark.sql") .load() //转换为定义的数据结构 .as[ProductRating] .rdd .map( rating =&gt; (rating.userId, rating.productId, rating.score) ) .cache() //避免RDD的重复计算 // 提取出用户和商品的所有数据集 val userRDD = ratingRDD.map(_._1).distinct() val productRDD = ratingRDD.map(_._2).distinct() //TODO 核心计算过程 //1 训练隐语义模型 val trainData = ratingRDD.map(x =&gt; Rating(x._1, x._2, x._3)) //定义模型训练参数 rank 隐语义的隐特征的个数 iterations 迭代次数 lambda 正则化项系数 val (rank, iterations, lambda) = (5, 10, 0.01) val model = ALS.train(trainData, rank, iterations, lambda) //2 获得预测评分矩阵，得到用户的推荐列表 //userRDD 和productRDD 做笛卡尔积 得到空的userProductRDD表示的评分矩阵 val userProducts = userRDD.cartesian(productRDD) val preRating = model.predict(userProducts) //得到从预测评分矩阵提得到用户推荐列表 val userRecs = preRating.filter(_.rating &gt; 0) //每个用户id 物品对应的id .map( rating =&gt; (rating.user, (rating.product, rating.rating))) .groupByKey() .map &#123; case (userId, recs) =&gt; //降序排列 sort UserRecs(userId, recs.toList.sortWith(_._2 &gt; _._2).take(USER_MAX_RECOMMENDATION) .map(x =&gt; Recommendation(x._1, x._2))) &#125; .toDF() //写回到mongoDB 数据库中 userRecs.write .option("uri", mongoConfig.uri) .option("collection", USER_RECS) .mode("overwrite") .format("com.mongodb.spark.sql") .save() //3 利用商品的特征向量，计算商品的相似度列表 // 3. 利用商品的特征向量，计算商品的相似度列表 val productFeatures = model.productFeatures.map&#123; case (productId, features) =&gt; ( productId, new DoubleMatrix(features) ) &#125; // 两两配对商品，计算余弦相似度 val productRecs = productFeatures.cartesian(productFeatures) .filter&#123; case (a, b) =&gt; a._1 != b._1 &#125; // 计算余弦相似度 .map&#123; case (a, b) =&gt; val simScore = consinSim( a._2, b._2 ) ( a._1, ( b._1, simScore ) ) &#125; .filter(_._2._2 &gt; 0.4) .groupByKey() .map&#123; case (productId, recs) =&gt; ProductRecs( productId, recs.toList.sortWith(_._2&gt;_._2).map(x=&gt;Recommendation(x._1,x._2)) ) &#125; .toDF() productRecs.write .option("uri", mongoConfig.uri) .option("collection", PRODUCT_RECS) .mode("overwrite") .format("com.mongodb.spark.sql") .save() spark.stop() &#125; def consinSim(product1: DoubleMatrix, product2: DoubleMatrix): Double = &#123; product1.dot(product2)/(product1.norm2()) * product2.norm2()&#125;&#125; 4.3.3 模型参数评估选取最优的参数 RMSE 求得误差 考察 12 ALSTrainer 代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071 object ALSTrainer&#123;def main(args: Array[String]): Unit = &#123; val config = Map( "spark.cores" -&gt; "local[*]", "mongo.uri" -&gt; "mongodb://localhost:27017/recommender", "mongo.db" -&gt; "recommender" ) // 创建一个spark config val sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("OfflineRecommender") // 创建spark session val spark = SparkSession.builder().config(sparkConf).getOrCreate() import spark.implicits._ implicit val mongoConfig = MongoConfig( config("mongo.uri"), config("mongo.db") ) // 加载数据 val ratingRDD = spark.read .option("uri", mongoConfig.uri) .option("collection", MONGODB_RATING_COLLECTION) .format("com.mongodb.spark.sql") .load() .as[ProductRating] .rdd .map( rating =&gt; Rating(rating.userId, rating.productId, rating.score) ).cache() // 数据集切分成训练集和测试集 val splits = ratingRDD.randomSplit(Array(0.8, 0.2)) val trainingRDD = splits(0) val testingRDD = splits(1) // 核心实现：输出最优参数 adjustALSParams( trainingRDD, testingRDD ) spark.stop()&#125; def adjustALSParams(trainData: RDD[Rating], testData: RDD[Rating]): Unit =&#123; // 遍历数组中定义的参数取值 val result = for( rank &lt;- Array(5, 10, 20, 50); lambda &lt;- Array(1, 0.1, 0.01) ) yield &#123; val model = ALS.train(trainData, rank, 10, lambda) val rmse = getRMSE( model, testData ) ( rank, lambda, rmse )&#125; // 按照rmse排序并输出最优参数 println(result.minBy(_._3))&#125; def getRMSE(model: MatrixFactorizationModel, data: RDD[Rating]): Double = &#123; //构建UserProducts， 得到预测的评分矩阵 val userProducts: RDD[(Int, Int)] = data.map(item=&gt;(item.user,item.product)) val predictRating: RDD[Rating] = model.predict(userProducts) //按照公式计算 RMSE，首先把预测评分和实际评分表做一个连接 以（userID和productID）做一个连接 val observed : RDD[((Int, Int), Double)] = data.map(item=&gt;((item.user,item.product),item.rating)) val predict: RDD[((Int, Int), Double)] = predictRating.map(item=&gt;((item.user,item.product),item.rating)) sqrt(observed.join(predict).map&#123; case((userId,productId),(actual,pre)) =&gt; val error = actual -pre error*error &#125;.mean()) &#125;&#125; 5 实时推荐模块5.1 分析用户最近的偏好 之前买过什么商品 基于这样的思想 构建模型 不需要那么精确 基于模型的实时架构 mongo redis 日志 评分数据 userId productId 时间戳 flume kafka Stream 过滤 recommender spark streaming 定义实时推荐算法 用户最近的评分 redis 推荐优先级的计算 基本原理 用户最近一段时间的口味是相似的 最近的k次评分 好评 差评 5.1.2 推荐优先级计算 5.1.3 实现创建项目 pom.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&lt;dependencies&gt; &lt;!-- Spark的依赖引入 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 引入Scala --&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 加入MongoDB的驱动 --&gt; &lt;!-- 用于代码方式连接MongoDB --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;casbah-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;casbah.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 用于Spark和MongoDB的对接 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt; &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;mongodb-spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- redis --&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- kafka --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.10.2.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 实时系统的搭建 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213package com.atguigu.onlineimport com.mongodb.casbah.commons.MongoDBObjectimport com.mongodb.casbah.&#123;MongoClient, MongoClientURI&#125;import org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.SparkConfimport org.apache.spark.sql.SparkSessionimport org.apache.spark.streaming.kafka010.&#123;ConsumerStrategies, KafkaUtils, LocationStrategies&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import redis.clients.jedis.Jedis// 定义一个连接助手对象，建立到redis和mongodb的连接object ConnHelper extends Serializable&#123; // 懒变量定义，使用的时候才初始化 lazy val jedis = new Jedis("localhost") lazy val mongoClient = MongoClient(MongoClientURI("mongodb://linux:27017/recommender"))&#125;case class MongoConfig( uri: String, db: String )// 定义标准推荐对象case class Recommendation( productId: Int, score: Double )// 定义用户的推荐列表case class UserRecs( userId: Int, recs: Seq[Recommendation] )// 定义商品相似度列表case class ProductRecs( productId: Int, recs: Seq[Recommendation] )object OnlineRecommender &#123; // 定义常量和表名 val MONGODB_RATING_COLLECTION = "Rating" val STREAM_RECS = "StreamRecs" val PRODUCT_RECS = "ProductRecs" val MAX_USER_RATING_NUM = 20 val MAX_SIM_PRODUCTS_NUM = 20 def main(args: Array[String]): Unit = &#123; val config = Map( "spark.cores" -&gt; "local[*]", "mongo.uri" -&gt; "mongodb://localhost:27017/recommender", "mongo.db" -&gt; "recommender", "kafka.topic" -&gt; "recommender" ) // 创建spark conf val sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("OnlineRecommender") val spark = SparkSession.builder().config(sparkConf).getOrCreate() val sc = spark.sparkContext val ssc = new StreamingContext(sc, Seconds(2)) import spark.implicits._ implicit val mongoConfig = MongoConfig( config("mongo.uri"), config("mongo.db") ) // 加载数据，相似度矩阵，广播出去 val simProductsMatrix = spark.read .option("uri", mongoConfig.uri) .option("collection", PRODUCT_RECS) .format("com.mongodb.spark.sql") .load() .as[ProductRecs] .rdd // 为了后续查询相似度方便，把数据转换成map形式 .map&#123;item =&gt; ( item.productId, item.recs.map( x=&gt;(x.productId, x.score) ).toMap ) &#125; .collectAsMap() // 定义广播变量 val simProcutsMatrixBC = sc.broadcast(simProductsMatrix) // 创建kafka配置参数 val kafkaParam = Map( "bootstrap.servers" -&gt; "linux:9092", "key.deserializer" -&gt; classOf[StringDeserializer], "value.deserializer" -&gt; classOf[StringDeserializer], "group.id" -&gt; "recommender", "auto.offset.reset" -&gt; "latest" ) // 创建一个DStream val kafkaStream = KafkaUtils.createDirectStream[String, String]( ssc, LocationStrategies.PreferConsistent, ConsumerStrategies.Subscribe[String, String]( Array(config("kafka.topic")), kafkaParam ) ) // 对kafkaStream进行处理，产生评分流，userId|productId|score|timestamp val ratingStream = kafkaStream.map&#123;msg=&gt; var attr = msg.value().split("\\|") ( attr(0).toInt, attr(1).toInt, attr(2).toDouble, attr(3).toInt ) &#125; // 核心算法部分，定义评分流的处理流程 ratingStream.foreachRDD&#123; rdds =&gt; rdds.foreach&#123; case ( userId, productId, score, timestamp ) =&gt; println("rating data coming!&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;") // TODO: 核心算法流程 // 1. 从redis里取出当前用户的最近评分，保存成一个数组Array[(productId, score)] val userRecentlyRatings = getUserRecentlyRatings( MAX_USER_RATING_NUM, userId, ConnHelper.jedis ) // 2. 从相似度矩阵中获取当前商品最相似的商品列表，作为备选列表，保存成一个数组Array[productId] val candidateProducts = getTopSimProducts( MAX_SIM_PRODUCTS_NUM, productId, userId, simProcutsMatrixBC.value ) // 3. 计算每个备选商品的推荐优先级，得到当前用户的实时推荐列表，保存成 Array[(productId, score)] val streamRecs = computeProductScore( candidateProducts, userRecentlyRatings, simProcutsMatrixBC.value ) // 4. 把推荐列表保存到mongodb saveDataToMongoDB( userId, streamRecs ) &#125; &#125; // 启动streaming ssc.start() println("streaming started!") ssc.awaitTermination() &#125; /** * 从redis里获取最近num次评分 */ import scala.collection.JavaConversions._ def getUserRecentlyRatings(num: Int, userId: Int, jedis: Jedis): Array[(Int, Double)] = &#123; // 从redis中用户的评分队列里获取评分数据，list键名为uid:USERID，值格式是 PRODUCTID:SCORE jedis.lrange( "userId:" + userId.toString, 0, num ) .map&#123; item =&gt; val attr = item.split("\\:") ( attr(0).trim.toInt, attr(1).trim.toDouble ) &#125; .toArray &#125; // 获取当前商品的相似列表，并过滤掉用户已经评分过的，作为备选列表 def getTopSimProducts(num: Int, productId: Int, userId: Int, simProducts: scala.collection.Map[Int, scala.collection.immutable.Map[Int, Double]]) (implicit mongoConfig: MongoConfig): Array[Int] =&#123; // 从广播变量相似度矩阵中拿到当前商品的相似度列表 val allSimProducts = simProducts(productId).toArray // 获得用户已经评分过的商品，过滤掉，排序输出 val ratingCollection = ConnHelper.mongoClient( mongoConfig.db )( MONGODB_RATING_COLLECTION ) val ratingExist = ratingCollection.find( MongoDBObject("userId"-&gt;userId) ) .toArray .map&#123;item=&gt; // 只需要productId item.get("productId").toString.toInt &#125; // 从所有的相似商品中进行过滤 allSimProducts.filter( x =&gt; ! ratingExist.contains(x._1) ) .sortWith(_._2 &gt; _._2) .take(num) .map(x=&gt;x._1) &#125; // 计算每个备选商品的推荐得分 def computeProductScore(candidateProducts: Array[Int], userRecentlyRatings: Array[(Int, Double)], simProducts: scala.collection.Map[Int, scala.collection.immutable.Map[Int, Double]]) : Array[(Int, Double)] =&#123; // 定义一个长度可变数组ArrayBuffer，用于保存每一个备选商品的基础得分，(productId, score) val scores = scala.collection.mutable.ArrayBuffer[(Int, Double)]() // 定义两个map，用于保存每个商品的高分和低分的计数器，productId -&gt; count val increMap = scala.collection.mutable.HashMap[Int, Int]() val decreMap = scala.collection.mutable.HashMap[Int, Int]() // 遍历每个备选商品，计算和已评分商品的相似度 for( candidateProduct &lt;- candidateProducts; userRecentlyRating &lt;- userRecentlyRatings )&#123; // 从相似度矩阵中获取当前备选商品和当前已评分商品间的相似度 val simScore = getProductsSimScore( candidateProduct, userRecentlyRating._1, simProducts ) if( simScore &gt; 0.4 )&#123; // 按照公式进行加权计算，得到基础评分 scores += ( (candidateProduct, simScore * userRecentlyRating._2) ) if( userRecentlyRating._2 &gt; 3 )&#123; increMap(candidateProduct) = increMap.getOrDefault(candidateProduct, 0) + 1 &#125; else &#123; decreMap(candidateProduct) = decreMap.getOrDefault(candidateProduct, 0) + 1 &#125; &#125; &#125; // 根据公式计算所有的推荐优先级，首先以productId做groupby scores.groupBy(_._1).map&#123; case (productId, scoreList) =&gt; ( productId, scoreList.map(_._2).sum/scoreList.length + log(increMap.getOrDefault(productId, 1)) - log(decreMap.getOrDefault(productId, 1)) ) &#125; // 返回推荐列表，按照得分排序 .toArray .sortWith(_._2&gt;_._2) &#125; def getProductsSimScore(product1: Int, product2: Int, simProducts: scala.collection.Map[Int, scala.collection.immutable.Map[Int, Double]]): Double =&#123; simProducts.get(product1) match &#123; case Some(sims) =&gt; sims.get(product2) match &#123; case Some(score) =&gt; score case None =&gt; 0.0 &#125; case None =&gt; 0.0 &#125; &#125; // 自定义log函数，以N为底 def log(m: Int): Double = &#123; val N = 10 math.log(m)/math.log(N) &#125; // 写入mongodb def saveDataToMongoDB(userId: Int, streamRecs: Array[(Int, Double)])(implicit mongoConfig: MongoConfig): Unit =&#123; val streamRecsCollection = ConnHelper.mongoClient(mongoConfig.db)(STREAM_RECS) // 按照userId查询并更新 streamRecsCollection.findAndRemove( MongoDBObject( "userId" -&gt; userId ) ) streamRecsCollection.insert( MongoDBObject( "userId" -&gt; userId, "recs" -&gt; streamRecs.map(x=&gt;MongoDBObject("productId"-&gt;x._1, "score"-&gt;x._2)) ) ) &#125;&#125; 6 实时系统的联调KafkaStreaming模块的搭建 pom.xml 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-streams&lt;/artifactId&gt; &lt;version&gt;0.10.2.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.10.2.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;kafkastream&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.atguigu.kafkastream.Application&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 代码的实现 Application 12345678910111213141516171819202122232425262728293031323334353637383940package com.atguigu.kafkastream; import org.apache.kafka.streams.KafkaStreams;import org.apache.kafka.streams.StreamsConfig;import org.apache.kafka.streams.processor.TopologyBuilder;import java.util.Properties;public class Application &#123; public static void main(String[] args) &#123; String brokers = "linux:9092"; String zookeepers = "linux:2181"; // 定义输入和输出的topic String from = "log"; String to = "recommender"; // 定义kafka stream 配置参数 Properties settings = new Properties(); settings.put(StreamsConfig.APPLICATION_ID_CONFIG, "logFilter"); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, brokers); settings.put(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeepers); // 创建kafka stream 配置对象 StreamsConfig config = new StreamsConfig(settings); // 定义拓扑构建器 TopologyBuilder builder = new TopologyBuilder(); builder.addSource("SOURCE", from) .addProcessor("PROCESSOR", ()-&gt;new LogProcessor(), "SOURCE") .addSink("SINK", to, "PROCESSOR"); // 创建kafka stream KafkaStreams streams = new KafkaStreams( builder, config ); streams.start(); System.out.println("kafka stream started!"); &#125;&#125; LogProcessor 12345678910111213141516171819202122232425262728293031323334353637383940package com.atguigu.kafkastream;import org.apache.kafka.streams.processor.Processor;import org.apache.kafka.streams.processor.ProcessorContext;/** * @ClassName: LogProcessor * @Description: * @Author: wushengran on 2019/4/28 15:08 * @Version: 1.0 */public class LogProcessor implements Processor&lt;byte[], byte[]&gt;&#123; private ProcessorContext context; @Override public void init(ProcessorContext processorContext) &#123; this.context = processorContext; &#125; @Override public void process(byte[] dummy, byte[] line) &#123; // 核心处理流程 String input = new String(line); // 提取数据，以固定前缀过滤日志信息 if( input.contains("PRODUCT_RATING_PREFIX:") )&#123; System.out.println("product rating data coming! " + input); input = input.split("PRODUCT_RATING_PREFIX:")[1].trim(); context.forward("logProcessor".getBytes(), input.getBytes()); &#125; &#125; @Override public void punctuate(long l) &#123; &#125; @Override public void close() &#123; &#125;&#125; 6.2 配置启动Flumeflume的conf 目录下 123456789101112131415161718192021222324agent.sources = exectailagent.channels = memoryChannelagent.sinks = kafkasink# For each one of the sources, the type is definedagent.sources.exectail.type = exec# 下面这个路径是需要收集日志的绝对路径，改为自己的日志目录agent.sources.exectail.command = tail –f/mnt/d/Projects/BigData/ECommerceRecommenderSystem/businessServer/src/main/log/agent.logagent.sources.exectail.interceptors=i1agent.sources.exectail.interceptors.i1.type=regex_filter# 定义日志过滤前缀的正则agent.sources.exectail.interceptors.i1.regex=.+PRODUCT_RATING_PREFIX.+# The channel can be defined as follows.agent.sources.exectail.channels = memoryChannel# Each sink's type must be definedagent.sinks.kafkasink.type = org.apache.flume.sink.kafka.KafkaSinkagent.sinks.kafkasink.kafka.topic = logagent.sinks.kafkasink.kafka.bootstrap.servers = localhost:9092agent.sinks.kafkasink.kafka.producer.acks = 1agent.sinks.kafkasink.kafka.flumeBatchSize = 20#Specify the channel the sink should use 12 启动flume 1./bin/flume-ng agent -c ./conf/ -f ./conf/log-kafka.properties -n agent -Dflume.root.logger=INFO,console 启动zookeeper 1234bin/zkServer.sh start启动kafkabin/kafka-server-start.sh -daemon ./config/server.properties 测试实时推荐模块 连接的东西 先启动zookeeper 和kafka 在启动redis 然后redis的客户端 6 冷启动问题的解决实际项目中遇到的问题 冷启动的问题 算法是基于隐语义模型的 冷启动问题的处理 进来注册的时候 让你勾选项 了解即可 7 其他形式的离线相似推荐买了这个商品的他、用户 跟商品的相似 对应的相似的产品作出推荐 推荐算法分类的时候 用户画像 基于商品内容推荐 7.1 基于内容的相似度推荐id 名称 图片url 分类 ugc标签 主要基于UGC标签 创建项目 pom文件 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scalanlp&lt;/groupId&gt; &lt;artifactId&gt;jblas&lt;/artifactId&gt; &lt;version&gt;$&#123;jblas.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Spark的依赖引入 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-mllib_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 引入Scala --&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 加入MongoDB的驱动 --&gt; &lt;!-- 用于代码方式连接MongoDB --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;casbah-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;casbah.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 用于Spark和MongoDB的对接 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt; &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;mongodb-spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 实现代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108package com.atguigu.contentimport org.apache.spark.SparkConfimport org.apache.spark.ml.feature.&#123;HashingTF, IDF, Tokenizer&#125;import org.apache.spark.ml.linalg.SparseVectorimport org.apache.spark.sql.SparkSessionimport org.jblas.DoubleMatrixcase class Product( productId: Int, name: String, imageUrl: String, categories: String, tags: String )case class MongoConfig( uri: String, db: String )// 定义标准推荐对象case class Recommendation( productId: Int, score: Double )// 定义商品相似度列表case class ProductRecs( productId: Int, recs: Seq[Recommendation] )object ContentRecommender &#123; // 定义mongodb中存储的表名 val MONGODB_PRODUCT_COLLECTION = "Product" val CONTENT_PRODUCT_RECS = "ContentBasedProductRecs" def main(args: Array[String]): Unit = &#123; val config = Map( "spark.cores" -&gt; "local[*]", "mongo.uri" -&gt; "mongodb://linux:27017/recommender", "mongo.db" -&gt; "recommender" ) // 创建一个spark config val sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("ContentRecommender") // 创建spark session val spark = SparkSession.builder().config(sparkConf).getOrCreate() import spark.implicits._ implicit val mongoConfig = MongoConfig( config("mongo.uri"), config("mongo.db") ) // 载入数据，做预处理 val productTagsDF = spark.read .option("uri", mongoConfig.uri) .option("collection", MONGODB_PRODUCT_COLLECTION) .format("com.mongodb.spark.sql") .load() .as[Product] .map( x =&gt; ( x.productId, x.name, x.tags.map(c=&gt; if(c=='|') ' ' else c) ) ) .toDF("productId", "name", "tags") .cache() // TODO: 用TF-IDF提取商品特征向量 // 1. 实例化一个分词器，用来做分词，默认按照空格分 val tokenizer = new Tokenizer().setInputCol("tags").setOutputCol("words") // 用分词器做转换，得到增加一个新列words的DF val wordsDataDF = tokenizer.transform(productTagsDF) // 2. 定义一个HashingTF工具，计算频次 val hashingTF = new HashingTF().setInputCol("words").setOutputCol("rawFeatures").setNumFeatures(800) val featurizedDataDF = hashingTF.transform(wordsDataDF) // 3. 定义一个IDF工具，计算TF-IDF val idf = new IDF().setInputCol("rawFeatures").setOutputCol("features") // 训练一个idf模型 val idfModel = idf.fit(featurizedDataDF) // 得到增加新列features的DF val rescaledDataDF = idfModel.transform(featurizedDataDF) // 对数据进行转换，得到RDD形式的features val productFeatures = rescaledDataDF.map&#123; row =&gt; ( row.getAs[Int]("productId"), row.getAs[SparseVector]("features").toArray ) &#125; .rdd .map&#123; case (productId, features) =&gt; ( productId, new DoubleMatrix(features) ) &#125; // 两两配对商品，计算余弦相似度 val productRecs = productFeatures.cartesian(productFeatures) .filter&#123; case (a, b) =&gt; a._1 != b._1 &#125; // 计算余弦相似度 .map&#123; case (a, b) =&gt; val simScore = consinSim( a._2, b._2 ) ( a._1, ( b._1, simScore ) ) &#125; .filter(_._2._2 &gt; 0.4) .groupByKey() .map&#123; case (productId, recs) =&gt; ProductRecs( productId, recs.toList.sortWith(_._2&gt;_._2).map(x=&gt;Recommendation(x._1,x._2)) ) &#125; .toDF() productRecs.write .option("uri", mongoConfig.uri) .option("collection", CONTENT_PRODUCT_RECS) .mode("overwrite") .format("com.mongodb.spark.sql") .save() spark.stop() &#125; def consinSim(product1: DoubleMatrix, product2: DoubleMatrix): Double =&#123; product1.dot(product2)/ ( product1.norm2() * product2.norm2() ) &#125;&#125; 7.2 基于ItemCF的推荐算法 123456789101112131415161718192021222324252627282930&lt;dependencies&gt; &lt;!-- Spark的依赖引入 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 引入Scala --&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 加入MongoDB的驱动 --&gt; &lt;!-- 用于代码方式连接MongoDB --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;casbah-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;casbah.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 用于Spark和MongoDB的对接 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt; &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;mongodb-spark.version&#125;&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109package com.atguigu.itemcfimport org.apache.spark.SparkConfimport org.apache.spark.sql.SparkSessioncase class ProductRating( userId: Int, productId: Int, score: Double, timestamp: Int )case class MongoConfig( uri: String, db: String )// 定义标准推荐对象case class Recommendation( productId: Int, score: Double )// 定义商品相似度列表case class ProductRecs( productId: Int, recs: Seq[Recommendation] )object ItemCFRecommender &#123; // 定义常量和表名 val MONGODB_RATING_COLLECTION = "Rating" val ITEM_CF_PRODUCT_RECS = "ItemCFProductRecs" val MAX_RECOMMENDATION = 10 def main(args: Array[String]): Unit = &#123; val config = Map( "spark.cores" -&gt; "local[*]", "mongo.uri" -&gt; "mongodb://linux:27017/recommender", "mongo.db" -&gt; "recommender" ) // 创建一个spark config val sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("ItemCFRecommender") // 创建spark session val spark = SparkSession.builder().config(sparkConf).getOrCreate() import spark.implicits._ implicit val mongoConfig = MongoConfig( config("mongo.uri"), config("mongo.db") ) // 加载数据，转换成DF进行处理 val ratingDF = spark.read .option("uri", mongoConfig.uri) .option("collection", MONGODB_RATING_COLLECTION) .format("com.mongodb.spark.sql") .load() .as[ProductRating] .map( x =&gt; ( x.userId, x.productId, x.score ) ) .toDF("userId", "productId", "score") .cache() // TODO: 核心算法，计算同现相似度，得到商品的相似列表 // 统计每个商品的评分个数，按照productId来做group by val productRatingCountDF = ratingDF.groupBy("productId").count() // 在原有的评分表上rating添加count val ratingWithCountDF = ratingDF.join(productRatingCountDF, "productId") // 将评分按照用户id两两配对，统计两个商品被同一个用户评分过的次数 val joinedDF = ratingWithCountDF.join(ratingWithCountDF, "userId") .toDF("userId","product1","score1","count1","product2","score2","count2") .select("userId","product1","count1","product2","count2") // 创建一张临时表，用于写sql查询 joinedDF.createOrReplaceTempView("joined") // 按照product1,product2 做group by，统计userId的数量，就是对两个商品同时评分的人数 val cooccurrenceDF = spark.sql( """ |select product1 |, product2 |, count(userId) as cocount |, first(count1) as count1 |, first(count2) as count2 |from joined |group by product1, product2 """.stripMargin ).cache() // 提取需要的数据，包装成( productId1, (productId2, score) ) val simDF = cooccurrenceDF.map&#123; row =&gt; val coocSim = cooccurrenceSim( row.getAs[Long]("cocount"), row.getAs[Long]("count1"), row.getAs[Long]("count2") ) ( row.getInt(0), ( row.getInt(1), coocSim ) ) &#125; .rdd .groupByKey() .map&#123; case (productId, recs) =&gt; ProductRecs( productId, recs.toList .filter(x=&gt;x._1 != productId) .sortWith(_._2&gt;_._2) .take(MAX_RECOMMENDATION) .map(x=&gt;Recommendation(x._1,x._2)) ) &#125; .toDF() // 保存到mongodb simDF.write .option("uri", mongoConfig.uri) .option("collection", ITEM_CF_PRODUCT_RECS) .mode("overwrite") .format("com.mongodb.spark.sql") .save() spark.stop() &#125; // 按照公式计算同现相似度 def cooccurrenceSim(coCount: Long, count1: Long, count2: Long): Double =&#123; coCount / math.sqrt( count1 * count2 ) &#125;&#125;]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>项目实战</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java面试]]></title>
    <url>%2Fblog4%2F2019%2F10%2F08%2Fjava%2F</url>
    <content type="text"><![CDATA[1 各大厂的面试题1.1 蚂蚁花呗一个小时 1.2美团的一面 垃圾回收器 1.3 百度 java集合类 synchronized 什么是对象锁 什么是全局锁 1.4 头条 1.5 美团的面试汇总 1.6 蚂蚁金服二面 1.7 讲解1 关于2018.12 月份 ，。互联网公司大规模的缩招 裁员 缩招不是不招聘 而是招聘更多的优质的咖啡啊工程师 2 将最近半年的大厂面试题进行了整理和划分 1 1.8 3 1 个人 1.8倍的工资 干三个人的活 3 第一次 提出高频最多的常见笔试面试题目 ArrayList HashMap 底层是什么东西 4 JVM/GC 多线程与高并发 1.8 技术框架 大厂的面试题 90% 1.9 redis的相关题目 哪些数据存mysql 哪些 redis 如何保持移植性 redis缓存给了多大的总内存 命中率多高 超大Value打满网卡的问题 1.10 消息中间件MQrabibtMQ 消息中间件 消息积压了两个小时 消息中间件只有一个 挂掉 影响业务 1.11 JVM+GC的解析 oom 的东西 了解 Out of Memory Error java内存溢出 四大引用 强 软 弱 虚 水平 泯然众人与 性能检测工具 1.12 JUC多线程及高并发1.13 面试重点jvm + GC JUC多线程高并发 本次讲解 互联网笔试题第二季 JVM/GC的知识 JUC的前提只是 超级熟悉java8的与、新特性 （Stream+lambdaExpress+函数接口+方法引用） 2 JUC多线程及高并发 current 并发 高并发 秒杀 多个线程访问 统一个资源 并行 各种事情一路并行去做 节水 道调料 atomic 院原子性 AtomicInteger 原子引用 2.1 请你谈谈你对volatile的理解 同步 synchronized 轻量级 什么是轻 三大特性 volatile是轻量级的同步机制 不是文科 理解 jMM关于同步的 2.1.1 JMM值内存可见性 volatile 可见性1 理论线程 —-》》》 工作内存 —-》》 每个线程的私有内存区域 变量 —-》》 主内存 —》》 共享内存区域 线程可以访问 t1改成37 了 t2 t3不知道 1 t1 拷走25 2 t1改成37 3 把37 写回主内存 线程2 和线程3 不知道 只要有一个线程修改完自己的工作空间值之后写回主内存 及时通知其他的线程 这种机制 JMM之内存模型的第一个特性可见性 主内存的值只要被修改 其他的线程马上获得通知 改课 干活 —》》线程 自己的工作内存 私有数据 new 3 个线程 java内存模型规定所有的变量在主内训 主内存 拷贝到自己的工作内存空间 不同的线程无法访问对方的工作内存 线程间的通信（传值） 必须靠主内存来完成 拷贝共享变量的初始值 线程的工作内存里面 西城区和东城区的售票员 电话确认不可能 线程运算完 写回主内存 可见性 有了最新消息 第一个通知 论证 2 代码灭有volatile的操作 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/8 * @描述： */import java.util.concurrent.TimeUnit;class MyData&#123; int number=0; public void addTO60() &#123; this.number=60; &#125;&#125;/** 1 验证volatile的可见性* 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰** */public class VolatileDemo &#123; public static void main(String[] args) &#123; //main是一切方法的运行入口 MyData myData=new MyData();//资源类 new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+"\t come in"); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; myData.addTO60(); System.out.println(Thread.currentThread().getName()+ "\t updated number value:"+ myData.number); &#125;,"AAA").start(); //第二个线程就是我们的main线程 while (myData.number==0) &#123; //main线程一直在这里等待循环 直到这个number不在等于0 没有可见性 &#125; System.out.println(Thread.currentThread().getName()+"\t misson is over"); &#125;&#125; 最后一句没有打印 没有人通知 main线程傻傻的等待 没有人通知我改了 我不知道 加上volatile的代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/8 * @描述： */import java.util.concurrent.TimeUnit;class MyData&#123; volatile int number=0; public void addTO60() &#123; this.number=60; &#125;&#125;/** 1 验证volatile的可见性* 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰** */public class VolatileDemo &#123; public static void main(String[] args) &#123; //main是一切方法的运行入口 MyData myData=new MyData();//资源类 new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+"\t come in"); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; myData.addTO60(); System.out.println(Thread.currentThread().getName()+ "\t updated number value:"+ myData.number); &#125;,"AAA").start(); //第二个线程就是我们的main线程 while (myData.number==0) &#123; //main线程一直在这里等待循环 直到这个number不在等于0 没有可见性 &#125; System.out.println(Thread.currentThread().getName()+"\t misson is over"+"main get value:"+myData.number); &#125;&#125; 可见性证明 缓存 JMM的一种内存抽象机制 抽象的概念 再次阅读 轻量级 乞丐版的synchronized 2.1.2 JMM之原子性 volatile不保证原子性volatile是不保证原子性的 1 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/8 * @描述： */import java.util.concurrent.TimeUnit;class MyData&#123; volatile int number=0; public void addTO60() &#123; this.number=60; &#125; /* 此时number的前面是加了voatile关键字的修饰的，volatile不保证原子性 */public void addPlus()&#123; number++;&#125;&#125;/** 1 验证volatile的可见性* 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰** */public class VolatileDemo &#123;//main一切方法的入口 public static void main(String[] args) &#123; MyData myData=new MyData(); for (int i = 1; i &lt;=20 ; i++) &#123; new Thread(()-&gt;&#123; for (int j = 1; j &lt;=1000 ; j++) &#123; myData.addPlus(); &#125; &#125;,String.valueOf(i) ).start(); &#125; //等待上面的20个线程全部计算完成 再用main线程取得的最终的结果值是什么 //后台的线程 /*1 main 2 后台GC线程 * */ while(Thread.activeCount()&gt;2) &#123; Thread.yield(); &#125; System.out.println(Thread.currentThread().getName()+"\t finally n umber value: "+ myData.number); &#125; /*volatile可以保证可见性 可以通知其线程主物理内存的值已经被修改 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰 2 验证volatile不保证原子性 2.1 原子性指的是什么意思？ 不可分割 完整性 也即某个线程在做某个具体的业务的时候 中间不可以被加塞或者分割 需要整体完整 要么同时成功 要么同时失败 一段时间内只允许一个操作 2.2 volatile 是否可以保证原子性 */ public static void seeOkByVolatile() &#123; MyData myData=new MyData();//资源类 new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+"\t come in"); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; myData.addTO60(); System.out.println(Thread.currentThread().getName()+ "\t updated number value:"+ myData.number); &#125;,"AAA").start(); //第二个线程就是我们的main线程 while (myData.number==0) &#123; //main线程一直在这里等待循环 直到这个number不在等于0 没有可见性 &#125; System.out.println(Thread.currentThread().getName()+"\t misson is over"+"main get value:"+myData.number); &#125;&#125; 出来的不是2万 说明volatile不能够保证原子性 有没有可能加到2万 但是有极其特殊的情况是可能达到2万的 加了synchronized 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/8 * @描述： */import java.util.concurrent.TimeUnit;class MyData&#123; volatile int number=0; public void addTO60() &#123; this.number=60; &#125; /* 此时number的前面是加了voatile关键字的修饰的，volatile不保证原子性 */public synchronized void addPlus()&#123; number++;&#125;&#125;/** 1 验证volatile的可见性* 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰** */public class VolatileDemo &#123;//main一切方法的入口 public static void main(String[] args) &#123; MyData myData=new MyData(); for (int i = 1; i &lt;=20 ; i++) &#123; new Thread(()-&gt;&#123; for (int j = 1; j &lt;=1000 ; j++) &#123; myData.addPlus(); &#125; &#125;,String.valueOf(i) ).start(); &#125; //等待上面的20个线程全部计算完成 再用main线程取得的最终的结果值是什么 //后台的线程 /*1 main 2 后台GC线程 * */ while(Thread.activeCount()&gt;2) &#123; Thread.yield(); &#125; System.out.println(Thread.currentThread().getName()+"\t finally n umber value: "+ myData.number); &#125; /*volatile可以保证可见性 可以通知其线程主物理内存的值已经被修改 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰 2 验证volatile不保证原子性 2.1 原子性指的是什么意思？ 不可分割 完整性 也即某个线程在做某个具体的业务的时候 中间不可以被加塞或者分割 需要整体完整 要么同时成功 要么同时失败 一段时间内只允许一个操作 2.2 volatile 不保证原子性的案例的演示 */ public static void seeOkByVolatile() &#123; MyData myData=new MyData();//资源类 new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+"\t come in"); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; myData.addTO60(); System.out.println(Thread.currentThread().getName()+ "\t updated number value:"+ myData.number); &#125;,"AAA").start(); //第二个线程就是我们的main线程 while (myData.number==0) &#123; //main线程一直在这里等待循环 直到这个number不在等于0 没有可见性 &#125; System.out.println(Thread.currentThread().getName()+"\t misson is over"+"main get value:"+myData.number); &#125;&#125; 所以说volatile是 轻量级的同步机制 java c java verbose 12345678910public class T1 &#123;volatile int n =0;public void add()&#123; n++;&#125;&#125; 写覆盖 1 1 1 操作了3次 加1 各自的工作内存加1 由于synchronized 不能保证原子性 getfield iadd putfield 你先写 你先写 原子性 没有写完的时候 另外一个线程已经被唤醒 putfield 可能线程写入 丢失 后面的线程可能会把前面的线程写覆盖掉 JMM内存模型要求保证原子性 volatile不保证原子性 运行程序保证是2万 方法1 synchronized xxx 功能|+性能 原子包装的整型类 AtomicInteger的引入 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/8 * @描述： */import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicInteger;class MyData&#123; //MyData.java ===&gt; MyData.class =====&gt; jvm字节码 volatile int number=0; public void addTO60() &#123; this.number=60; &#125; /* 此时number的前面是加了voatile关键字的修饰的，volatile不保证原子性 */public void addPlus()&#123; number++;&#125;AtomicInteger atomicInteger=new AtomicInteger(); public void addAtomic() &#123;atomicInteger.getAndIncrement(); &#125;&#125;/** 1 验证volatile的可见性* 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰** */public class VolatileDemo &#123;//main一切方法的入口 public static void main(String[] args) &#123; MyData myData=new MyData(); for (int i = 1; i &lt;=20 ; i++) &#123; new Thread(()-&gt;&#123; for (int j = 1; j &lt;=1000 ; j++) &#123; myData.addPlus(); myData.addAtomic(); &#125; &#125;,String.valueOf(i) ).start(); &#125; //等待上面的20个线程全部计算完成 再用main线程取得的最终的结果值是什么 //后台的线程 /*1 main 2 后台GC线程 * */ while(Thread.activeCount()&gt;2) &#123; Thread.yield(); &#125; System.out.println(Thread.currentThread().getName()+"\t finally n umber value: "+ myData.number); System.out.println(Thread.currentThread().getName()+"\t AtomicInteger type finally n umber value: "+ myData.atomicInteger); &#125; /*volatile可以保证可见性 可以通知其线程主物理内存的值已经被修改 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰 2 验证volatile不保证原子性 2.1 原子性指的是什么意思？ 不可分割 完整性 也即某个线程在做某个具体的业务的时候 中间不可以被加塞或者分割 需要整体完整 要么同时成功 要么同时失败 一段时间内只允许一个操作 2.2 volatile 不保证原子性的案例的演示 */ public static void seeOkByVolatile() &#123; MyData myData=new MyData();//资源类 new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+"\t come in"); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; myData.addTO60(); System.out.println(Thread.currentThread().getName()+ "\t updated number value:"+ myData.number); &#125;,"AAA").start(); //第二个线程就是我们的main线程 while (myData.number==0) &#123; //main线程一直在这里等待循环 直到这个number不在等于0 没有可见性 &#125; System.out.println(Thread.currentThread().getName()+"\t misson is over"+"main get value:"+myData.number); &#125;&#125; 2.1.3 volatile指令重排 编译器 和处理器 常常做指令重排 源代码 编译器的优化的重排 指令并行的重排 内存系统的重排 最终执行的命令 数据依赖性 单线程 —-》》 多线程 源代码 12345678 底层 不一定是按照这个顺序 而是多线程环境 答题的顺序和 重排1 语句四存在数据的依赖性不能排到第一条 重排2 非计算机的了解即可 volatile禁止指令重排 线程的安全得到保证 a的前面加不加volatile a拿到的不是1 是 0 单线程环境不用担心指令重排 多线程 单线程编译器优化 编译器优化 指令并行重排的优化 内存系统的重排 3个重排 volatile可见性 Volatile的三大特性的讲解 JUC的包里面大规模使用到了单例模式 2.1.4 volatile的单例模式1单机版的单线程 12345678910111213141516171819202122232425262728293031323334353637383940package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/9 * @描述： */import javafx.scene.SnapshotParameters;/** 单机版的单线程* */public class SingleDemo &#123; private static SingleDemo instance=null; private SingleDemo()&#123; System.out.println(Thread.currentThread().getName()+"\t 我是构造方法Singledemo()" ); &#125; public static SingleDemo getInstance() &#123; if(instance==null) &#123; instance=new SingleDemo(); &#125; return instance; &#125; public static void main(String[] args) &#123; System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance()); System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance()); System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance()); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/9 * @描述： */import javafx.scene.SnapshotParameters;/** 单机版的单线程* */public class SingleDemo &#123; private static SingleDemo instance=null; private SingleDemo()&#123; System.out.println(Thread.currentThread().getName()+"\t 我是构造方法Singledemo()" ); &#125; public static SingleDemo getInstance() &#123; if(instance==null) &#123; instance=new SingleDemo(); &#125; return instance; &#125; public static void main(String[] args) &#123;// System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance());// System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance());// System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance());//// System.out.println();// System.out.println();// System.out.println(); for (int i = 0; i &lt;10 ; i++) &#123; new Thread(()-&gt; &#123; SingleDemo.getInstance(); &#125;,String.valueOf(i)).start(); &#125; &#125;&#125; 多线程 10个现在变成 6条 加上synchronized 解决问题 synchronized 整个代码都锁了 加上synchronized单例volatile的解析DCL（双端检测）介绍DCL的单例模式 高并发的环境下企业推崇的 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/9 * @描述： */import javafx.scene.SnapshotParameters;/** 单机版的单线程* */public class SingleDemo &#123; /* * 保证在多线程的指令不重排 * */ private static volatile SingleDemo instance=null; private SingleDemo()&#123; System.out.println(Thread.currentThread().getName()+"\t 我是构造方法Singledemo()" ); &#125; //DCL 模式 Double Check Lock 双端检索机制 //加锁前判断 //加锁判断 //在多线程的条件下 底层有指令重排 //如果没有控制好指令重排 public static SingleDemo getInstance() &#123; if (instance == null) &#123; synchronized (SingleDemo.class) &#123; if (instance == null) &#123; instance = new SingleDemo(); &#125; &#125; &#125; return instance; &#125; public static void main(String[] args) &#123;// System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance());// System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance());// System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance());//// System.out.println();// System.out.println();// System.out.println(); for (int i = 0; i &lt;10 ; i++) &#123; new Thread(()-&gt; &#123; SingleDemo.getInstance(); &#125;,String.valueOf(i)).start(); &#125; &#125;&#125; 先获得 再去加 i++ 2.2 CAS1 比较并交换什么是比较和交换呢 期望值和物理内存的真实值一样 修改为更新值 期望值和物理内存的真实值不一样 重新获得真实值 CASDemo源码123456789101112131415161718192021222324252627282930package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/9 * @描述： *//** CAS 是什么？ ==》》 compare and set* 先比较后交换* 比较并交换** */import java.util.concurrent.atomic.AtomicInteger;public class CASDemo &#123; public static void main(String[] args) &#123; AtomicInteger atomicInteger=new AtomicInteger(5); //劳动成果写进主物理内存 System.out.println(atomicInteger.compareAndSet(5,2019 )+"\t current data:"+atomicInteger.get()); System.out.println(); System.out.println(atomicInteger.compareAndSet(5,1024 )+"\t current data:"+atomicInteger.get()); System.out.println(); &#125;&#125; 结果 compare and set 同 修改成功 不同 修改失败 为什么用synchronized 不用CAS CAS原理 谈谈unsafe类1.UnSafe 是CAS的核心类 由于Java 方法无法直接访问底层 ,需要通过本地(native)方法来访问,UnSafe相当于一个后面,基于该类可以直接操作特额定的内存数据.UnSafe类在于sun.misc包中,其内部方法操作可以向C的指针一样直接操作内存,因为Java中CAS操作的助兴依赖于UNSafe类的方法. 2.变量ValueOffset,便是该变量在内存中的偏移地址,因为UnSafe就是根据内存偏移地址获取数据的 3.变量value和volatile修饰,保证了多线程之间的可见性. 什么是CASCAS的全称为Compare-And-Swap ,它是一条CPU并发原语.它的功能是判断内存某个位置的值是否为预期值,如果是则更新为新的值,这个过程是原子的. CAS并发原语提现在Java语言中就是sun.miscUnSaffe类中的各个方法.调用UnSafe类中的CAS方法,JVM会帮我实现CAS汇编指令.这是一种完全依赖于硬件 功能,通过它实现了原子操作,再次强调,由于CAS是一种系统原语,原语属于操作系统用于范畴,是由若干条指令组成,用于完成某个功能的一个过程,并且原语的执行必须是连续的,在执行过程中不允许中断,也即是说CAS是一条原子指令,不会造成所谓的数据不一致的问题. ABA问题 集合内的线程不安全问题 CAS的缺点 时间开销大 有个do whie 只能保证一个共享变量的原子性 ​ 一个变量可以保证原子性，多个变量可以靠锁来保证原子性 引发出的ABA问题 2.3 原子AtomicInteger的ABA问题 谈谈ABA问题的产生CAS算法实现一个重要的前提取出内存中的某个时刻的数据并比较替换，那么在这个时间差里会导致数据的变化 线程one 在位置V取出A 线程 two在V位置取出A two，线程B进行了一些操作将数值变成了B，然后线程two又将V位置的数据编程A， 这个时候线程one 进CAS操作发现内存中仍然是A，然后one操作成功 原子引用 AtomicReference12345678910111213141516171819202122232425262728293031323334353637import lombok.AllArgsConstructor;import lombok.Getter;import lombok.Setter;import lombok.ToString;import java.util.concurrent.atomic.AtomicReference;@Setter@Getter@AllArgsConstructor@ToStringclass User &#123; String userName; int age;&#125;public class AtomicReferenceDemo &#123; public static void main(String[] args) throws InterruptedException &#123; //建立用户zs User zs = new User("zs", 22); //用户ls User ls = new User("ls", 25); //原子引用 AtomicReference&lt;User&gt; userAtomicReference=new AtomicReference&lt;&gt;(); userAtomicReference.set(zs); System.out.println(userAtomicReference.compareAndSet(zs,ls)+"\t"+userAtomicReference.get().toString()); // System.out.println(userAtomicReference.compareAndSet(zs, ls)+"\t"+userAtomicReference.get().toString()); &#125;&#125; 时间戳原子引用AtomicStampedReference中间不知道修改了多少次 修改的时候加时间 新增一种机制 修改版本号（类似时间戳） T1 100 1 T2 100 1 101 2 100 3 ABA 乐观锁 自己的值总是不会被改变 自己很自信 JUC工具包 AtomicStampedReference ABA问题的解决1234/** * An &#123;@code AtomicStampedReference&#125; maintains an object reference * along with an integer "stamp", that can be updated atomically. */ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/10 * @描述： *//*开始是100到101101 到* */import org.omg.PortableInterceptor.INACTIVE;import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicReference;import java.util.concurrent.atomic.AtomicStampedReference;public class ABADemo &#123; static AtomicReference&lt;Integer&gt; atomicReference = new AtomicReference&lt;&gt;(100); static AtomicStampedReference&lt;Integer&gt; atomicStampedReference = new AtomicStampedReference&lt;&gt;(100, 1); public static void main(String[] args) &#123; System.out.println("=====以下是ABA问题的产生======================"); new Thread(() -&gt; &#123; //期望值真实值一样改成101 atomicReference.compareAndSet(100, 101); atomicReference.compareAndSet(101, 100); &#125;, "t1").start(); new Thread(() -&gt; &#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //暂停一秒钟 保证上面的1线程完成了一次ABA操作 System.out.println(atomicReference.compareAndSet(100, 2019) + "\t" + atomicReference.get()); &#125;, "t2").start(); //暂停一会进程 try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("======以下是ABA问题的解决========="); /* * * 100 101 100 * 1 2 3 * */ new Thread(() -&gt; &#123; int stamp = atomicStampedReference.getStamp(); System.out.println(Thread.currentThread().getName() + "\t第1次版本号" + stamp); //暂停1s t3线程 try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //期望值 更新值 期望的版本号 更新的版本号 atomicStampedReference.compareAndSet(100,101,atomicStampedReference.getStamp(),atomicStampedReference.getStamp()+1); System.out.println(Thread.currentThread().getName() + "\t第2次版本号" + atomicStampedReference.getStamp()); atomicStampedReference.compareAndSet(101,100,atomicStampedReference.getStamp(),atomicStampedReference.getStamp()+1); System.out.println(Thread.currentThread().getName() + "\t第3次版本号" +atomicStampedReference.getStamp()); &#125;, "t3").start(); /* * * 期望值100 2019 * * 1 想改成2 * */ new Thread(() -&gt; &#123; int stamp = atomicStampedReference.getStamp(); System.out.println(Thread.currentThread().getName() + "\t第1次版本号" + stamp); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; boolean result=atomicStampedReference.compareAndSet(100,2019,stamp,stamp+1); System.out.println(Thread.currentThread().getName() + "\t修改成功否：" +result+"\t当前最新实际版本号"+atomicStampedReference.getStamp()); System.out.println(Thread.currentThread().getName()+"当前实际最新值:"+atomicStampedReference.getReference()); &#125;,"t4").start(); &#125;&#125; 2.4 ArryList是线程不安全的，请大家编写一个 3 集合类ArrayListArrayList 前身是Vector vector 有synchronized Collections接口 Collection API的熟练运用程度 123public HashSet(int initialCapacity) &#123; map = new HashMap&lt;&gt;(initialCapacity); &#125; hashSet的底层是Map 为什么一个装1 一个装2 因为 hasSet添加的时候前面是key 后面是一个Object对象 Object对象是null的 在多线程的情况下的时候 我们使用ArrayList会出现 java.util.ConcurrentMoidificationException 这个异常 我们的解决策略有3个 1 new Vector Vector在ArrayList之前就出现了 那么ArrayList出现的意义是什么呢 Vector 保证了安全性 但是却使用了synchronized加锁，虽然保证了一定的安全性，但是并发性却下降了 2 Collections.synchronize3dList(new ArrayList) Collection 集合的接口 Collections 集合接口的一些补充 有高并发的条件下为ArrayList实现线程安全的类 写时复制 3 new CopyOnWriteArrayList() 集合类不安全之Set集合类不安全之Map2.4 Transfervalue醒脑小练习栈运行 堆存储 main方法的20复印了一份 去执行栈中的方法changeValue1 最后存在main方法中的值还是20 方法的作域和jvm的分布 基本类型传的是复印件 引用类型传递的是地址 person的值被改过了 String str=”abc”; test.changeValue3(“String…….”+str) String的特殊性 一种是字符串常量池 去池子里面检查有没有abc String特殊 String去找xxx 没有新建xxx 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/10 * @描述： */import com.mlz.study.entity.Person;import java.util.concurrent.TransferQueue;public class TestTransferValue &#123; public void changeValue1(int age) &#123; age = 30; &#125; private void changeValue2(Person str) &#123; str.setPersonName("xxx"); &#125; private void changeValue3(String str) &#123; str = "xxx"; &#125; /** * 栈运行 * 堆 存储 */ public static void main(String[] args) &#123; TestTransferValue test = new TestTransferValue(); int age = 20; test.changeValue1(age); System.out.println("age-----" + age); Person person = new Person("abc"); test.changeValue2(person); System.out.println("personName-----" + person.getPersonName()); String str = "abc"; test.changeValue3(str); System.out.println("String........." + str); &#125;&#125; 分析图片 2.5 java锁之公平锁和非公平锁公平锁/非公平锁/可重入锁/递归锁/自旋锁谈谈你的理解?请手写一个自旋锁 order policy 排序策略 公平锁 队列 先来后到 课间休息 默认非公平 加塞加上 加不上 非公平锁 加塞提问 快 定义公平锁 是指多个线程按照申请锁的顺序来获取锁类似排队打饭 先来后到非公平锁 是指在多线程获取锁的顺序并不是按照申请锁的顺序,有可能后申请的线程比先申请的线程优先获取到锁,在高并发的情况下,有可能造成优先级反转或者饥饿现象 反转 饥饿的理解 两者的区别公平锁性能下降 公平锁/非公平锁 并发包ReentrantLock的创建可以指定构造函数的boolean类型来得到公平锁或者非公平锁 默认是非公平锁 公平锁 先来后到 非公平锁 先抢先得 性能提升 题外话Java ReentrantLock而言,通过构造哈数指定该锁是否是公平锁 默认是非公平锁 非公平锁的优点在于吞吐量必公平锁大. 对于synchronized而言 也是一种非公平锁. 2.6 可重入锁（递归锁）是什么 京津冀黔 生活 12345678910111213public sync void method01()&#123;method02()&#125;public sync void method02()&#123;&#125; method01() 就是大门 但是里面的锁是不上的 默认是非公平的可重入锁 method01（）本身是同步 method2（） 又是同步 内层递归函数能获取method01的方法 ReentrantLock/synchronized就是一个典型的可重入锁知识代码说明 代码部分 synchronized 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/10 * @描述： *//* * 线程操作资源类 * *//** 同一个线程访问两个同步方法 但是他们确实访问同一把锁* 外层synchronized 内层的synchronized* */class Phone &#123; public synchronized void sendSMS() throws Exception &#123; System.out.println(Thread.currentThread().getName() + "\t invoked sendSMS"); sendEmail(); &#125; public synchronized void sendEmail() throws Exception &#123; System.out.println(Thread.currentThread().getName() + "\t #####invoked sendEmail"); &#125;&#125;/* * 外层函数获得锁之后 内层递归函数仍然能够获取该锁的代码 * * */public class ReenterLockDemo &#123; public static void main(String[] args) &#123; Phone phone = new Phone(); new Thread(() -&gt; &#123; try &#123; phone.sendSMS(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, "t1").start(); new Thread(() -&gt; &#123; try &#123; phone.sendSMS(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, "t2").start(); &#125;&#125; ReentrantLock 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/10 * @描述： *//* * 线程操作资源类 * */import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/* * 同一个线程访问两个同步方法 但是他们确实访问同一把锁 * 外层synchronized 内层的synchronized * * case two * * ReentLock就是一个典型的可重入锁 * */class Phone implements Runnable &#123; public synchronized void sendSMS() throws Exception &#123; System.out.println(Thread.currentThread().getName() + "\t invoked sendSMS"); sendEmail(); &#125; public synchronized void sendEmail() throws Exception &#123; System.out.println(Thread.currentThread().getName() + "\t #####invoked sendEmail"); &#125; Lock lock = new ReentrantLock(); @Override public void run() &#123; get(); &#125; public void get() &#123; lock.lock(); try &#123; System.out.println(Thread.currentThread().getName() + "\t invoked get()"); set(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void set() &#123; lock.lock(); lock.lock(); try &#123; System.out.println(Thread.currentThread().getName() + "\t #####invoked set()"); &#125; finally &#123; lock.unlock(); lock.unlock(); &#125; &#125;&#125;/* * 外层函数获得锁之后 内层递归函数仍然能够获取该锁的代码 * * */public class ReenterLockDemo &#123; public static void main(String[] args) &#123; Phone phone = new Phone(); new Thread(() -&gt; &#123; try &#123; phone.sendSMS(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, "t1").start(); new Thread(() -&gt; &#123; try &#123; phone.sendSMS(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, "t2").start(); /* * 只要锁时匹配的有配对就行 * 两把锁 * 3把锁 * n把锁 * */ System.out.println(); System.out.println(); System.out.println(); //暂停一会 try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; /*phone实现了Runnable接口 * */ Thread t3 = new Thread(phone); Thread t4 = new Thread(phone); t3.start(); t4.start(); &#125;&#125; 可重入锁最大的作用就是避免死锁配对 加两对 编译 2.7 自旋锁理论讲解循环替代了阻塞 久安少线程的上下文切换 循环会消耗CPU 多次回来循环调用 循环调查的方式就叫做自旋 空闲 获得锁 忙 干自己的事情 好处： 不会造成阻塞 坏处：长时间 的话会 降低系统的性能 代码验证1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/11 * @描述： *//* * 题目： 实现一个自旋锁 * 循环锁的好处： * 循环比较直到成功为止 * */import com.mlz.study.read.SingleDemo;import java.util.SortedMap;import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicInteger;import java.util.concurrent.atomic.AtomicReference;public class SpinLockDemo &#123; /* * 原子引用 * */ /* * AtomicInteger 0 * 引用 null * */ AtomicReference&lt;Thread&gt; atomicReference = new AtomicReference&lt;&gt;(); public void mylock() &#123; Thread thread = Thread.currentThread(); System.out.println(Thread.currentThread().getName() + "\t come in O(n_n)O"); while (!atomicReference.compareAndSet(null, thread)) &#123; &#125; &#125; public void myunlock() &#123; Thread thread = Thread.currentThread(); atomicReference.compareAndSet(thread, null); System.out.println(Thread.currentThread().getName() + "\t invoked myunlock()"); &#125; public static void main(String[] args) &#123; /* * 泛型 原子引用线程 * */ SpinLockDemo spinLockDemo = new SpinLockDemo(); new Thread(() -&gt; &#123; spinLockDemo.mylock(); //暂停一会线程 try &#123; TimeUnit.SECONDS.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; spinLockDemo.myunlock(); &#125;, "AA").start(); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; new Thread(() -&gt; &#123; spinLockDemo.mylock(); //暂停一会线程 try &#123; TimeUnit.SECONDS.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; spinLockDemo.myunlock(); &#125;, "BB").start(); &#125;&#125; 2.8 java锁之读写锁独占锁（写锁） 共享锁 sync —–&gt;&gt; lock —&gt;&gt; lock 上卫生间 unlock ReentrantReadWriteLock 读写分离 数据的一致性 并发性读写 demo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/11 * @描述： *//*多个线程同时读写一个资源类没有任何问题 所以为了满足并发量 读取共享资源应该可以同时进行但是如果一个线程相当于读取共享资源来，就不应该再有其他线程对该资源进行读或者写技术人员思维方式After before读写锁Lock为啥读读能共存读写能共存写写不能共存写操作 原子 + 独占中间的真个过程必须是一个完整的统一体中间不许被分割 不许被打断* *//* * 资源类 * */import java.util.HashMap;import java.util.Map;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.ReentrantLock;import java.util.concurrent.locks.ReentrantReadWriteLock;class MyCache &#123; private volatile Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); private ReentrantReadWriteLock rwLock = new ReentrantReadWriteLock();//private Lock lock=new ReentrantLock(); //Redis 读 写 清空 缓存框架 public void put(String key, Object value) &#123; rwLock.writeLock().lock(); try &#123; System.out.println(Thread.currentThread().getName() + "\t 正在写入:" + key); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; map.put(key, value); System.out.println(Thread.currentThread().getName() + "\t 写入完成:"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; rwLock.writeLock().unlock(); &#125; &#125; public void get(String key) &#123; rwLock.readLock().lock(); try &#123; System.out.println(Thread.currentThread().getName() + "\t 正在读取:"); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; Object result = map.get(key); System.out.println(Thread.currentThread().getName() + "\t 读取完成:" + result); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; rwLock.readLock().unlock(); &#125; &#125;&#125;public class ReadWriteDemo &#123; public static void main(String[] args) &#123; MyCache myCache = new MyCache(); for (int i = 1; i &lt;= 5; i++) &#123; final int tempInt = i; new Thread(() -&gt; &#123; myCache.put(tempInt + "", tempInt + ""); &#125;, String.valueOf(i)).start(); &#125; for (int i = 1; i &lt;= 5; i++) &#123; final int tempInt = i; new Thread(() -&gt; &#123; myCache.get(tempInt + ""); &#125;, String.valueOf(i)).start(); &#125; &#125;&#125; idea设置代码模块化trylock 自动生成方法设置template alt + crtl +s 读写分离 严格控制 完整独立性 读写分离代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/11 * @描述： *//*多个线程同时读写一个资源类没有任何问题 所以为了满足并发量 读取共享资源应该可以同时进行但是如果一个线程相当于读取共享资源来，就不应该再有其他线程对该资源进行读或者写技术人员思维方式After before读写锁Lock为啥读读能共存读写能共存写写不能共存写操作 原子 + 独占中间的真个过程必须是一个完整的统一体中间不许被分割 不许被打断* *//* * 资源类 * */import java.util.HashMap;import java.util.Map;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.ReentrantLock;import java.util.concurrent.locks.ReentrantReadWriteLock;class MyCache &#123; private volatile Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); private ReentrantReadWriteLock rwLock = new ReentrantReadWriteLock();//private Lock lock=new ReentrantLock(); //Redis 读 写 清空 缓存框架 public void put(String key, Object value) &#123; rwLock.writeLock().lock(); try &#123; System.out.println(Thread.currentThread().getName() + "\t 正在写入:" + key); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; map.put(key, value); System.out.println(Thread.currentThread().getName() + "\t 写入完成:"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; rwLock.writeLock().unlock(); &#125; &#125; public void get(String key) &#123; rwLock.readLock().lock(); try &#123; System.out.println(Thread.currentThread().getName() + "\t 正在读取:"); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; Object result = map.get(key); System.out.println(Thread.currentThread().getName() + "\t 读取完成:" + result); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; rwLock.readLock().unlock(); &#125; &#125;&#125;public class ReadWriteDemo &#123; public static void main(String[] args) &#123; MyCache myCache = new MyCache(); for (int i = 1; i &lt;= 5; i++) &#123; final int tempInt = i; new Thread(() -&gt; &#123; myCache.put(tempInt + "", tempInt + ""); &#125;, String.valueOf(i)).start(); &#125; for (int i = 1; i &lt;= 5; i++) &#123; final int tempInt = i; new Thread(() -&gt; &#123; myCache.get(tempInt + ""); &#125;, String.valueOf(i)).start(); &#125; &#125;&#125; 加锁和不加锁的区别 12 2.9 CountDownLatch前提任务完成之后 再去完成最后的任务 做减法 定义 让一些线程阻塞直到另外一些完成后才唤醒 CountDownLatch主要有两个方法 当一个或者多个线程调用await方法的时候，调用线程会被阻塞，其他线程调用，countDown方法 计数器减1 （调用CountDown方法的时候，线程不会被阻塞），当计数器的值变为0的时候调用await（）方法被阻塞的线程被唤醒，继续执行 demo11234567891011121314151617181920212223242526272829303132333435363738394041424344package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/12 * @描述： */import java.util.concurrent.CountDownLatch;import java.util.concurrent.TimeUnit;public class CountDownLatchDemo &#123; /* * 没有用 CountdownLatch * * */ public static void main(String[] args) throws InterruptedException &#123; CountDownLatch countDownLatch = new CountDownLatch(6); for (int i = 1; i &lt;= 6; i++) &#123; new Thread(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + "\t 上完自习,离开教室"); countDownLatch.countDown(); &#125;, String.valueOf(i)).start(); &#125; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; countDownLatch.await(); System.out.println(Thread.currentThread().getName() + "\t ****************************班长最后关门走人"); &#125;&#125; demo2一统天下 灭六国 枚举类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.mlz.study.Interview.enums;/* * @创建人: MaLingZhao * @创建时间: 2019/10/12 * @描述： */import lombok.Getter;public enum CountryEnum &#123; /* * 美剧 ONE TWO * * 代码版本的mysql数据库 * * ONE * */ ONE(1, "齐"), TWO(2, "楚"), THREE(3, "燕"), FOUR(4, "韩"), FIVE(5, "赵"), SIX(6, "魏"); @Getter private Integer retCode; @Getter private String retMessage; CountryEnum(Integer retCode, String retMessage) &#123; this.retCode = retCode; this.retMessage = retMessage; &#125; public static CountryEnum forEach_CountryEnum(int index) &#123; CountryEnum[] myArray = CountryEnum.values(); for (CountryEnum element : myArray) &#123; if (index == element.getRetCode()) &#123; return element; &#125; &#125; return null; &#125;&#125; Demo 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/12 * @描述： */import com.mlz.study.Interview.enums.CountryEnum;import java.util.concurrent.CountDownLatch;import java.util.concurrent.TimeUnit;public class CountDownLatchDemo &#123; /* * 没有用 CountdownLatch * * */ public static void main(String[] args) throws InterruptedException &#123; CountDownLatch countDownLatch = new CountDownLatch(6); for (int i = 1; i &lt;= 6; i++) &#123; new Thread(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + "\t 国被灭"); countDownLatch.countDown(); &#125;, CountryEnum.forEach_CountryEnum(i).getRetMessage()).start(); &#125; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; countDownLatch.await(); System.out.println(Thread.currentThread().getName() + "\t ****************************秦国一统天下"); &#125;&#125; 枚举类使用更加方便 1234System.out.println(CountryEnum.ONE); System.out.println(CountryEnum.ONE.getRetCode());; System.out.println(CountryEnum.ONE.getRetMessage()); CyclicBarrier集齐7颗龙珠就能召唤神龙 人到齐了才能开会 demo案例123456789101112131415161718192021222324252627282930313233343536package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/12 * @描述： */import java.util.concurrent.BrokenBarrierException;import java.util.concurrent.CyclicBarrier;public class CyclicBarrierDemo &#123; public static void main(String[] args) &#123; CyclicBarrier cyclicBarrier = new CyclicBarrier(7, () -&gt; System.out.println("**********召唤神龙")); for (int i = 1; i &lt;= 7; i++) &#123; final int tempInt = i; new Thread(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + "\t 收集到第:" + tempInt + "龙珠"); try &#123; cyclicBarrier.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; &#125;, String.valueOf(i)).start(); &#125; &#125;&#125; Semaphore信号灯 小米的秒杀接口 基础 高级 synchronized 小米秒杀 放出手机 下降99 多个线程抢多份资源 来一辆 走一辆 代替synchronized和lock 定义两个目的 共享资源的互斥使用 另一个用于并发线程数的控制 demo案例123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/12 * @描述： */import java.util.concurrent.Semaphore;import java.util.concurrent.TimeUnit;public class SemaphoreDemo &#123; public static void main(String[] args) &#123; //模拟3个停车位 Semaphore semaphore = new Semaphore(3); for (int i = 1; i &lt;= 6; i++) &#123;//模拟6辆车 new Thread(() -&gt; &#123; try &#123; semaphore.acquire(); System.out.println(Thread.currentThread().getName() + "\t抢到车位"); //暂停一会线程 try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + "\t停车3秒离开车位"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; semaphore.release(); &#125; &#125;, String.valueOf(i)).start(); &#125; &#125;&#125; 2.9 阻塞队列理论定义阻塞队列,顾名思义,首先它是一个队列,而一个阻塞队列在数据结构中所起的作用大致如图所示: 线程1往阻塞队列中添加元素二线程2从队列中移除元素当阻塞队列是空时,从队列中获取元素的操作将会被阻塞.当阻塞队列是满时,往队列中添加元素的操作将会被阻塞.同样试图往已满的阻塞队列中添加新圆度的线程同样也会被阻塞,知道其他线程从队列中移除一个或者多个元素或者全清空队列后使队列重新变得空闲起来并后续新增. 队列 有序 生产线程 消费线程 队列 空 获取元素操作被阻塞 队列满 队列添加元素操作被阻塞 卖蛋糕的问题 柜台空 xxx 柜台满 xxx 请写一个阻塞队列 为什么用在多线程领域:所谓阻塞,在某些情况下会挂起线程(即线程阻塞),一旦条件满足,被挂起的线程优惠被自动唤醒 为什么需要使用BlockingQueue 好处是我们不需要关心什么时候需要阻塞线程,什么时候需要唤醒线程,因为BlockingQueue都一手给你包办好了 在concurrent包 发布以前,在多线程环境下,我们每个程序员都必须自己去控制这些细节,尤其还要兼顾效率和线程安全,而这会给我们的程序带来不小的复杂度. 架构 BlockQueue 消息系统 底层就是这个 ArrayBlockingQueue 数组组成的有界阻塞队列 LinkedBlockingQueue 链表结构组成的有界阻塞队列 Integer.MAX_VALUE SynchronousQueue 不存储元素的阻塞队列，也即是单个元素的阻塞队列 生产一个， 消费一个 定制版的 下单开工 Deque 双端队列 阻塞队列的核心方法抛出异常 当阻塞队列满时,再往队列里面add插入元素会抛IllegalStateException: Queue full当阻塞队列空时,再往队列Remove元素时候回抛出NoSuchElementException特殊值 插入方法,成功返回true 失败返回false移除方法,成功返回元素,队列里面没有就返回null一直阻塞 当阻塞队列满时,生产者继续往队列里面put元素,队列会一直阻塞直到put数据or响应中断退出当阻塞队列空时,消费者试图从队列take元素,队列会一直阻塞消费者线程直到队列可用.超时退出 当阻塞队列满时,队列会阻塞生产者线程一定时间,超过后限时后生产者线程就会退出 热身123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/12 * @描述： */import java.util.ArrayList;import java.util.List;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;/** 队列* 阻塞队列* 2.1 阻塞队列有没有好的一面** 2.2 不得不阻塞，你如何管理** 银行排队* */public class BlockingQueueDemo &#123; public static void main(String[] args) &#123; //你用过什么List //你除了ArrayList和LinkedList 你还用过什么 //CopyOnWriteArrayList //List list=new ArrayList(); /* * * 队列只有3个位置 * */ BlockingQueue&lt;String&gt; blockingQueue=new ArrayBlockingQueue&lt;&gt;(3); System.out.println(blockingQueue.add("a")); System.out.println(blockingQueue.add("b")); System.out.println(blockingQueue.add("c")); /* * 队首元素 * */ System.out.println(blockingQueue.element()); /* * 不合法的队列 队列满 添加元素 * */ //System.out.println(blockingQueue.add("d")); System.out.println(blockingQueue.remove()); System.out.println(blockingQueue.remove()); System.out.println(blockingQueue.remove()); /* * 空的时候报异常 * */ // System.out.println(blockingQueue.remove()); System.out.println( Integer.MAX_VALUE); &#125;&#125; api的布尔返回值12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/12 * @描述： */import java.util.ArrayList;import java.util.List;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;/* * 队列 * 阻塞队列 * 2.1 阻塞队列有没有好的一面 * * 2.2 不得不阻塞，你如何管理 * * 银行排队 * */public class BlockingQueueDemo &#123; public static void main(String[] args) &#123; //你用过什么List //你除了ArrayList和LinkedList 你还用过什么 //CopyOnWriteArrayList //List list=new ArrayList(); BlockingQueue&lt;String&gt; blockingQueue = new ArrayBlockingQueue&lt;&gt;(3); System.out.println(blockingQueue.offer("a")); System.out.println(blockingQueue.offer("b")); System.out.println(blockingQueue.offer("c")); System.out.println(blockingQueue.offer("x" )); System.out.println(blockingQueue.peek()); System.out.println(blockingQueue.poll()); System.out.println(blockingQueue.poll()); System.out.println(blockingQueue.poll()); System.out.println(blockingQueue.poll()); &#125;&#125; 老子堵着你1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/12 * @描述： */import java.util.ArrayList;import java.util.List;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;/* * 队列 * 阻塞队列 * 2.1 阻塞队列有没有好的一面 * * 2.2 不得不阻塞，你如何管理 * * 银行排队 * */public class BlockingQueueDemo &#123; public static void main(String[] args) throws InterruptedException &#123; //你用过什么List //你除了ArrayList和LinkedList 你还用过什么 //CopyOnWriteArrayList //List list=new ArrayList(); BlockingQueue&lt;String&gt; blockingQueue = new ArrayBlockingQueue&lt;&gt;(3); blockingQueue.put("a"); blockingQueue.put("a"); blockingQueue.put("a"); System.out.println("===================="); //blockingQueue.put("a"); /* * 堵着 * */ blockingQueue.take(); blockingQueue.take(); blockingQueue.take(); blockingQueue.take(); &#125;&#125; 心情好 放你走吧123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/12 * @描述： */import java.util.ArrayList;import java.util.List;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;import java.util.concurrent.TimeUnit;/* * 队列 * 阻塞队列 * 2.1 阻塞队列有没有好的一面 * * 2.2 不得不阻塞，你如何管理 * * 银行排队 * */public class BlockingQueueDemo &#123; public static void main(String[] args) throws InterruptedException &#123; //你用过什么List //你除了ArrayList和LinkedList 你还用过什么 //CopyOnWriteArrayList //List list=new ArrayList(); BlockingQueue&lt;String&gt; blockingQueue = new ArrayBlockingQueue&lt;&gt;(3); System.out.println(blockingQueue.offer("a",2L, TimeUnit.SECONDS)); System.out.println(blockingQueue.offer("a",2L, TimeUnit.SECONDS)); System.out.println(blockingQueue.offer("a",2L, TimeUnit.SECONDS)); System.out.println(blockingQueue.offer("a",2L, TimeUnit.SECONDS)); &#125;&#125; SynchronousBlcokQueue专属定制版 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/12 * @描述： */import java.util.concurrent.BlockingQueue;import java.util.concurrent.SynchronousQueue;import java.util.concurrent.TimeUnit;public class SynchronousQueueDemo &#123; public static void main(String[] args) &#123; BlockingQueue&lt;Integer&gt; blockingQueue=new SynchronousQueue&lt;&gt;(); new Thread(()-&gt;&#123; try &#123; System.out.println(Thread.currentThread().getName() + "\t put 1"); blockingQueue.put(1); System.out.println(Thread.currentThread().getName() + "\t put 2"); blockingQueue.put(2); System.out.println(Thread.currentThread().getName() + "\t put 3"); blockingQueue.put(3); &#125;catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;,"AAA").start(); new Thread(()-&gt;&#123; try &#123; try &#123; TimeUnit.SECONDS.sleep(5); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName()+"\t"+blockingQueue.take()); try &#123; TimeUnit.SECONDS.sleep(5); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName()+"\t"+blockingQueue.take()); try &#123; TimeUnit.SECONDS.sleep(5); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName()+"\t"+blockingQueue.take()); &#125;catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;,"BBB").start(); &#125;&#125;]]></content>
      <tags>
        <tag>面试</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LoggAnalysising]]></title>
    <url>%2Fblog4%2F2019%2F08%2F16%2FLoggAnalysising%2F</url>
    <content type="text"><![CDATA[1 日志生成模块日志生成模块的架构]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>项目实战</tag>
        <tag>大数据项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实时项目]]></title>
    <url>%2Fblog4%2F2019%2F08%2F12%2FRealTimeProjectsparkES%2F</url>
    <content type="text"><![CDATA[12 第一章1 项目的架构1.1 需求的特点离线需求 一般是根据前一日的数据生成报表等数据，虽然统计指标、报表繁多，但是对时效性不敏感。 实时需求 主要侧重于对当日数据的实时监控，通常业务逻辑相对离线需求简单一下，统计指标也少一些，但是更注重数据的时效性，以及用户的交互性。 实时架构 离线架构 2 框架的搭建2.1 框架的架构 2.2 框架的搭建代码部分呢 1建立父工程gmall1205-dw 引入pom.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100 &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.10.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt;&lt;properties&gt; &lt;spark.version&gt;2.1.1&lt;/spark.version&gt; &lt;scala.version&gt;2.11.8&lt;/scala.version&gt; &lt;log4j.version&gt;1.2.17&lt;/log4j.version&gt; &lt;slf4j.version&gt;1.7.22&lt;/slf4j.version&gt; &lt;fastjson.version&gt;1.2.47&lt;/fastjson.version&gt; &lt;httpclient.version&gt;4.5.5&lt;/httpclient.version&gt; &lt;httpmime.version&gt;4.3.6&lt;/httpmime.version&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;!--此处放日志包，所有项目都要引用--&gt; &lt;!-- 所有子项目的日志框架 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;jcl-over-slf4j&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 具体的日志实现 --&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;version&gt;$&#123;httpclient.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpmime&lt;/artifactId&gt; &lt;version&gt;$&#123;httpmime.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;$&#123;fastjson.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-hive_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 2 建立子模块 公共模块gmall1205-common pom.xml文件 12345678910111213141516&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpmime&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 建立子模块 模拟数据 gmall1205-mock 加入公共模块的依赖 gmall1205-common pom 4.1 建立类RandomDate 包名 com.atguigu.gmall1205.mock.util 这是工具类 123456789101112131415161718192021222324import java.util.Date;import java.util.Random;public class RandomDate &#123; Long logDateTime =0L;// int maxTimeStep=0 ; public RandomDate (Date startDate , Date endDate,int num) &#123; Long avgStepTime = (endDate.getTime()- startDate.getTime())/num; this.maxTimeStep=avgStepTime.intValue()*2; this.logDateTime=startDate.getTime(); &#125; public Date getRandomDate() &#123; int timeStep = new Random().nextInt(maxTimeStep); logDateTime = logDateTime+timeStep; return new Date( logDateTime); &#125;&#125; 4.2 RanOpt 1234567891011121314151617public class RanOpt&lt;T&gt;&#123; T value ; int weight; public RanOpt ( T value, int weight )&#123; this.value=value ; this.weight=weight; &#125; public T getValue() &#123; return value; &#125; public int getWeight() &#123; return weight; &#125; &#125; 4.3RandomOptionGroup 12345678910111213141516171819202122232425262728293031323334353637383940import java.util.ArrayList;import java.util.List;import java.util.Random;public class RandomOptionGroup&lt;T&gt; &#123; int totalWeight=0; List&lt;RanOpt&gt; optList=new ArrayList(); public RandomOptionGroup(RanOpt&lt;T&gt;... opts) &#123; for (RanOpt opt : opts) &#123; totalWeight+=opt.getWeight(); for (int i = 0; i &lt;opt.getWeight() ; i++) &#123; optList.add(opt); &#125; &#125; &#125; public RanOpt&lt;T&gt; getRandomOpt() &#123; int i = new Random().nextInt(totalWeight); return optList.get(i); &#125; public static void main(String[] args) &#123; RanOpt[] opts= &#123;new RanOpt("zhang3",20),new RanOpt("li4",30),new RanOpt("wang5",50)&#125;; RandomOptionGroup randomOptionGroup = new RandomOptionGroup(opts); for (int i = 0; i &lt;10 ; i++) &#123; System.out.println(randomOptionGroup.getRandomOpt().getValue()); &#125; &#125; &#125; *4.4 RandomNum * 1234567public class RandomNum &#123; public static final int getRandInt(int fromNum,int toNum)&#123; return fromNum+ new Random().nextInt(toNum-fromNum+1); &#125;&#125; 1 5新建日志发送工具 发送到采集系统的web端口 LogUploader 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import org.apache.http.HttpEntity;import org.apache.http.NameValuePair;import org.apache.http.client.ClientProtocolException;import org.apache.http.client.config.RequestConfig;import org.apache.http.client.entity.UrlEncodedFormEntity;import org.apache.http.client.methods.CloseableHttpResponse;import org.apache.http.client.methods.HttpPost;import org.apache.http.entity.mime.MultipartEntityBuilder;import org.apache.http.entity.mime.content.FileBody;import org.apache.http.impl.client.CloseableHttpClient;import org.apache.http.impl.client.HttpClients;import org.apache.http.message.BasicNameValuePair;import org.apache.http.util.EntityUtils;import java.io.File;import java.io.IOException;import java.io.OutputStream;import java.io.UnsupportedEncodingException;import java.net.HttpURLConnection;import java.net.URL;import java.util.ArrayList;import java.util.List;public class LogUploader &#123; public static void sendLogStream(String log)&#123; try&#123; //不同的日志类型对应不同的URL URL url =new URL("http://logserver/log"); HttpURLConnection conn = (HttpURLConnection) url.openConnection(); //设置请求方式为post conn.setRequestMethod("POST"); //时间头用来供server进行时钟校对的 conn.setRequestProperty("clientTime",System.currentTimeMillis() + ""); //允许上传数据 conn.setDoOutput(true); //设置请求的头信息,设置内容类型为JSON conn.setRequestProperty("Content-Type", "application/x-www-form-urlencoded"); System.out.println("upload" + log); //输出流 OutputStream out = conn.getOutputStream(); out.write(("log="+log).getBytes()); out.flush(); out.close(); int code = conn.getResponseCode(); System.out.println(code); &#125; catch (Exception e)&#123; e.printStackTrace(); &#125; &#125;&#125; http://logserver/log?{json} 6 日志生成类 com.atguigu.gmall1205.mock 包名 startup和wvwnt两种事件日志 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175import com.alibaba.fastjson.JSON;import com.alibaba.fastjson.JSONObject;import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Date;import java.util.Random;public class JsonMocker &#123; int startupNum=100000; int eventNum=200000 ; RandomDate logDateUtil= null; RanOpt[] osOpts= &#123;new RanOpt("ios",3),new RanOpt("andriod",7) &#125;; RandomOptionGroup&lt;String&gt; osOptionGroup= new RandomOptionGroup(osOpts); Date startTime= null; Date endTime= null; RanOpt[] areaOpts= &#123;new RanOpt("beijing",10), new RanOpt("shanghai",10),new RanOpt("guangdong",20),new RanOpt("hebei",5), new RanOpt("heilongjiang",5),new RanOpt("shandong",5),new RanOpt("tianjin",5), new RanOpt("shan3xi",5),new RanOpt("shan1xi",5),new RanOpt("sichuan",5) &#125;; RandomOptionGroup&lt;String&gt; areaOptionGroup= new RandomOptionGroup(areaOpts); String appId="gmall1205"; RanOpt[] vsOpts= &#123;new RanOpt("1.2.0",50),new RanOpt("1.1.2",15), new RanOpt("1.1.3",30), new RanOpt("1.1.1",5) &#125;; RandomOptionGroup&lt;String&gt; vsOptionGroup= new RandomOptionGroup(vsOpts); RanOpt[] eventOpts= &#123;new RanOpt("addFavor",10),new RanOpt("addComment",30), new RanOpt("addCart",20), new RanOpt("clickItem",40) &#125;; RandomOptionGroup&lt;String&gt; eventOptionGroup= new RandomOptionGroup(eventOpts); RanOpt[] channelOpts= &#123;new RanOpt("xiaomi",10),new RanOpt("huawei",20), new RanOpt("wandoujia",30), new RanOpt("360",20), new RanOpt("tencent",20) , new RanOpt("baidu",10), new RanOpt("website",10) &#125;; RandomOptionGroup&lt;String&gt; channelOptionGroup= new RandomOptionGroup(channelOpts); RanOpt[] quitOpts= &#123; new RanOpt(true,20),new RanOpt(false,80)&#125;; RandomOptionGroup&lt;Boolean&gt; isQuitGroup= new RandomOptionGroup(quitOpts); public JsonMocker( ) &#123; &#125; public JsonMocker(String startTimeString ,String endTimeString,int startupNum,int eventNum) &#123; try &#123; startTime= new SimpleDateFormat("yyyy-MM-dd").parse(startTimeString); endTime= new SimpleDateFormat("yyyy-MM-dd").parse(endTimeString); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; logDateUtil= new RandomDate(startTime,endTime,startupNum+eventNum); &#125; String initEventLog(String startLogJson)&#123; /*`type` string COMMENT '日志类型', `mid` string COMMENT '设备唯一 表示', `uid` string COMMENT '用户标识', `os` string COMMENT '操作系统', `appid` string COMMENT '应用id', `area` string COMMENT '地区' , `evid` string COMMENT '事件id', `pgid` string COMMENT '当前页', `npgid` string COMMENT '跳转页', `itemid` string COMMENT '商品编号', `ts` bigint COMMENT '时间',*/ JSONObject startLog = JSON.parseObject(startLogJson); String mid= startLog.getString("mid"); String uid= startLog.getString("uid"); String os= startLog.getString("os"); String appid=this.appId; String area=startLog.getString("area"); String evid = eventOptionGroup.getRandomOpt().getValue(); int pgid = new Random().nextInt(50)+1; int npgid = new Random().nextInt(50)+1; int itemid = new Random().nextInt(50); // long ts= logDateUtil.getRandomDate().getTime(); JSONObject jsonObject = new JSONObject(); jsonObject.put("type","event"); jsonObject.put("mid",mid); jsonObject.put("uid",uid); jsonObject.put("os",os); jsonObject.put("appid",appid); jsonObject.put("area",area); jsonObject.put("evid",evid); jsonObject.put("pgid",pgid); jsonObject.put("npgid",npgid); jsonObject.put("itemid",itemid); return jsonObject.toJSONString(); &#125; String initStartupLog( )&#123; /*`type` string COMMENT '日志类型', `mid` string COMMENT '设备唯一标识', `uid` string COMMENT '用户标识', `os` string COMMENT '操作系统', , `appId` string COMMENT '应用id', , `vs` string COMMENT '版本号', `ts` bigint COMMENT '启动时间', , `area` string COMMENT '城市' */ String mid= "mid_"+ RandomNum.getRandInt(1,500); String uid=""+ RandomNum.getRandInt(1,500); String os=osOptionGroup.getRandomOpt().getValue(); String appid=this.appId; String area=areaOptionGroup.getRandomOpt().getValue(); String vs = vsOptionGroup.getRandomOpt().getValue(); //long ts= logDateUtil.getRandomDate().getTime(); String ch=os.equals("ios")?"appstore": channelOptionGroup.getRandomOpt().getValue(); JSONObject jsonObject = new JSONObject(); jsonObject.put("type","startup"); jsonObject.put("mid",mid); jsonObject.put("uid",uid); jsonObject.put("os",os); jsonObject.put("appid",appid); jsonObject.put("area",area); jsonObject.put("ch",ch); jsonObject.put("vs",vs); return jsonObject.toJSONString(); &#125;public static void genLog() &#123; JsonMocker jsonMocker = new JsonMocker(); jsonMocker.startupNum = 1000000; for (int i = 0; i &lt; jsonMocker.startupNum; i++) &#123; String startupLog = jsonMocker.initStartupLog(); jsonMocker.sendLog(startupLog); while (!jsonMocker.isQuitGroup.getRandomOpt().getValue()) &#123; String eventLog = jsonMocker.initEventLog(startupLog); jsonMocker.sendLog(eventLog); &#125; try &#123; Thread.sleep(20); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; public void sendLog(String log) &#123; LogUploader.sendLogStream(log); &#125; public static void main(String[] args) &#123; genLog(); &#125;&#125; 2.3开始接收日志怎么接受日志呢 用springBoot 2.3.1 springBoot是干什么的简介 Spring Boot 是由 Pivotal 团队提供的全新框架，其设计目的是用来简化新 Spring 应用的初始搭建以及开发过程。 该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。 有了springboot 我们就可以…内嵌Tomcat,不再需要外部的Tomcat不再需要那些千篇一律，繁琐的xml文件。 更方便的和各个第三方工具（mysql,redis,elasticsearch,dubbo等等整合），而只要维护一个配置文件即可。 2.3.2 springBoot和ssm的关系​ springboot整合了springmvc ，spring等核心功能。也就是说本质上实现功能的还是原有的spring ,springmvc的包，但是springboot单独包装了一层，这样用户就不必直接对springmvc， spring等，在xml中配置。 2.3.3 没有xml我们打哪里配置1 springboot实际上就是把以前需要用户手工配置的部分，全部作为默认项。除非用户需要额外更改不然不用配置。这就是所谓的：“约定大于配置” 2 如果需要特别配置的时候，去修改application.properties 2.3.4 快速上手快速搭建模块 整合第三方 框架的版本里面可以修改 springboot的配置 logger 两个都想继承 多了一个springBoot的parent 怎么办 pom.xml 修改 springboot的版本将这个模块的依赖剪切到父模块中 123456&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.10.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; 同时让springboot继承父模块 在logger的xml中添加 12345&lt;parent&gt; &lt;artifactId&gt;gmall1205-parent&lt;/artifactId&gt; &lt;groupId&gt;com.atguigu.gmall1205&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; 3 日志保存配置logserver 反向代理 application.properties 代理本机 80 两款软件可以使用 修改 post方式改成get请求 能通 12345678910111213141516171819202122232425262728293031323334package com.atguigu.gmall1205.logger.controller;/* * @创建人: MaLingZhao * @创建时间: 2019/9/25 * @描述： */import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.*;@RestController//@Controller//RestConrtoller=Controller+ ResponseBondy 不用每个方法前加ResponseBiondy了public class LoggerController &#123; //下面的两个配置是完全等价的 //@RequestMapping(value="/log",method= RequestMethod.POST) //@PostMapping("/log") //@ResponseBody @RequestMapping("/log") public String dolog(@RequestParam("log") String logJson) &#123; //logJson直接注入到变量 //字符串当做返回的模板的名字的jsp 加上返回ResponseBondy 返回的是字符串 return logJson; &#125;&#125; 修改logger 打通关系 123456789101112131415161718192021222324252627282930313233343536package com.atguigu.gmall1205.logger.controller;/* * @创建人: MaLingZhao * @创建时间: 2019/9/25 * @描述： */import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.*;@RestController//@Controller//RestConrtoller=Controller+ ResponseBondy 不用每个方法前加ResponseBiondy了public class LoggerController &#123; //下面的两个配置是完全等价的 //@RequestMapping(value="/log",method= RequestMethod.POST) @PostMapping("/log") //@ResponseBody //@RequestMapping("/log") public String dolog(@RequestParam("log") String logJson) &#123; //logJson直接注入到变量 //字符串当做返回的模板的名字的jsp 加上返回ResponseBondy 返回的是字符串 System.out.println(logJson); return logJson; &#125;&#125; 两个模块启动 pom.xml 12345678910111213141516 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-log4j&lt;/artifactId&gt; &lt;version&gt;1.3.8.RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 不使用spring-boot-starter-logging 使用spring-boot-starter-log4j exclusions 排斥在外的 添加log.properties private static final org.slf4j.Logger logger = LoggerFactory.getLogger(LogJsonController.class) ; logger.info 1234567891011121314151617181920212223242526# 自定义格式log4j.appender.atguigu.MyConsole=org.apache.log4j.ConsoleAppender#log4j.appender.atguigu.MyConsole.target=System.err# 这里配置的是输出错误的内容 我们需要配置System.outlog4j.appender.atguigu.MyConsole.target=System.outlog4j.appender.atguigu.MyConsole.layout=org.apache.log4j.PatternLayout log4j.appender.atguigu.MyConsole.layout.ConversionPattern=%d&#123;yyyy-MM-dd HH:mm:ss&#125; %10p (%c:%M) - %m%n log4j.appender.atguigu.File=org.apache.log4j.DailyRollingFileAppender# 具体输出到那个文件里面log4j.appender.atguigu.File.file=d:/applog/gmall1205/log/app.log# 扩展名# 生成的新的日志 重新起名字 app.log_2018_5xxxxxlog4j.appender.atguigu.File.DatePattern=&apos;.&apos;yyyy-MM-ddlog4j.appender.atguigu.File.layout=org.apache.log4j.PatternLayoutlog4j.appender.atguigu.File.layout.ConversionPattern=%m%n#最后的类追加 输出的类log4j.logger.com.atguigu.gmall1205.logger.controller.LoggerController=info,atguigu.File,atguigu.MyConsole#trace debug info warn error fatal 毁灭性的#中间四个常用# error# 级别越低 输出的内容越多 级别 trace debug info warn error fatal 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.atguigu.gmall1205.logger.controller;/* * @创建人: MaLingZhao * @创建时间: 2019/9/25 * @描述： */import com.alibaba.fastjson.JSON;import com.alibaba.fastjson.JSONObject;import com.fasterxml.jackson.databind.util.JSONPObject;import org.slf4j.LoggerFactory;import org.springframework.web.bind.annotation.*;@RestController//@Controller//RestConrtoller=Controller+ ResponseBondy 不用每个方法前加ResponseBiondy了public class LoggerController &#123; private static final org.slf4j.Logger logger = LoggerFactory.getLogger(LoggerController.class) ; //下面的两个配置是完全等价的 //@RequestMapping(value="/log",method= RequestMethod.POST) @PostMapping("/log") //@ResponseBody //@RequestMapping("/log") public String dolog(@RequestParam("log") String logJson) &#123; //logJson直接注入到变量 //字符串当做返回的模板的名字的jsp 加上返回ResponseBondy 返回的是字符串 /* 在什么地方封装时间戳 这样的话时间会更加的准确 在收集 的时候 给 从日志到springBoot */ // 补时间戳 //fastjson转字符串 JSONObject jsonpObject =JSON.parseObject(logJson); jsonpObject.put("ts",System.currentTimeMillis()); //洛盘到logfile log4j logger.info(jsonpObject.toJSONString()); //发送kafka System.out.println(logJson); return logJson; &#125;&#125; 完成日志的输出洛盘 到目录查看日志文件 3 日志服务器集群3.1 发送到kafkakafka已经进行了引入 1@Autowired KafkaTemplate&lt;String,String&gt; kafkaTemplate; kafkTemplate.sen() 发送 到什么地方 配置application.properties 123456789101112#包含所有第三方的配置server.port=80# 配置kafka集群spring.kafka.bootstrap-servers= hadoop102:9092,hadoop103:9092,hadoop104:9092#配置编码格式# 指定消息key和消息体的编解码方式spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializerspring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer 在hosts的方案中最好选择第二种 在common做一个常量类 com.atguigu.gmall1205.common.constant包 12 12345678910package com.atguigu.gmall.dw.constant;public class GmallConstants &#123; public static final String KAFKA_TOPIC_STARTUP="GMALL_STARTUP"; public static final String KAFKA_TOPIC_EVENT="GMALL_EVENT"; &#125; loggercontroller的修改 12 @RestController // Controller+Responsebodypublic class LoggerController { 1234567@AutowiredKafkaTemplate&lt;String,String&gt; kafkaTemplate;private static final org.slf4j.Logger logger = LoggerFactory.getLogger(LoggerController.class) ;//@RequestMapping(value = &quot;/log&quot;,method = RequestMethod.POST) =&gt;@PostMapping(&quot;/log&quot;)public String dolog(@RequestParam(&quot;log&quot;) String logJson)&#123; 1234567891011121314 // 补时间戳 JSONObject jsonObject = JSON.parseObject(logJson); jsonObject.put("ts",System.currentTimeMillis()); // 落盘到logfile log4j logger.info(jsonObject.toJSONString()); // 发送kafka if("startup".equals(jsonObject.getString("type")) )&#123; kafkaTemplate.send(GmallConstant.KAFKA_TOPIC_STARTUP,jsonObject.toJSONString()); &#125;else&#123; kafkaTemplate.send(GmallConstant.KAFKA_TOPIC_EVENT,jsonObject.toJSONString()); &#125; return "success";&#125; } 3.2 打jar包1 添加logger install 注意安装的父项目 3台机器依次执行 1569413890196](RealTimeProjectsparkES\1569413890196.png) 对应的log4j 上传jar包 java -jar 命令出现异常 非root用户只能使用1024以上的端口号 端口问题 改变端口 修改UpLoader 上传日志 产生日志 后台启动 3.2.3 日志的生成的脚本12 12345678910111213141516171819202122232425#!/bin/bashJAVA_BIN=/opt/module/jdk1.8.0_144/bin/javaPROJECT=gmall1205APPNAME=gmall1205-logger-0.0.1-SNAPSHOT.jarSERVER_PORT=8080 case $1 in "start") &#123; for i in hadoop102 hadoop103 hadoop104 do echo "========启动日志服务: $i===============" ssh $i "$JAVA_BIN -Xms32m -Xmx64m -jar /applog/$PROJECT/$APPNAME --server.port=$SERVER_PORT &gt;/dev/null 2&gt;&amp;1 &amp;" done &#125;;; "stop") &#123; for i in hadoop102 hadoop103 hadoop104 do echo "========关闭日志服务: $i===============" ssh $i "ps -ef|grep $APPNAME |grep -v grep|awk '&#123;print \$2&#125;'|xargs kill" &gt;/dev/null 2&gt;&amp;1 done &#125;;; esac source /etc/profile 写全路径 添加执行权限 4 nginx4.1 nginx的定义定义： Nginx (“engine x”) 是一个高性能的HTTP和反向代理服务器,特点是占有内存少，并发能力强，事实上nginx的并发能力确实在同类型的网页服务器中表现较好，中国大陆使用nginx网站用户有：百度、京东、新浪、网易、腾讯、淘宝等。 4.2 nginx与tomcat的关系1除了tomcat以外，apache,nginx,jboss,jetty等都是http服务器。 但是nginx和apache只支持静态页面和CGI协议的动态语言，比如perl、php等，但是nginx不支持java。Java程序只能通过与tomcat配合完成。 1nginx与tomcat 配合，为tomcat集群提供反向代理服务、负载均衡等服务 nginx处理请求 替代不了tomcat 处理不了java php nginx可以处理+ 4.3 nginx的三大功能4.3.1 反向代理反向代理什么是反向代理？先看什么是正向代理 再看什么是反向代理 ​ 4.3.2 负载均衡4.3.3 动静分离 4.4 安装nginx4.4.1 安装依赖1sudo yum -y install openssl openssl-devel pcre pcre-devel zlib zlib-devel gcc gcc-c++ 4.4.2 安装包解压缩nginx-xx.tar.gz包。进入解压缩目录，执行./configure –prefix=/home/atguigu/nginx make &amp;&amp; make install 路径在 赋予权限 nginx占用80端口，默认情况下非root用户不允许使用1024以下端口 执行这个命令 sudo setcap cap_net_bind_service=+eip /home/atguigu/nginx/sbin/nginx 如果报错的话 ln -s /usr/local/lib/libpcre.so.1 /lib64 修改 conf/nginx.conf文件 负载均衡策略 使用80端口 123456789101112131415161718192021http&#123; .......... upstream logserver&#123; server hadoop102:8080 weight=1; server hadoop103:8080 weight=1; server hadoop104:8080 weight=1; &#125; server &#123; listen 80; server_name logserver; location / &#123; root html; index index.html index.htm; proxy_pass http://logserver; proxy_connect_timeout 10; &#125; ..........&#125; nginx**的命令**启动 启动命令: 在/usr/local/nginx/sbin目录下执行 ./nginx 关闭 关闭命令: 在/usr/local/nginx/sbin目录下执行 ./nginx -s stop 重新加载 重新加载命令: 在/usr/local/nginx/sbin目录下执行 ./nginx -s reload nginx的地址是哪里 三台服务器处理数据 如何体现 4.3 启动nginx进行测试windows发送模拟日志 nginx负责路由 日志服务复杂接收 更新集群启动脚本logger-cluster.sh 123456789101112131415161718192021222324252627282930#!/bin/bashJAVA_BIN=/opt/module/jdk1.8.0_144/bin/javaPROJECT=gmall1205APPNAME=gmall1205-logger-0.0.1-SNAPSHOT.jarSERVER_PORT=8080 case $1 in "start") &#123; for i in hadoop102 hadoop103 hadoop104 do echo "========: $i===============" ssh $i "$JAVA_BIN -Xms32m -Xmx64m -jar /applog/$PROJECT/$APPNAME --server.port=$SERVER_PORT &gt;/dev/null 2&gt;&amp;1 &amp;" done echo "========NGINX===============" /home/atguigu/nginx/sbin/nginx &#125;;; "stop") &#123; echo "======== NGINX===============" /home/atguigu/nginx/sbin/nginx -s stop for i in hadoop102 hadoop103 hadoop104 do echo "========: $i===============" ssh $i "ps -ef|grep $APPNAME |grep -v grep|awk '&#123;print \$2&#125;'|xargs kill" &gt;/dev/null 2&gt;&amp;1 done &#125;;; esac source /etc/profile cat /etc/profiel &gt;~/.bashrc 全路径 12345/bin/kafka-console-consumer.sh --bootstrap-server hadoop02:9092,hadoop103:9092,hadoop104:9092 --topic GMALL_STARTUP --from-beginning #启动log-cluster.sh#最后tail -20f 接收文件的目录 端口号访问hadoop102/也行 或者添加主机的映射 nginx的使用指南 对应的host文件的配置 到了这 第二章 日活DAU1 搭建实时处理模块1.1 思路1 消费kafka 2 过滤当日已经计入的日活设备 3 把每次新增的当日活动信息保存到ES 4 ES查询数据 发布成数据接口 洛盘文件 kafka sparkStreaming 消费kafka 新建项目 pom.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.atguigu.gmall1205&lt;/groupId&gt; &lt;artifactId&gt;gmall1205-common&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.10.2.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.searchbox&lt;/groupId&gt; &lt;artifactId&gt;jest&lt;/artifactId&gt; &lt;version&gt;5.3.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.java.dev.jna&lt;/groupId&gt; &lt;artifactId&gt;jna&lt;/artifactId&gt; &lt;version&gt;4.5.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.codehaus.janino&lt;/groupId&gt; &lt;artifactId&gt;commons-compiler&lt;/artifactId&gt; &lt;version&gt;2.7.8&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; spark redis 和 ES DauApp 1.2 工具类 1.2.1 MyKafkaUtil1234567891011121314151617181920212223242526272829303132333435363738import org.apache.kafka.common.serialization.StringDeserializerimport java.util.Propertiesimport org.apache.kafka.clients.consumer.ConsumerRecordimport org.apache.spark.streaming.StreamingContextimport org.apache.spark.streaming.dstream.InputDStreamimport org.apache.spark.streaming.kafka010.&#123;ConsumerStrategies, KafkaUtils, LocationStrategies&#125;object MyKafkaUtil &#123; private val properties: Properties = PropertiesUtil.load("config.properties") val broker_list = properties.getProperty("kafka.broker.list") // kafka消费者配置 val kafkaParam = Map( "bootstrap.servers" -&gt; broker_list,//用于初始化链接到集群的地址 "key.deserializer" -&gt; classOf[StringDeserializer], "value.deserializer" -&gt; classOf[StringDeserializer], //用于标识这个消费者属于哪个消费团体 "group.id" -&gt; "gmall_consumer_group", //如果没有初始化偏移量或者当前的偏移量不存在任何服务器上，可以使用这个配置属性 //可以使用这个配置，latest自动重置偏移量为最新的偏移量 "auto.offset.reset" -&gt; "latest", //如果是true，则这个消费者的偏移量会在后台自动提交,但是kafka宕机容易丢失数据 //如果是false，会需要手动维护kafka偏移量 "enable.auto.commit" -&gt; (true: java.lang.Boolean) ) // 创建DStream，返回接收到的输入数据 // LocationStrategies：根据给定的主题和集群地址创建consumer // LocationStrategies.PreferConsistent：持续的在所有Executor之间分配分区 // ConsumerStrategies：选择如何在Driver和Executor上创建和配置Kafka Consumer // ConsumerStrategies.Subscribe：订阅一系列主题 def getKafkaStream(topic: String,ssc:StreamingContext): InputDStream[ConsumerRecord[String,String]]=&#123; val dStream = KafkaUtils.createDirectStream[String,String](ssc, LocationStrategies.PreferConsistent,ConsumerStrategies.Subscribe[String,String](Array(topic),kafkaParam)) dStream &#125;&#125; 1.2.2 PropertiesUtil1234567891011121314151617181920212223242526272829303132333435363738import org.apache.kafka.common.serialization.StringDeserializerimport java.util.Propertiesimport org.apache.kafka.clients.consumer.ConsumerRecordimport org.apache.spark.streaming.StreamingContextimport org.apache.spark.streaming.dstream.InputDStreamimport org.apache.spark.streaming.kafka010.&#123;ConsumerStrategies, KafkaUtils, LocationStrategies&#125;object MyKafkaUtil &#123; private val properties: Properties = PropertiesUtil.load("config.properties") val broker_list = properties.getProperty("kafka.broker.list") // kafka消费者配置 val kafkaParam = Map( "bootstrap.servers" -&gt; broker_list,//用于初始化链接到集群的地址 "key.deserializer" -&gt; classOf[StringDeserializer], "value.deserializer" -&gt; classOf[StringDeserializer], //用于标识这个消费者属于哪个消费团体 "group.id" -&gt; "gmall_consumer_group", //如果没有初始化偏移量或者当前的偏移量不存在任何服务器上，可以使用这个配置属性 //可以使用这个配置，latest自动重置偏移量为最新的偏移量 "auto.offset.reset" -&gt; "latest", //如果是true，则这个消费者的偏移量会在后台自动提交,但是kafka宕机容易丢失数据 //如果是false，会需要手动维护kafka偏移量 "enable.auto.commit" -&gt; (true: java.lang.Boolean) ) // 创建DStream，返回接收到的输入数据 // LocationStrategies：根据给定的主题和集群地址创建consumer // LocationStrategies.PreferConsistent：持续的在所有Executor之间分配分区 // ConsumerStrategies：选择如何在Driver和Executor上创建和配置Kafka Consumer // ConsumerStrategies.Subscribe：订阅一系列主题 def getKafkaStream(topic: String,ssc:StreamingContext): InputDStream[ConsumerRecord[String,String]]=&#123; val dStream = KafkaUtils.createDirectStream[String,String](ssc, LocationStrategies.PreferConsistent,ConsumerStrategies.Subscribe[String,String](Array(topic),kafkaParam)) dStream &#125;&#125; 1.2.3 工具类 RedisUtil1234567891011121314151617181920212223242526object RedisUtil &#123; var jedisPool:JedisPool=null def getJedisClient: Jedis = &#123; if(jedisPool==null)&#123;// println("开辟一个连接池") val config = PropertiesUtil.load("config.properties") val host = config.getProperty("redis.host") val port = config.getProperty("redis.port") val jedisPoolConfig = new JedisPoolConfig() jedisPoolConfig.setMaxTotal(100) //最大连接数 jedisPoolConfig.setMaxIdle(20) //最大空闲 jedisPoolConfig.setMinIdle(20) //最小空闲 jedisPoolConfig.setBlockWhenExhausted(true) //忙碌时是否等待 jedisPoolConfig.setMaxWaitMillis(500)//忙碌时等待时长 毫秒 jedisPoolConfig.setTestOnBorrow(true) //每次获得连接的进行测试 jedisPool=new JedisPool(jedisPoolConfig,host,port.toInt) &#125;// println(s"jedisPool.getNumActive = $&#123;jedisPool.getNumActive&#125;") // println("获得一个连接") jedisPool.getResource &#125;&#125; 1.3 配置1.3.1 config.properties123456# Kafka配置kafka.broker.list=hadoop102:9092,hadoop103:9092,hadoop104:9092# Redis配置redis.host=hadoop102redis.port=6379 1.4 样例类 case class Startup 123456789101112131415case class StartUpLog(mid:String, uid:String, appid:String, area:String, os:String, ch:String, logType:String, vs:String, var logDate:String, var logHour:String, var logHourMinute:String, var ts:Long ) &#123;&#125; 1.5 业务类RealtimeStartupApp 12345678910111213141516object DauAPP &#123; def main(args: Array[String]): Unit = &#123; val conf: SparkConf = new SparkConf().setAppName("Dau").setMaster("local[*]") val ssc = new StreamingContext(conf,Seconds(5)) val inputStream: InputDStream[ConsumerRecord[String, String]] = MyKafkaUtil.getKafkaStream(GmallConstant.KAFKA_TOPIC_STARTUP,ssc) inputStream.foreachRDD(rdd=&gt; println( rdd.map(_.value()).collect().mkString("\n")))ssc.start() ssc.awaitTermination() &#125;&#125; 启动jsonMocker 和 logger-cluster.sh 观看结果 redis string list set hash zset uv user visit dau daily active user hash 存两个值 zset 排序 list 和set list 可以重复 set不可以重复 type set key dau：2019-06-03 value： mids redis 过滤日活的策略 redis的安装配置 电影推荐系统 设计原则 尽可能的较少redis的打开关闭的次数 redis的对接检查 redis的去重12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152object DauApp &#123; def main(args: Array[String]): Unit = &#123; val sparkConf: SparkConf = new SparkConf().setAppName("dau_app").setMaster("local[*]") val ssc = new StreamingContext(sparkConf,Seconds(5)) val inputDstream: InputDStream[ConsumerRecord[String, String]] = MyKafkaUtil.getKafkaStream(GmallConstant.KAFKA_TOPIC_STARTUP,ssc) // inputDstream.foreachRDD&#123;rdd=&gt; // println(rdd.map(_.value()).collect().mkString("\n")) // &#125; // 转换处理 val startuplogStream: DStream[Startuplog] = inputDstream.map &#123; record =&gt; val jsonStr: String = record.value() val startuplog: Startuplog = JSON.parseObject(jsonStr, classOf[Startuplog]) val date = new Date(startuplog.ts) val dateStr: String = new SimpleDateFormat("yyyy-MM-dd HH:mm").format(date) val dateArr: Array[String] = dateStr.split(" ") startuplog.logDate = dateArr(0) startuplog.logHour = dateArr(1).split(":")(0) startuplog.logHourMinute = dateArr(1) startuplog &#125; startuplogStream.foreachRDD&#123;rdd=&gt; //executor rdd.foreachPartition &#123; startuplogStr =&gt; val jedis: Jedis = RedisUtil.getJedisClient for (startuplog &lt;- startuplogStr) &#123; val key = "dau:" + startuplog.logDate val value = startuplog.mid jedis.sadd(key, value) println(startuplog) &#125; jedis.close() &#125; &#125; ssc.start() ssc.awaitTermination() &#125;&#125; 2 ES的安装2.1 为什么用ES最强的是sql 很复杂的业务 Hive比mysql更强 spark mr rdd mysql sql写不出来 可以写 最差的是redis和hbase Es有自己的语言 ES的安装 为什么用yml 用户自定义函数 全文检索 ES比较强 关联查询 join sql 关系型数据库 ES索引 ES 复杂的查询 mysql 元数据细 写入的速度 redis最快 hbase Hive mysql es 最慢 建索引 工作复杂 不直接写到磁盘 缓冲区到磁盘 其实有延迟 数据块但是放弃了一致性 业务操作 关系型数据库容量低 严谨NoSql 严谨 mysql 数据存储 老二 Oracle老大 明细 对数据的容量的要求 英语单词的学习 英语如何学习 hbase es 更好 集群 单机 ES 3台机器里 6.8 不是6.8 装演示安装 6.6 的版本 之前的版本 5 和6 重要的不通电 解压缩 cluster-name 集群名称 天然集群 es的三个颜色 是不是集群 yellow 亚健康状态 green状态 搭建集群最简单的 最复杂的集群是mysql 官方的集群 redis的集群 说明集群的名称是一样的 一家的名字 名字不能一样 每台机器的真实的ip地址 三台机子 es的安装说明2.2将数据保存到es字段的类型 a b c name:: zhangsan string json { “name”:”zhangsan” } kibana的安装与使用 添加的字段 如何使用 1234567891011121314151617181920es5.x ex6.x ex7.x database indextable type index docrow documentcolumn field名字 彼此之间的区分es documentfield6 系列 表的结构的概念取消了7 的时候type 就了document和field不纠结5的架构了string&#123;&#125; 聚合的字段必须使用 keyword es查询之前准确的定义字段 不能精确的推断 第三章 cannal1 为什么使用canal1.1 作用同步sql 做拉表 更新redis 某些情况无法从日志中获取信息 ，同时也无法利用sqoopp、等ETL工具实现对数据的实时监控 1.2 canal的工作原理工作原理简单 把自己伪装成salve 从master复制信息 1.3 了解一下mysql的binlogmysql二进制日志可以说是mysql最重要的日志了 他记录了dml 和ddl语句，以事件的形式记录，还包括语 语句所消耗的时间 开启二进制日志会造成1%的性能的消耗 1.3..2二进制文件的应用的场景Mysql Replication 在Master端开启binlog master把这个文件传递给slaves 达到master-slaves 数据一致的目的 其二 自然就是数据恢复了 通过mysqlbinlog工具来恢复数据 、二进制日志包括两类文件 二进制索引文件 二进制日志文件 ddl语句和dml语句 数据导入到 kibina的使用 es和mysql是正好相反 es速度快 所有的字段 mysql不会给你建立索引 字段的分类 建立索引但是不分词 jest jedis 操作es jest 操作es的 说一下es ES的保存2 mysql的安装3 canal的安装4 es的保存2批量插入ES 12 ES查询总数数据通过接口发布出来 java 日活的总数 分时间点的日活 ES中怎么写 12 查询的数据 将查询的数据发布成接口 发布数据的模块 新建工程spring 项目 com.atguigu.gmall1205 建立 **** es的分时查询]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>项目实战</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[社交项目]]></title>
    <url>%2Fblog4%2F2019%2F06%2F17%2FSocialProject%2F</url>
    <content type="text"><![CDATA[1项目介绍1.1 扯淡需要搭建成服务的功能制作镜像—-》》 docekr 镜像做容器微服务与容器的关系springCloud 实现微服务之间的通信spring全家桶按照传统的方式 UML 建模语句大的体系统一建模语言 power designer 建模数据库干什么用的 谈业务用的 公司 做了开发 什么也不用管开发出来 投资方 讲架构 UML建模语言 所有的类 powerdesigner 电脑安装一个 建模相关的东西 他们用这个谈公司 谈投资 1.2 系统架构设计简化了开发 专注于开发就好 nginx apache公司优化的发布软件的服务 nginx 解压可用 解压到没有空格的目录 json格式数据 返回json数据 前后端的约定 传这个返回这个 自己设定的状态码 1.3 RESTful的开发风格为了restful而生 spring全家桶 在spring全家桶用的越来越多 增删改查 4钟方法 get put delete post get 安全 幂等 获取表示 变更时获取表示 Post 不安全 且不幂等 插入 数据库来了一条 做了重复操作 Put 不安全 幂等 同一条更新语句同时执行了两次 Delete 不安全 幂等 删除资源 Docker 创建mysql 开发微服务 连一个公用的mysql mysql 下载镜像 pull即可 mq消息队列 dock pull 。。。。 配置国内的源 国内的服务器有3个 目前的还行 连接 配置ip地址 1.3 创建mysql的微服务测试工具PostMan 父工程 1.4 配置商品sringboot的仓库1234567891011121314151617181920212223242526272829303132333435363738&lt;!-- 配置maven的默认仓库--&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;spring-snapshots&lt;/id&gt; &lt;name&gt;Spring Snapshots&lt;/name&gt; &lt;url&gt;https://repo.spring.io/snapshot&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;spring-milestones&lt;/id&gt; &lt;name&gt;Spring Milestones&lt;/name&gt; &lt;url&gt;https://repo.spring.io/milestone&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;spring-snapshots&lt;/id&gt; &lt;name&gt;Spring Snapshots&lt;/name&gt; &lt;url&gt;https://repo.spring.io/snapshot&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;pluginRepository&gt; &lt;id&gt;spring-milestones&lt;/id&gt; &lt;name&gt;Spring Milestones&lt;/name&gt; &lt;url&gt;https://repo.spring.io/milestone&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; 12 ResponseBoby javabean –&gt;&gt;json RequestBody json—&gt;&gt;javaBean Result类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566&lt;!-- springboot最基本的架构--&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;!-- 配置maven的默认仓库--&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;spring-snapshots&lt;/id&gt; &lt;name&gt;Spring Snapshots&lt;/name&gt; &lt;url&gt;https://repo.spring.io/snapshot&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;spring-milestones&lt;/id&gt; &lt;name&gt;Spring Milestones&lt;/name&gt; &lt;url&gt;https://repo.spring.io/milestone&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;spring-snapshots&lt;/id&gt; &lt;name&gt;Spring Snapshots&lt;/name&gt; &lt;url&gt;https://repo.spring.io/snapshot&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;pluginRepository&gt; &lt;id&gt;spring-milestones&lt;/id&gt; &lt;name&gt;Spring Milestones&lt;/name&gt; &lt;url&gt;https://repo.spring.io/milestone&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; common模块 1.5 封装对象Result1234567public class Result &#123; private boolean flag; private Integer code; private String message;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859多了object类public class Result &#123; private boolean flag; private Integer code; private String message; private Object data; public Result(boolean flag, Integer code, String message) &#123; this.flag = flag; this.code = code; this.message = message; &#125; public Result(boolean flag, Integer code, String message, Object data) &#123; this.flag = flag; this.code = code; this.message = message; this.data = data; &#125; public Result() &#123; &#125; public boolean isFlag() &#123; return flag; &#125; public void setFlag(boolean flag) &#123; this.flag = flag; &#125; public Integer getCode() &#123; return code; &#125; public void setCode(Integer code) &#123; this.code = code; &#125; public String getMessage() &#123; return message; &#125; public void setMessage(String message) &#123; this.message = message; &#125; public Object getData() &#123; return data; &#125; public void setData(Object data) &#123; this.data = data; &#125;&#125; 注意是什么有参构造 1.6 分页对象1234567891011121314151617181920212223242526272829303132PageResultpublic class PageResult&lt;T&gt;&#123; private long total; private List&lt;T&gt; rows; public PageResult() &#123; &#125; public long getTotal() &#123; return total; &#125; public void setTotal(long total) &#123; this.total = total; &#125; public List&lt;T&gt; getRows() &#123; return rows; &#125; public void setRows(List&lt;T&gt; rows) &#123; this.rows = rows; &#125; public PageResult(long total, List&lt;T&gt; rows) &#123; this.total = total; this.rows = rows; &#125;&#125; 2的12次方 雪花算法 提供了分布式ID生成器 IdWorkder 基础微服务 增删改查 1.7 跨预处理CrossJoin123456789@SpringBootApplication@CrossOrigin //跨预处理public class BaseAppApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(BaseAppApplication.class); &#125;&#125; 1.8 docker 安装mysql5.6前提 centos7 123456docker search mysqldocker pull mysql:5.6docker run -p 3306:3306 --name mysql -v /mysql/conf:/etc/mysql/conf.d -v /mysql/logs:/logs -v /mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.6 含义的讲解 -v 代表挂载前面的是Linux主机 后面的是 对应mysql的端口 启动容器的命令 编写简单的CURD 运行镜像 docker exec -it MySQL运行成功后的容器ID /bin/bash 1.9 根据标签编写CRUD代码分布式开发必须加序列化 使用IO流在不同的平台传输 PostMan 异常处理 controllerAdvice postMan的使用方法 发送消息 接受信息 访问 异常处理的类 添加 int i=1/0 在postman上显示false 内部用 不会在其他的模块用 service dao pojo 三个模块就够了 @PathValiable 12@RestControllerAdvice@ExceptionHandler(value = Exception.class) 1234@RequestMapping(method = RequestMethod.POST) @RequestMapping(value="/&#123;labelId&#125;",method = RequestMethod.GET) @RequestMapping(value="/&#123;labelId&#125;",method = RequestMethod.GET) public Result findById(@PathVariable("labelId") String labelId) 第二天 查询与缓存2 .1基础微服务 条件查询2.1.1 启动docker1systemctl start docker docker start id name 2.1.2 写的方法 先写service 再写dao.+ 2.2 分页条件查询12345678 @RequestMapping(value="/label/search/&#123;page&#125;/&#123;size&#125;",method = RequestMethod.POST)public Result pageQuery(@RequestBody Label label,@PathVariable int page,@PathVariable int size)&#123; Page&lt;Label&gt; pageData=labelService.pageQuery(label,page,size); return new Result(true,StatusCode.OK,"查询成功",new PageResult&lt; Label&gt;(pageData.getTotalElements(),pageData.getContent()));&#125; 招聘代码的生成 、什么是DockFile DockFile的镜像 构建卷积神经网络 热门企业 hql jpql业务说明 hql的差别 热门企业 推荐职位 最新职位 必须要写sql语句 热门企业列表 推荐职位列表 最新职位列表 问答微服务开发 中间表 联查 中间表 @Query的用法 直接写sql语句 直接写jpq语句 现在sqlyog上写 然后在代码里面写 文章微服务开发springData JPA的联系结束 springdata 的使用 dockketfile ​ 0 想其他的方法决绝现在面对 springCache的使用 springdataRedis的使用、 Docker 安装redisdocker pull redis docker image redis mkdir /redis/data docker run -p 6379:6379 -v /redis/data : /data -d redis redis-server docker exec -it xxx redis-cli /bin/bash 1docker exec -it 43f7a65ec7f8 redis-cli spring Cache 缓存无法设置过期时间 运用的时间 场景 spring Cache 不破坏模块 面试题 缓存 项目那一部分用到缓存 一个redis 一个springCache 需不需要设置过期时间 单点登录global session 不是真的全局session 整个servlet 一个session对应一个servlet容器 不能一个session被多个servlet共用 项目代码 第三方的分布式项目 jwt cas 有状态的登录服务端储存信息 追随者无状态的流行 jwt 无状态登录jwt越来越流行了 一次登录 在分布式的任何形式都可以使用 qq音乐，一系列的软件都登录了 流行了 服务器端不存 登录信息 存的话 session存 客户端 服务端 客户端 cookie 服务器端 session cookie的id 和session的id你对应 服务端没东西 没有状态 知识验证有没有这个东西 day03 的文档型数据库复习 新的知识点 mongodb 有可能问到 随着大数据的普及 mongoDB就是为了大数据而生的 大数据班 大数据工程师 java的高级工程师 完全不会java做大数据 不行的 深圳 最早开大数据 一直到项目二 到了最后转了大数据的相关的东西 mongoDB的特点和体系结构 实际开发 不会直接敲命令操作 java端操作mongodb springDataMongoDB完成吐槽服务的开发 最重要的 MongoDB 什么时候用Oracle Mysql 关系型数据库 redis 菲关系型数据库 mongoDB菲关系数据库 但是最像关系型数据库 关系型数据库和非关系型数据库之间的数据库的关系关系型数据库 表与表之间有关系 一对多 对对多 多对多 中间表关联 什么时候用mongoDB1 数据量大 2 写入操作频繁 3 价值低 淘宝买东西 什么是MongoDB跨平台 面向文档 java 面向对象 java中 关系型数据库 一个集合 MongoDb中的一个文档 类似json的Bson格式 对json的扩展 操作的很多数据可以认为是你json MongoDb的特点支持很多种语言 Oracle Connection 对象 username password 这些语言不能用Oracle、 mysql 对mongoDb的支持 java javascript操作 一般不建议 Ertlong和.net语言基本不怎么用了 主流语言java 体系结构 集合多个文档的集合 表 document collection database MongoDB的数据类型的介绍null {“x”:null} true false {“x”:true} 数值 {“x”:3.14} {“x”:3} bson中 都是浮点型对于数值更加严格{“x:NumberLong(3)} 字符串： UTF-8的字符串 表示 {x：”/[abc]”/} ^$ 数组 {“x”:[a,b,c]} 对象ID 安装mongodb 安装 配置环境变量 bin 然后启动服务端 启动客户端 mongo命令 ![1569739678347]SocialProject/1569739678347.png) 表的结构 直接写id mongodb相当主键 关系型数据库 吐槽业务的说明吐槽 丢了 无伤大雅 选型mongoDB mongoDB的吞吐量比关系型数据库大的多 id 内容 日期 回复数 可见 上级id 无限制吐槽 复杂 类似于Oracle数据库的emp表 树型结构 一张表展现出n层的结构 优先考虑emp表 吐槽 回复 回复的人 再次回复 创建数据库和集合docker 连接Mongo ![1569740634113]SocialProject/1569740634113.png) ![1569740793266]SocialProject/1569740793266.png) 插入数据 1234 db.spit.insert(&#123;_id:"1",content:"我还是没有想明白到底为啥出错",userid:"1012",nickname:"小明",visits:NumberInt(2020)&#125;);db.spit.insert(&#123;_id:"2",content:"加班到半夜",userid:"1013",nickname:"凯撒",visits:NumberInt(1023)&#125;);db.spit.insert(&#123;_id:"3",content:"手机流量超了咋办？",userid:"1013",nickname:"凯撒",visits:NumberInt(111)&#125;);db.spit.insert(&#123;_id:"4",content:"坚持就是胜利",userid:"1014",nickname:"诺诺",visits:NumberInt(1223)&#125;); docker 构建mongo 123456docker ps -a在机器上启动mongodocker run -p 27017:27017 -v /mongo/db:/data/db -d mongo + 镜像id在容器上启动客户端docker run -it mongo mongo --host 192.168.2.110 docker run -di –name=tensquare_mongo -p 27017:27017 mongo Day04ElasticSearch安装直接解压 9200端口和9300端口 直接解压进入bin目录 开箱即用 elasticsearch Restfule操作ES head插件的安装Day05 rabbitMQRabbitMQ的简介kafka&gt; RabbitMQ &gt;activeMQ 哪个快 哪个安全 RabbitMQ 中间点 大数据 kafka java RabbitMQ 消息队列 知道几种 最安全的rabbitmq的 电商用RabbitMQ也有 但是比较少 保证订单足够的安全 \使用ActiveMQ 追求效率高的话RabbitMQ 金融可以用RabbitMQ RabbitMQ Erlang AMQP 高级消息队列 可靠 灵活 支持消息集群 管理界面 rabbitMQ的架构发送消息 RabbitServer 送快递 100个人 ![1569743989061]SocialProject/1569743989061.png) 消息的发送者 消息的接受者 ActiveMQ 100个消息 一个队列 消息发给交换器 直接 分裂 主题 三种连接模式 经过交换器 RabbitMQ和ActiveMQ的最重要的区别 主要的概念RabbitMQ server Producer Consumer Exchange Queuue Routing Key windows条件下安装rabbitMQ Docker 安装rabbitMq、docker pull rabbitmq ​ b9e17734a1b2 1docker run -d -p 5672:5672 -p 15672:15672 --name rabbitmq rabbitmq + 镜像 12docker run ‐di ‐‐name=tensquare_rabbitmq ‐p 5671:5617 ‐p 5672:5672 ‐p4369:4369 ‐p 15671:15671 ‐p 15672:15672 ‐p 25672:25672 rabbitmq]]></content>
      <categories>
        <category>javaEE</category>
      </categories>
      <tags>
        <tag>项目实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卡扣项目]]></title>
    <url>%2Fblog4%2F2019%2F06%2F12%2FCarsListenerProject%2F</url>
    <content type="text"><![CDATA[车流量监控项目1 车流量监控介绍1.1 数据的采集1.1.1 数据从哪里来埋点卡口信息的话 每次拍摄信息都会传到服务端 网站或者页面设置埋点 socket后台获取 和前端开发人员联系好 卡口数据 厂商约定 Flume监控指定的文件夹 转移到HDFS里 大部分在Hive Hive有计算的negligible 还有一条流程 实时数据 实时数据的话 通常在分布式消息队列中读取，如kafka 实时的log 实时的写入消息队列，实时从kafka读取数据 log日志 Flume1.2 模块介绍1.2.1 卡流量分析 SparkCore功能点 top5 获得卡扣号通过的车最多 使用架构 卡口车流量转化率 Spark Core 各区域车流量最高top5的道路统计 Spark SQL 稽查布控 道路实时拥堵统计 Spark Streaming 一个卡扣号对应多个摄像头 基础数据介绍 大数据开发流程 需求分析 功能需求]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>项目实战</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MachineLearning]]></title>
    <url>%2Fblog4%2F2019%2F05%2F16%2FMachineLearning%2F</url>
    <content type="text"><![CDATA[1 机器学习工具理论和技术小前提在学习机器学习之前首先应该具备一定的数学基础 还有就是掌握python基本库的使用 主要就是numpy,pandas,和matplotlib 安装好python的环境 anaconda的安装推荐安装anaconda的环境 anaconda可以切换不同的虚拟环境 它是一个方便的pythob包的管理和环境管理软件，可以用它配置不同的项目环境 在安装的时候如果需要在cmd中使用我们的环境 我们需要配置环境变量 在linux的操作系统中也是一样的 conda env –list 查看我们安装的anaconda的虚拟环境 我们会使用python的不同版本 如果使用2.7版本我们只需要敲入命令 1conda create -n your_env_name python=2.7 如果我们使用的是python的3.x版本 比如3.5 的版本同时呢我们还需要pandas numpy的虚拟环境 1conda create -n your_env_name python=3.5 numpy pandas 当我们指定的时候 如果我们不指定版本的话 它默认显示的是最新的版本 1conda update --all 例如我现在要创建一个名叫 learningpy的基于py3的环境 12conda update --allconda create -n learningpy python=3.7 之后会出现让我们安装一些包 比如你要安装TensorFlow，而TensorFlow会用到很多像前置包像pandas、matiplot等，如果你在单纯的python下没有安装pandas等包就直接安装TensorFlow，那么和有可能无法使用，而使用conda安装TensorFlow将会询问你并自动帮你把缺少的前置包安装好 创建完新的环境我们查看环境列表 只要你的硬盘够大，你就可以创造很多个不同的环境那么现在我们有多个环境了，如何切换环境呢？windows 123456activate 环境名` 退出时记得退出命令哦 `deactivate` linux和mac用户的命令不一样 `source source activate 环境名` `source deactivate 环境名 anaconda包管理上文我们提到了创建环境时的包管理，那么我们创建好环境后如何进行包的安装、更新和卸载呢？ 当然我们任然可以通过pip安装更新删除 这里我们介绍conda 12345678910111213conda list 列举当前环境下的所有包conda list -n packagename 列举某个特定名称包conda install packagename 为当前环境安装某包conda install -n envname packagename 为某环境安装某包conda search packagename 搜索某包conda updata packagename 更新当前环境某包conda update -n envname packagename 更新某特定环境某包conda remove packagename 删除当前环境某包conda remove -n envname packagename 删除某环境环境某包conda本身和anaconda、python本身也算包conda update condaconda update anacondaconda update python conda默认源可能速度比较慢 可以添加其他源，常用的有清华TUNA 123conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/conda config --set show_channel_urls yes 在包后面显示来源 第三条执行安装包时会显示来自哪个源，一目了然 source.png 教育网用户可以添加ipv6源，速度很快 123conda config --add channels https://mirrors6.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors6.tuna.tsinghua.edu.cn/anaconda/pkgs/main/conda config --set show_channel_urls yes 在包后面显示来源 anaconda实现原理解析anaconda在目录下的envs文件夹保存了环境配置，也就是把所有的安装在这个环境下的包放在同一个文件夹中 当创建一个新环境时，anaconda将在envs中创建一个新的文件夹，这个文件夹包括了你安装在这个环境中的所有包 anaconda通过巧妙的包管理解决的一个大难题，确实方便了很多。 下一期会讲如何在第三方软件中使用anaconda的不同环境配置。 2 机器学习简介2.1 什么是机器学习 学习是人工智能的一个分支。人工智能的研究是从以“推理”为重点到以“知识”为重点，再到以“学习”为重点，一条自然、清晰的脉络。机器学习是实现人工智能的一个途径，即以机器学习为手段解决人工智能中的问题。机器学习算法是一类从数据中自动分析获得规律（模型），并利用规律对未知数据进行预测的算法 2.2 为什么需要机器学习 21世纪机器学习又一次被人们关注，而这些关注的背后是因为整个环境的改变，我们的数据量越来越多，硬件越来越强悍。急需要解放人的生产力，自动去寻找数据的规律。解决更多专业领域的问题。机器学习已广泛应用于数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人等领域. 2.3 开发机器学习的步骤是什么 1 收集数据 ​ 网络爬虫 ​ 从RSS反馈或者API中得到信息、设备发送过来的实测数据。 2 准备输入数据 ​ 数据的格式必须必符合要求 3 分析输入数据 ​ 判断数据中有没有垃圾数据 4 训练算法 ​ 机器学习算法从这一步才真正开始学习。如果使用无监督学习算法，由于不存在 目标变量值，故而也不需要训练算法，所有与算法相关的内容在第5步 ​ 5 测试算法 使用上一步机器学习学到的知识信息 ，评估算法的准确率，根据需要重新训练你的算法 2.4 需要具备的技能1 学会分析问题 2 掌握算法的基本能思想，学会对问题用相应的算法解决 3 学会试用库和框架解决问题 3 sklearn与特征工程3.1 数据来源 类型 大部分的数据都来自已有的数据库，如果没有的话也可以交给很多爬虫工程师去采集，来提供。也可以来自平时的记录，反正数据无处不在，大都是可用的。 数据的类型按照机器学习的数据分类我们可以将数据分成： 标称型：标称型目标变量的结果只在有限目标集中取值，如真与假(标称型目标变量主要用于分类) 数值型：数值型目标变量则可以从无限的数值集合中取值，如0.100，42.001等 (数值型目标变量主要用于回归分析) 按照数据的本身分布特性 离散型 连续型 那么什么是离散型和连续型数据呢？首先连续型数据是有规律的,离散型数据是没有规律的 离散变量是指其数值只能用自然数或整数单位计算的则为离散变量.例如，班级人数、进球个数、是否是某个类别等等 连续型数据是指在指定区间内可以是任意一个数值,例如，票房数据、花瓣大小分布数据 3.2 数据的特征抽取 现实世界中多数特征都不是连续变量，比如分类、文字、图像等，为了对非连续变量做特征表述，需要对这些特征做数学化表述，因此就用到了特征提取. sklearn.feature_extraction提供了特征提取的很多方法 分类特征变量的提取我们将城市和环境作为字典数据，来进行特征的提取。 sklearn.feature_extraction.DictVectorizer(sparse = True) 将映射列表转换为Numpy数组或scipy.sparse矩阵 fit_transform(X,y) 应用并转化映射列表X，y为目标类型 inverse_transform(X[, dict_type]) 将Numpy数组或scipy.sparse矩阵转换为映射列表sparse 是否转换为scipy.sparse矩阵表示，默认开启 文本特征提取（只限于英文） 文本的特征提取应用于很多方面，比如说文档分类、垃圾邮件分类和新闻分类。那么文本分类是通过词是否存在、以及词的概率（重要性）来表示。 (1)文档的中词的出现 数值为1表示词表中的这个词出现，为0表示未出现 sklearn.feature_extraction.text.CountVectorizer() 将文本文档的集合转换为计数矩阵（scipy.sparse matrices） fit_transform(raw_documents,y) 文本的特征提取应用于很多方面，比如说文档分类、垃圾邮件分类和新闻分类。那么文本分类是通过词是否存在、以及词的概率（重要性）来表示。 (1)文档的中词的出现 数值为1表示词表中的这个词出现，为0表示未出现 sklearn.feature_extraction.text.CountVectorizer() 将文本文档的集合转换为计数矩阵（scipy.sparse matrices） 方法fit_transform(raw_documents,y) 学习词汇词典并返回词汇文档矩阵 1234from sklearn.feature_extraction.text import CountVectorizercontent = ["life is short,i like python","life is too long,i dislike python"]vectorizer = CountVectorizer()print(vectorizer.fit_transform(content).toarray()) 需要toarray()方法转变为numpy的数组形式 温馨提示：每个文档中的词，只是整个语料库中所有词，的很小的一部分，这样造成特征向量的稀疏性（很多值为0）为了解决存储和运算速度的问题，使用Python的scipy.sparse矩阵结构 (2)TF-IDF表示词的重要性 TfidfVectorizer会根据指定的公式将文档中的词转换为概率表示。（朴素贝叶斯介绍详细的用法） class sklearn.feature_extraction.text.TfidfVectorizer() 方法fit_transform(raw_documents,y) 学习词汇和idf，返回术语文档矩阵。 12345from sklearn.feature_extraction.text import TfidfVectorizercontent = ["life is short,i like python","life is too long,i dislike python"]vectorizer = TfidfVectorizer(stop_words='english')print(vectorizer.fit_transform(content).toarray())print(vectorizer.vocabulary_) 图像特征提取3.3 数据的特征预处理数据的特征预处理单个特征（1）归一化 归一化首先在特征（维度）非常多的时候，可以防止某一维或某几维对数据影响过大，也是为了把不同来源的数据统一到一个参考区间下，这样比较起来才有意义，其次可以程序可以运行更快。 例如：一个人的身高和体重两个特征，假如体重50kg，身高175cm,由于两个单位不一样，数值大小不一样。如果比较两个人的体型差距时，那么身高的影响结果会比较大，k-临近算法会有这个距离公式。 min-max方法 常用的方法是通过对原始数据进行线性变换把数据映射到[0,1]之间，变换的函数为： X′=x−minmax−minX^{‘}{=}\frac{x-min}{max-min}X′=max−minx−min 其中min是样本中最小值，max是样本中最大值，注意在数据流场景下最大值最小值是变化的，另外，最大值与最小值非常容易受异常点影响，所以这种方法鲁棒性较差，只适合传统精确小数据场景。 min-max自定义处理 这里我们使用相亲约会对象数据在MatchData.txt，这个样本时男士的数据，三个特征，玩游戏所消耗时间的百分比、每年获得的飞行常客里程数、每周消费的冰淇淋公升数。然后有一个 所属类别，被女士评价的三个类别，不喜欢、魅力一般、极具魅力。 首先导入数据进行矩阵转换处理 4 机器学习模型监督学习回归模型-线性模型 一元线性回归 多元线性回归 非线性回归模型 最小二乘法 分类模型– k近邻（KNN） 无监督学习聚类 k均值（k-means） 降维 TFIDF算法12import numpy as npimport pandas as pd 1. 定义数据和预处理12345678910111213docA=&quot;The cat sat on my bed&quot;docB=&quot;The dog sat on my knees&quot;bowA=docA.split(&quot; &quot;)bowB=docB.split(&quot; &quot;)bowA#构建词库 A和B所有的词合在一起 list 转成set 用set 集合本身有一个方法去重# union相当于求一个并集wordSet= set(bowA).union(set(bowB))wordSet&#123;&apos;The&apos;, &apos;bed&apos;, &apos;cat&apos;, &apos;dog&apos;, &apos;knees&apos;, &apos;my&apos;, &apos;on&apos;, &apos;sat&apos;&#125; 2 进行词数统计1234567891011121314#用一个统计字典来保存词出现的次数 从集合创建一个字典 把他们当做keywordDictA= dict.fromkeys(wordSet, 0)wordDictB= dict.fromkeys(wordSet, 0)wordDictA# 遍历文档 统计词数# 对于每一个bowA出现的次数for word in bowA: wordDictA[word]+=1for word in bowB: wordDictB[word]+=1pd.DataFrame([wordDictA,wordDictB]) bed knees my dog sat The cat on 0 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 3 计算词频123456789101112131415161718192021def computeTF(wordDict,bow): #用一个字典对象记录tf,把所有的词对应在bow文档中的tf都算出来 tfDict=&#123;&#125; nbowCount=len(bow) for word,count in wordDict.items(): tfDict[word]=count / nbowCount return tfDicttfA=computeTF(wordDictA,bowA)tfB=computeTF(wordDictB,bowB)tfA&#123;&apos;bed&apos;: 0.16666666666666666, &apos;knees&apos;: 0.0, &apos;my&apos;: 0.16666666666666666, &apos;dog&apos;: 0.0, &apos;sat&apos;: 0.16666666666666666, &apos;The&apos;: 0.16666666666666666, &apos;cat&apos;: 0.16666666666666666, &apos;on&apos;: 0.16666666666666666&#125; 4 计算逆文档频率 idf1234567891011121314151617181920212223242526272829# wordcountList的信息 里面包含了各个文档的信息def computeIDF(wordDictList): #用一个字典对象保存idf的结果，每个词作为key,初始值为 0 idfDict=dict.fromkeys(wordDictList[0],0) N=len(wordDictList) import math for wordDict in wordDictList: #遍历字典中的每个词汇,统计Ni for word,count in wordDict.items(): if(count&gt;0): #先把Ni增加1 存入到idfDict里面 idfDict[word]+=1 #已经得到所有词汇对应的Ni 根据公式把它替换成为最后的idf值 for word,ni in idfDict.items(): idfDict[word]=math.log10((N+1)/(ni+1)) return idfDictidfs=computeIDF([wordDictA,wordDictB])idfs &#123;&apos;bed&apos;: 0.17609125905568124, &apos;knees&apos;: 0.17609125905568124, &apos;my&apos;: 0.0, &apos;dog&apos;: 0.17609125905568124, &apos;sat&apos;: 0.0, &apos;The&apos;: 0.0, &apos;cat&apos;: 0.17609125905568124, &apos;on&apos;: 0.0&#125; 5 计算TF-IDF我们通过计算得到文档中的词汇的关键的程度 12345678910111213def computeTFIDF(tf,idfs): tfidf=&#123;&#125; for word,tfval in tf.items(): tfidf[word]=tfval * idfs[word] return tfidftfidfA=computeTFIDF(tfA,idfs)tfidfB=computeTFIDF(tfB,idfs)pd.DataFrame([tfidfA,tfidfB]) Out[9]: sat knees cat The dog bed my on 0 0.0 0.000000 0.029349 0.0 0.000000 0.029349 0.0 0.0 1 0.0 0.029349 0.000000 0.0 0.029349 0.000000 0.0 0.0]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作系统]]></title>
    <url>%2Fblog4%2F2019%2F05%2F12%2FOperatingSystem%2F</url>
    <content type="text"><![CDATA[操作系统第一章 概述1课程概述 1.1 课程介绍基本概念及原理 中断及系统调用 进程和线程 内存管理 进程及线程 调度 同步 1.2 操作系统实验 准备 启动 物理内存的管理 虚拟内存 进程及线程 内核线程管理 用户进程管理 cpu调度 同步和互斥 文件系统 #################### 如何设计文件系统 操作系统和真实环境的比较 真正的比较计算机 1.3 预备知识计算机结构原理 数据结构 C和汇编语言 2 什么是操作系统没有一个精确的定义 2.1 功能上 用户角度 控制软件 ​ 管理应用程序 ​ 服务 ​ 杀死应用程序 对下 资源管理 分配资源 管理外设 2.2 功能的抽象 2.3 操作系统的层次架构 软件 应用软件 系统软件 系统 功能 操作系统 编译器 共享的程序库 操作系统 硬件管理和控制 面向应用程序 shell windows GUI 操作系统暴露的接口 kernel在shell之下 操作系统内核的角度看问题 cpu 内存 磁盘 内存 物理内存 虚拟内存 技术手段 在有限的物理内存之下 disk 磁盘块 底层 抽象出文件系统 中断处理 和设备驱动 2.4 操作系统的特征1 并发 一个cpu 并行 同时执行 多个cpu 存在多个应用程序 OS管理和调度 2 共享 同时访问资源 互斥共享 A B 两个块 如何共享 3 虚拟 多道程序技术 一个计算机 虚拟出多台机器 4 异步 什么时候结束 输出的结果是相同的 程序走走停停的 但是OS要保证运行的结果相同 3 为什么学习操作系统3.1综合课程编程语言 数据结构 算法 计算机的体系结构 材料 操作系统概念和原理 源代码 技能 操作系统的设计和实现 3.2 已经有何很多操作系统 要不要学操作系统 底层的操作 操作系统进行设计 改进和扩展 分析了解和掌握 很多知识 能力 Windows Linux 硬件发展不断的变化 一直向前发展 学习 atguigu sudo atguigu用户 ip地址和防火墙 我听发哦的我会忘记 3.3 操作系统 计算机研究的基石之一 顶级的计算机操作系统 庞大的操作系统 支持底层的架构 ACM 学术界 工业界 顶级会议 实际的操作系统 很大 很复杂 并发导致的编码困难 硬件的错误 管理计算机系统的核心软件 对算法的设计提出了要求 操作系统出错 意味着计算机出错 操作系统 是安全的基础 原理和概念 不代表操作系统主要关注的点 算法 关键部分的一个小部分 IO的磁盘调度 进程调度 关注小了 滞后当前研究和产业现状 权衡 时间和空间 空间和时间 操作系统需要权衡 中断 异常 IO 如何工作 如何对硬件的特征处理 上层的概念和理解 理解C代码和汇编代码 系统级的平衡 具体的角度 4 如何学习操作系统我看到的我能记住 我能做的 更深入的了解 动手操作 实现 尝试完成实验提出的要求 5 操作系统实例 面向桌面的操作系统 面向服务器的操作系统 手机 移动中断 工控领域 Unix BSD操作系统 Unix 基于C语言的Linux系统 改变世界 BSD 在unix的基础上发展 Linux BSD 发行软件 在网站的协议 广大的发展 基于Unix BSD Solaris IOS 惠普 Linux操作系统 一个学生学习计算机知识 用一个操作系统 经历了漫长的发展 很多的操作系统基于Linux Ubuntu Linux内核 都是基于Linux内核开发的 Windows操作系统 最早是dos IBM 个人计算机 GUI 图形化界面 桌面 服务器 手机 服务器和终端 微软 —-》》 大众完成特定的事情 计算机的推广 实施操作系统 6 操作系统的历史和发展计算机硬件 1921 2012 年 计算机发生了很大的变化 科学家 —-》》 日常 6.1 早期的计算机计算机 早期的计算机 输入 计算机 输出 单用户的处理过程 CPU的计算能力强 计算过程流水线化 批处理阶段 6.2 顺序执行和批处理 6.3 内存容量增大 CPU执行多个程序 cpu很贵 程序执行 操作 IO IO的操作效率 远远比不了CPU的速度 多个IO操作 Read 让Write 使用CPU的资源 中断 通知操作系统 程序2 停止 程序1 执行 调度 切换 交互性不够好 与计算机进行交互 分时 千分之一秒产生一次分时 第一个程序占 交给第二个 程序 人的反应速度慢 将时间分成很小的时间段 有了分时调度 人们可以更方便的执行计算机程序 中断 6.4 个人电脑操作系统80 年代 一个一般用户用一台电脑 价钱提高 功能提升 文字处理 数据存储 提高IO的交互性 两个趋势 集成电路的发展 一个cpu集成多个cpu核 网络得到了飞速的发展 分布式操作系统 用户前端 数据中心 Internet Internet 松紧 耦合 系统 操作系统的演变 6.5 计算机的未来的发展趋势大数据 云计算 大量的嵌入式设备 很多的工作交给计算机来处理 给人的学习方便 6 7 操作系统结构简单的操作系统 比较弱 操作系统 x80 8086机器 硬件的限制 计算机很难有突破 早期面向个人的计算机 服务器 计算机 更高层次的设计 Unix操作系统 C语言 实现 C语言方便移植 获得图灵奖和美国总统访问 向下 向上 分析uCore操作系统 实际的OS在硬件上是如何发展的 把内核尽量变的尽量小巧 微内核的设计 是一种服务的形式实现 松耦合的架构 地址隔离 无法 影响彼此 数据导内核 内核 产业界 很少采用微内核 性能 操作系统的设计分成两块 Office 面向office 的Linux Kernel IOS Kernel 完成对硬件管理 速度 OS 虚拟出操作出多台操作系统 计算机的发展 CPU的计算能力快速发展 充分分发挥计算机的效率 8 小结什么是操作系统 为什么学习 如何学习操作系统 操作系统实例 操作系统结构 第二章2.1 启动 中断 异常 和 系统调用 启动 接口 控制外设 中断 异常 系统调用 实现 中断 异常 和 系统调用 CPU Memory IO Disk 存放OS BIOS 基本IO处理系统 检查外射 加载软件 OS 放在OS Bootloader 加载OS的 BIOS 干什么 特定的地址开始执行 地址是固定的 CS 和 IP CS 段寄存器 IP 指令指针寄存器 POST（加电自检） 寻找显卡 外射 把bootloader 硬盘放到内存中去 bootloader 代码块 加载到内存 控制权交给os os的最起始的地址 操作系统与设备程序交互 interface 中断IO 系统调用 异常 外射 中断 外设 异常和系统调用 应用程序 操作系统 特殊的软件 可以信任的软件 屏蔽底层的复杂性 中断来源于外设 键盘 鼠标 网卡 声卡 异常 应用程序意向不到的行为 系统调用 应用程序强求操作提供服务 读写文件 网络包 系统调用 中断 异步 异常 同步 系统调用 异步或者同步 发出请求 返回的时间是异步的 发数据 马上做其他的事情 完成异步发送完成的请求 你做一件事 等一会返回结果 做其他的事情 系统调用不会重复的使用 2.2 中断 异常和系统调用 表 key 中断号 特定的编号 对应的地址 软件 保存当前的处理状态 终端服务程序 清除中断标记 异常 异常编号 保存 现场 异常处理 杀死异常的程序 重新执行异常指令 恢复线程 接口 接口 系统调用 应用程序 成功 失败 程序访问使用 API实现相应的操作系统的服务 定义哪些系统调用 操作系统的实现 应用程序 提供什么样的功能 Library 访问系统调用的接口 用户态 应用程序执行的特权级的状态 不能完全控制计算机 内核态 CPU运行状态 可以执行任何的指令 完全控制 用户调用兄消退给你调用 CPU将系统调用的权利返回给用户 函数调用 系统调用 用户态和内核态的转换 开销 相应 的汇报 安全 可靠 跨越操作系统的边界 操作系统 外设 整个系统安全可靠 正常的处理应用程序 操作系统有自己的堆栈 维护堆栈 退出 堆栈保存 进入 恢复 应用程序 操作系统不信任程序 对参数做检查 基于安全层面的考虑 从内核态导入用户态 内存空间的数据拷贝 第三章 计算机操作系统及 内存分配体系3.1 计算机操作系统及 内存分配体系主要的3部分 CPU 内存 程序代码 IO 外设 内存的层次结构 抽象 内存中运行不用考虑底层的细节 访问一个联系的地址空间 逻辑地址空间 保护 有个不同的应用程序 隔离的机制的实现 进程之间的交互 当内存中放了很多应用程序 最需要内存的数据放在内存中 其余临时放到磁盘上去 应用程序透明 P1 P2 P3 P4 P4的数据没必要放到内存 逻辑地址空间 物理地址空间 程序重定位 分段 虚拟内存 按需分页虚拟内存 3.2 地址空间与地址生成地址空间 地址生成 、地址空间定义 地址生成 安全检查 地址空间 物理 硬件 内存条 磁盘 逻辑地址 一维线性 映射空间 C程序 汇编程序 C程序 变量的名字 地址 汇编程序 符号 代表函数和名字 汇编器 机器语言 大的程序 小的程序 Linker 小的变成大的程序 Loader 符号的地址 —–》》》》 具体的逻辑地址 指令取出来 逻辑地址 查找逻辑地址 MMU 存在着映射关系 具体的物理地址在什么地方 一个指令 MMU 查找逻辑地址映射表 找到 主存 —-》》总线—-》》》CPU 起始地址 长度 cpu执行指令 逻辑地址是否满足区域的限制 地址安全监测 3.3 连续内存分配 3个内存分配算法内存的碎片空间 内部碎片 外部碎片 数据分配空间 分配算法 首次适配算法 最优分配算法 最差分配算法 最优适配算法那 避免了 3.4 连续分配空间 压缩式和交换式碎片 非连续分配和连续分配 非连续分配的缺点 如何建立虚拟地址和物理地址之间的转换 软件方案 硬件方案 分段机制 分页机制 分段的管理机制 计算机程序 各种各样的段组成 各种子程序 程序的分段地址空间 分段的寻址方案 段 更好的分离和共享 相应的分离代码段 数据的相对的隔离 左边连续的逻辑地址 右边分散的物理地址 逻辑地址空间 连续的字节流 代码 数据 堆 栈 软件实现 软件进行映射 开销很大 通过硬件的知识 分段地址方案 地址 一维的逻辑地址 表示的方法 段 Segment 段机制寻址的方式 Segment number 段寄存器和地址寄存器 都可以存在 映射机制 一维线性 映射到多为的物理地址 段的起始地址 段的长度 一般 地址在一个合法的空间之内的 段地址 + offsett CPU为了寻址做了很多工作 第四章 虚拟内存5.1 虚拟内存的起因 起因 程序运行的时候越来越不够用 电子游戏 小游戏 —》》 大游戏 计算机性能 想办法 更多程序跑在有限的内存中去 很大的存储器 更大 更快 更便宜的非易失性存储器 更有效的管理物理内训 更大 更快 更便宜 受限于内存的特质 新的内存 满足这个条件 更大 更快 更便宜 更不容易丢失内存 5.2 覆盖技术80年代产生 小的内存 运行大的程序 640k的大小 虚拟出更大的空间 常用功能 必选部分 可选部分 不存在调用关系 分时 例子 代码 只读 释放那空间 程序员进行管理 高出 开销 程序员 设计的开销 换入 换出操作 时间的开销 时间 早期的时候 使得大型软件的使用得以实现 Dos 各种各样的东西 5.3交换技术目标 内存管理单元 进程的地址空间 换出 外存中的数据 换出 进程的地址空间 内存到外存 换入 外存进内存 内存导出去 后端存储 硬盘 硬盘中的数据导回来 具体的实现复杂 交换技术考虑什么问题 交换时机的确定 ​ 内存空间不够的时候 多大的内存空间 ​ 换出空间被占用了 程序换入的重定位 动态地址映射 覆盖技术和交换技术 覆盖 共享一块内存区域 代价程序员的手动指定 交换 程序之间 换入换出一个程序 操作系统的内部完成 开销比较大 5.4虚拟技术覆盖技术的问题 程序员的管理 分析 告诉相应的处理技术 粒度太大 交换技术 进程的地址空间都交换出来 增加了处理器的开销 虚拟技术—目标 进程的部分内容 交换 不需要程序员的干涉 操作系统和MMU 程序具有局部性 程序在较短的时间范围之内 指令地址和指令的操作数地址 时间局部性 空间局部性 程序的局部性很好 程序的效率很高 操作系统利用局部性 虚拟内存的理想的管理状态 什么是缺页中断 第七章 进程进程的描述 进程的状态 线程 进程间通信 线程互斥与同步 死锁问题 进程 什么是进程 为什么用进程 执行的程序执行过程 跑一个程序 跑多个程序 程序的概念表示 多个程序的实例 更好的表示程序的执行的过程 编译 执行程序 代码段 数据段 执行文件的形式存在 动态执行的过程 7.1 进程的组成进程执行的功能 程序是产生进程的基础 多次执行程序]]></content>
      <categories>
        <category>计算机必备知识</category>
      </categories>
      <tags>
        <tag>计算机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springboot]]></title>
    <url>%2Fblog4%2F2019%2F04%2F13%2Fspringboot%2F</url>
    <content type="text"><![CDATA[springboot的学习1.1 什么是springbootSpringBoot是Spring项目中的一个子工程，与我们所熟知的Spring-framework 同属于spring的产品。 Spring Boot称为搭建程序的`脚手架 其最主要作用就是帮我们快速的构建庞大的spring项目，并且尽可能的减少一切xml配置，做到开箱即用，迅速上手，让我们关注于业务而非配置。 我们可以使用SpringBoot创建java应用，并使用java –jar 启动它，就能得到一个生产级别的web工程。 1.2 为什么学习 springbootjava一直被人诟病的一点就是臃肿、麻烦 复杂的配置 项目各种配置其实是开发时的损耗， 因为在思考 Spring 特性配置和解决业务 问题之间需要进行思维切换，所以写配置挤占了写应用程序逻辑的时间。 混乱的依赖管理 项目的依赖管理也是件吃力不讨好的事情。决定项目里要用哪些库就已经够让人头痛的了，你还要知道这些库的哪个版本和其他库不会有冲突，这也是件棘手的问题。并且，依赖管理也是一种损耗，添加依赖不是写应用程序代码。一旦选错了依赖的版本，随之而来的不兼容问题毫无疑问会是生产力杀手。 ​ 1.3 SpringBoot的特点 独立 内嵌tomcatjetty 和undertow （不需要打包成war） starter配置 提供产品级的功能，如：安全指标、运行状况监测和外部化配置等 绝对不会生成代码，并且不需要XML配置 总而言之 开箱即用 2 实战小Demo2.1.创建工程 2.2.引入依赖 pom.xml 12345678910111213&lt;!-- 所有的springboot的工程都以spring父工程为父工程 --&gt;&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.6.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 2.3 HelloController的编写 代码: 12345678910111213141516171819202122232425/* * @创建人: MaLingZhao * @创建时间: 2019/4/13 * @描述： */import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.EnableAutoConfiguration;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RestController;@RestController@EnableAutoConfigurationpublic class HelloController &#123; @GetMapping("hello") public String hello()&#123; return "hello Spring Boot!"; &#125; public static void main(String[] args) &#123; SpringApplication.run(HelloController.class, args); &#125;&#125; 运行 测试 测试成功 2.4 解释1@EnableAutoConfiguration开启spring应用程序的自动配置，SpringBoot基于你所添加的依赖和你自己定义的bean，试图去猜测并配置你想要的配置。比如我们引入了spring-boot-starter-web，而这个启动器中帮我们添加了tomcat、SpringMVC的依赖。此时自动配置就知道你是要开发一个web应用，所以就帮你完成了web及SpringMVC的默认配置了！ 2 启动器123456&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 需要注意的是，我们并没有在这里指定版本信息。因为SpringBoot的父工程已经对版本进行了管理了。 这些都是SpringBoot根据spring-boot-starter-web这个依赖自动引入的，而且所有的版本都已经管理好，不会出现冲突。 2.5 思考1 异常多个controller怎么办 几个启动 难道要在每一个Controller中都添加一个main方法和@EnableAutoConfiguration注解，这样启动一个springboot程序也太麻烦了。也无法同时启动多个Controller，因为每个main方法都监听8080端口。所以，一个springboot程序应该只有一个springboot的main方法。 所以，springboot程序引入了一个全局的引导类。 1234567891011121314151617181920package com.mlz.springboot;/* * @创建人: MaLingZhao * @创建时间: 2019/4/13 * @描述： */import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.EnableAutoConfiguration;@EnableAutoConfigurationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 修改helloApplication 12345678910111213@RestControllerpublic class HelloController &#123; @GetMapping("hello") public String hello()&#123; return "hello Spring Boot!"; &#125; public static void main(String[] args) &#123; SpringApplication.run(HelloController.class, args); &#125;&#125; 启动发现 以前的Controller 配置了ComponentScan注解 现在没有了 2 解决添加@ComponentScan注解 12345678910@EnableAutoConfiguration@ComponentScanpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 成功 3 反思@ComponentScan的作用 配置组件扫描的指令。提供了类似与&lt;context:component-scan&gt;标签的作用 通过basePackageClasses或者basePackages属性来指定要扫描的包。如果没有指定这些属性，那么将从声明这个注解的类所在的包开始，扫描包及子包 而我们的@ComponentScan注解声明的类就是main函数所在的启动类，因此扫描的包是该类所在包及其子包。一般启动类会放在一个比较浅的包目录中。 4 放大招一个顶俩或更多我们现在的引导类中使用了@EnableAutoConfiguration和@ComponentScan注解，有点麻烦。springboot提供了一种简便的玩法：@SpringBootApplication注解 12345678@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 3 配置原理一般xml配置 例如原来的数据库连接池的配置 1234567&lt;!-- 配置连接池 --&gt;&lt;bean id="dataSource" class="com.alibaba.druid.pool.DruidDataSource" init-method="init" destroy-method="close"&gt; &lt;property name="url" value="$&#123;jdbc.url&#125;" /&gt; &lt;property name="username" value="$&#123;jdbc.username&#125;" /&gt; &lt;property name="password" value="$&#123;jdbc.password&#125;" /&gt;&lt;/bean&gt; 现在 1 原来的配置spring 1.0 在此时因为jdk1.5刚刚出来，注解开发并未盛行，因此一切Spring配置都是xml格式，想象一下所有的bean都用xml配置，细思极恐啊，心疼那个时候的程序员2秒 spring 2.0 Spring引入了注解开发，但是因为并不完善，因此并未完全替代xml，此时的程序员往往是把xml与注解进行结合，貌似我们之前都是这种方式。 spring 3.0 3.0以后Spring的注解已经非常完善了，因此Spring推荐大家使用完全的java配置来代替以前的xml，不过似乎在国内并未推广盛行。然后当SpringBoot来临，人们才慢慢认识到java配置的优雅。 2 尝试java的配置java配置主要靠java类和一些注解来达到和xml配置一样的效果，比较常用的注解有： @Configuration：声明一个类作为配置类，代替xml文件 @Bean：声明在方法上，将方法的返回值加入Bean容器，代替&lt;bean&gt;标签 @Value：属性注入 @PropertySource：指定外部属性文件。 1 Druid依赖引入12345&lt;dependency&gt; &lt;groupId&gt;com.github.drtrang&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot2-starter&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt;&lt;/dependency&gt; 2 jdbc.properties1234jdbc.driverClassName=com.mysql.jdbc.Driverjdbc.url=jdbc:mysql://127.0.0.1:3306/magejdbc.username=rootjdbc.password=123 3 配置数据源JdbcConfiguration 12345678910111213141516171819202122232425262728293031323334353637383940import javax.sql.DataSource;import java.io.PrintWriter;import java.sql.Connection;import java.sql.SQLException;import java.sql.SQLFeatureNotSupportedException;import java.util.logging.Logger;@Configuration //声明一个类是java配置类@PropertySource("classpath:jdbc.properties")public class JdbcConfiguration &#123; @Value("$&#123;jdbc.driverClassName&#125;") String driverClassName; @Value("$&#123;jdbc.username&#125;") String username; @Value("$&#123;jdbc.password&#125;") String password; @Value("$&#123;jdbc.url&#125;") String url; //把方法的返回值注入到spring容器 @Bean public DataSource dataSource() &#123; DruidDataSource dataSource=new DruidDataSource(); dataSource.setDriverClassName(this.driverClassName); dataSource.setPassword(this.password); dataSource.setUsername(this.username); dataSource.setUrl(this.url); return dataSource; &#125; 启动项目没有出现问题]]></content>
      <tags>
        <tag>java</tag>
        <tag>web框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka 的学习]]></title>
    <url>%2Fblog4%2F2019%2F03%2F12%2Fkafka%2F</url>
    <content type="text"><![CDATA[1 kafka概述2 kafka的集群部署3 Kafka的工作流程分析4 kafka的API实战4.1 环境准备4.1.1 zk集群和kafka集群启动 kafka集群打开一个消费者4.1.2创建maven 工程kafkaAPIpom.xml 12 ​ ​ ​ org.apache.kafka ​ kafka-clients ​ 0.11.0.0 ​ ​ ​ ​ org.apache.kafka ​ kafka_2.12 ​ 0.11.0.0 ​ 观察源码学习 替换 com.atguigu.lafka 4.2 生产者javaAPI4.3 消费者javaAPI5Kafka的producer烂机器6KafkaStreams7 扩展]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据框架</tag>
      </tags>
  </entry>
</search>
