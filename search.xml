<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MATLAB]]></title>
    <url>%2Fblog4%2F2019%2F10%2F12%2FMATLAB-0%2F</url>
    <content type="text"><![CDATA[机器学习及MatLab实现1 MATLAB入门基础1.1 MATLAB的简介1.1.1 matlab的教程地址https://ww2.mathworks.cn/help/matlab/ matlab**是什么？** 主要用于科学的计算 用的人 工程师 科学家 量化分析 可视化 计算 1.1.2 matlab的应用 matlab的图像的信号处理 工具包 每年两个版本 matlab matlab2014a matlab2014b 对于版本的选择 如果用途 常规的运算版本之间的差别不是特别大 1.2 安装路径的选择 预览窗口 1.3 MATLAB的变量的命名规则 区分大小写 变量的长度不超过63位 变量名以字母开头 可以由字母 、数字和下划线组成 但是不能使用标点 变联名应该更加简洁，通过变量 名k暗处所表示的物理意义 1.4 MATLAB的数据类型数字 字符与字符串 矩阵 元胞数组 结构体 1.5MATLAB的矩阵操作矩阵的定义和构造 矩阵的四则运算 矩阵的下标 1.6 MATLAB逻辑与流程控制if…. else …end for end while …end switch…case …end 1.7 MATLAB脚本与函数文件脚本文件 函数文件 1.8 MATLAB的基本绘图操作二维平面图 三维立体图 图形的保存与退出 1.9 MATLAB的文件导入mat load txt load xls xlsread xlsread csv csvwrite scvread 2 MATLAB的进阶人工神经网络 什么是人工神经网络]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>MATLAB</tag>
        <tag>业余学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark的学习]]></title>
    <url>%2Fblog4%2F2019%2F10%2F12%2Fspark%2F</url>
    <content type="text"><![CDATA[Spark day011 Spark初始1.1 什么是spark1.2 总体的技术栈的讲解1.3 spark的演变历史 1.4 Spark与MapReduce的区别 1.5 Spark的运行模式 2 spark Java-Scala 混编Maven开发idea创建maven工程 pom文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172&lt;!-- 配置以下可以解决 在jdk1.8环境下打包时报错 “-source 1.5 中不支持 lambda 表达式” --&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- Spark-core --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;2.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- SparkSQL --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;2.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- SparkSQL ON Hive--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-hive_2.11&lt;/artifactId&gt; &lt;version&gt;2.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;!--mysql依赖的jar包--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;/dependency&gt; &lt;!--SparkStreaming--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;2.3.1&lt;/version&gt; &lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt; &lt;/dependency&gt; &lt;!-- SparkStreaming + Kafka --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt; &lt;version&gt;2.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 向kafka 生产数据需要包 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.10.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;!--连接 Redis 需要的包--&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.6.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Scala 包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;2.11.7&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-compiler&lt;/artifactId&gt; &lt;version&gt;2.11.7&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-reflect&lt;/artifactId&gt; &lt;version&gt;2.11.7&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.collections&lt;/groupId&gt; &lt;artifactId&gt;google-collections&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;!-- 在maven项目中既有java又有scala代码时配置 maven-scala-plugin 插件打包时可以将两类代码一起打包 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.scala-tools&lt;/groupId&gt; &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt; &lt;version&gt;2.15.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!-- maven 打jar包需要插件 --&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;2.4&lt;/version&gt; &lt;configuration&gt; &lt;!-- 设置false后是去掉 MySpark-1.0-SNAPSHOT-jar-with-dependencies.jar 后的 “-jar-with-dependencies” --&gt; &lt;!--&lt;appendAssemblyId&gt;false&lt;/appendAssemblyId&gt;--&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.bjsxt.scalaspark.sql.windows.OverFunctionOnHive&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;assembly&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!-- 以上assembly可以将依赖的包打入到一个jar包中，下面这种方式是使用maven原生的方式打jar包，不将依赖的包打入到最终的jar包中 --&gt; &lt;!--&lt;plugin&gt;--&gt; &lt;!--&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;--&gt; &lt;!--&lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;--&gt; &lt;!--&lt;version&gt;2.4&lt;/version&gt;--&gt; &lt;!--&lt;configuration&gt;--&gt; &lt;!--&lt;archive&gt;--&gt; &lt;!--&lt;manifest&gt;--&gt; &lt;!--&lt;addClasspath&gt;true&lt;/addClasspath&gt;--&gt; &lt;!--&amp;lt;!&amp;ndash; 指定当前主类运行时找依赖的jar包时 所有依赖的jar包存放路径的前缀 &amp;ndash;&amp;gt;--&gt; &lt;!--&lt;classpathPrefix&gt;/alljars/lib&lt;/classpathPrefix&gt;--&gt; &lt;!--&lt;mainClass&gt;com.bjsxt.javaspark.sql.CreateDataSetFromHive&lt;/mainClass&gt;--&gt; &lt;!--&lt;/manifest&gt;--&gt; &lt;!--&lt;/archive&gt;--&gt; &lt;!--&lt;/configuration&gt;--&gt; &lt;!--&lt;/plugin&gt;--&gt; &lt;!-- 拷贝依赖的jar包到lib目录 --&gt; &lt;!--&lt;plugin&gt;--&gt; &lt;!--&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;--&gt; &lt;!--&lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;--&gt; &lt;!--&lt;executions&gt;--&gt; &lt;!--&lt;execution&gt;--&gt; &lt;!--&lt;id&gt;copy&lt;/id&gt;--&gt; &lt;!--&lt;phase&gt;package&lt;/phase&gt;--&gt; &lt;!--&lt;goals&gt;--&gt; &lt;!--&lt;goal&gt;copy-dependencies&lt;/goal&gt;--&gt; &lt;!--&lt;/goals&gt;--&gt; &lt;!--&lt;configuration&gt;--&gt; &lt;!--&lt;outputDirectory&gt;--&gt; &lt;!--&amp;lt;!&amp;ndash; 将依赖的jar 包复制到target/lib下&amp;ndash;&amp;gt;--&gt; &lt;!--$&#123;project.build.directory&#125;/lib--&gt; &lt;!--&lt;/outputDirectory&gt;--&gt; &lt;!--&lt;/configuration&gt;--&gt; &lt;!--&lt;/execution&gt;--&gt; &lt;!--&lt;/executions&gt;--&gt; &lt;!--&lt;/plugin&gt;--&gt; &lt;/plugins&gt; &lt;/build&gt; 3 SparkCore3.1 RDD*概念 * 弹性分布式数据集 五大特性 partition 函数作用在每一个partition RDD之间有一系列的依赖关系 分区器是作用在K，V格式的RDD上的 RDD提供一系列的最佳的计算位置 3.2 Spark任务执行的原理3.3 Spark的代码流程 3.4 Transformation转换算子filter map flatMap smaple reduceByKey sortBy/sortByKey 3.5 Action行动算子count take first foreach collect 3.6 控制算子cache persist checkpoint 三种都可以将RDD持久化 cache和persist都是懒执行的 checkpoint 不仅可以 持久化到磁盘 还能切断RDD之间的依赖关系 4 集群的搭建和测试4.1 搭建slaves文件的配置 saprk-env.sh文件的配置 12 SPARK_MASTER_IP:master的ip SPARK_MASTER_PORT:提交任务的端口，默认是7077 4.2Local模式123456bin/spark-submit \--class org.apache.spark.examples.SparkPi \--executor-memory 1G \--total-executor-cores 2 \./examples/jars/spark-examples_2.11-2.1.1.jar \100 解压缩tar包直接运行 不需要配置 Pi的官方群里 4.3 StandAlone模式Master + slaves 构成的集群 conf 文件 配置slaves文件123hadoop102hadoop103hadoop104 12 sprk-env.sh 文件的配置配置JobHistoryServerspark-default.conf 12spark.eventLog.enabled truespark.eventLog.dir hdfs://hadoop102:9000/directory 在hdfs上创建目录 spark-env,sh 123export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=30 -Dspark.history.fs.logDirectory=hdfs://hadoop102:9000/directory" HA的配置 12345678注释掉如下内容：#SPARK_MASTER_HOST=hadoop102#SPARK_MASTER_PORT=7077添加上如下内容：export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=hadoop102,hadoop103,hadoop104 -Dspark.deploy.zookeeper.dir=/spark" saprkHA的访问 1234/opt/module/spark/bin/spark-shell \--master spark://hadoop102:7077,hadoop103:7077 \--executor-memory 2g \--total-executor-cores 2 4.1. Yarn模式4.4 Mesos模式4.5 几种模式的对比4.6 测试PI的计算的案例 yarn上的测试 ./spark-submit –master yarn –class xxxx jar包 100000 Standalone ./spark-submit –master spark://hadoop102:7077 –class jar包 参数10000 Sparkday021 StandAlone的两种提交任务的方式2 Yarn的两种提交任务的方式、3 补充部分算子4 术语解释5宽依赖和窄依赖6Stage7 Spark的资源调度和任务调度广播变量123456789101112131415 不使用广播变量 有很多个driver端的变量副本 使用广播变量只有一分变量副本 大大的减少了内存 广播变量只能在Driver端定义 在Executor中使用 不能再Executor中改变广播量的值BlockManager100个task 只有driver的一份的变量副本 广播zhangsan 再来一个lisi 线程是不安全的 广播变量的代码 1234567891011121314151617181920212223242526272829303132333435363738394041object BrodCastTest &#123; def main(args: Array[String]): Unit = &#123; val conf=new SparkConf() conf.setMaster("local[*]") conf.setAppName("test") val sc = new SparkContext(conf) val list: List[String] = List[String]("zhangsan","lisi") //使用广播变量 速度会提高 val bcList: Broadcast[List[String]] = sc.broadcast(list) //每个executor用了一个broadcast 就会有一个task val nameRDD: RDD[String] = sc.parallelize(List[String]("zhangsan","lisi","wangwu")) val result: RDD[String] = nameRDD.filter(name =&gt; &#123; val innerList: List[String] = bcList.value !list.contains(name) &#125;) result.foreach(println) &#125;&#125;Driver i=0ExecutorExecutor累加器在Driver端定义初始化 累加器伪代码 12345678910111213141516171819val rdd=sc.textfilerdd.count()//统计数据var i=0val rdd2=rdd.map(one=&gt;&#123;i+=1one&#125;)rdd2.collect()println("i="+i) UDFuser defined function 用户自定义函数 12345678910111213141516171819202122232425bject UDF &#123; def main(args: Array[String]): Unit = &#123; val spark: SparkSession = SparkSession.builder().master("local[*]").appName("UDF").getOrCreate() val nameList: List[String] = List[String]("zhangsan","lisi","wangwu","zhaoliu","tianqi") //不要忘记隐式转换 import spark.implicits._ //简历一个列 val nameDF: DataFrame = nameList.toDF("name") //注册表的名称 nameDF.createOrReplaceTempView("students") //注册 去名称 注册udf的函数的名称 //注册一个列 spark.udf.register("STRLEN",(name :String) =&gt;&#123; name.length &#125;) spark.sql("select name,STRLEN(name) as length from students sort by length desc" ) .show() &#125; UDAFuser defined aggregate function 用户自定义聚合函数 继承UeserDefinedAggregateFuntion 实现8个方法 最重要的initialize update merge方法 map端按照groupby 每个RDD分区内按照groupby的字符安分组 initialize方法 初始化设置 update****方法**** Reduce端 merge方法 开窗函数 spark的题目 spark运行模式 spark核心RDD 注意点 spark的代码流程 spark的技术栈相关的技术 介绍联系 spark资源调度 基于standalone和yarn提交任务两种方式的区别 什么是sparkRDD spark 计算模式【术语–&gt;RDD的宽窄依赖 —&gt; Stage 】 spakr资源调度和任务调度流程 粗粒度 细粒度 资源申请 spark的源码看懂 一部分 整体的流程 sparkSubmit —-&gt;&gt;Driver —&gt;&gt;向Master注册Application–&gt;&gt;Application 申请资源 —-&gt;&gt; 任务调度源码 spark Shuffle 两种shuffleManager 发展过程hash shuffleManager sparkShuffle优化 11.内存管理 调内存 8G的内存]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据框架</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java面试突击]]></title>
    <url>%2Fblog4%2F2019%2F10%2F12%2FjavaInterview%2F</url>
    <content type="text"><![CDATA[1 面试前的准备简介秋招 春招 秋招的难度比春招大 秋招的含金量更高 内推 需要优秀的简历 github有比较好的开源项目 大厂邀请你参加你面试 2 面试须知1 两份简历hr 主要讲自己的经历 技术能力一语带过 技术面试官 讲自己的技术 突出自己的项目经历 技术细节 项目经验 2 着装简单大方 3 简历携带4 提前刷题5 准备项目项目架构 主要承担了什么角色 你负责了什么 用了哪些技术 你解决了 你协助团队解决了哪些问题 3 重点简历 数据结构与算法 自学 好 2 java1 java基础知识重载 重写方法里的参数 重写父类的方法 （返回值 父类private修饰 不能重写） StringBuilder StringBuffer StringString 1private final char value[]; final 修饰 Striing类型不可变 StringBuilder StringBuffer 和String 12public final class StringBuilder extends AbstractStringBuilder 12345*/ public final class StringBuffer extends AbstractStringBuilder implements java.io.Serializable, CharSequence&#123; 继续看源码 1234567891011/abstract class AbstractStringBuilder implements Appendable, CharSequence &#123; /** * The value is used for character storage. */ char[] value; /** * The count is the number of characters used. */ int count; char[] 没有final修饰所以StringBuilder和StringBuffer都是可变的 继续比较StringBuilder和StringBuffer StringBuilder 1234567891011121314public StringBuffer(CharSequence seq) &#123; this(seq.length() + 16); append(seq); &#125; @Override public synchronized int length() &#123; return count; &#125; @Override public synchronized int capacity() &#123; return value.length; &#125; 我们看到了synchronized ， 因为StringBuffer使用了同步锁，所以它的线程安全性得到了提高， 但是性能却下降了 使用的场景 String 少量数据 StringBuilder 单线程 大量数据 StringBuffer 多线程的大量数据 自动拆箱和装箱基本类型和引用类型的相互转换 ==和equals的区别== 基本类型来说 比较数值 对于引用类型比较的是地址 equals String aa=”ab”; String bb=”ab”; 放在常量池中 aa和bb是相等的 String a=new String(“ab”) String b=new String(“ab”) a是一个引用 b是一个引用 a和b的地址是不一样的 String的equals 是重写过的 比较的是连个字符串的值 而Objec中的equals方法是比较的两个对象的地址 final的关键字的总结变量 基本 类型 不能改变 引用类新型 初始化之后不能指向新的对象 方法 保证方法不被改变 类 这个类不能被继承 所有的方式 被认为是final类型 object中的方法notify notifyAll wait java的集合框架ArrayList和LinkedList异同同 不保证线程的安全 java的多线程java虚拟机设计模式计算机网络 Linux Mysql Redis Spring 消息队列 Dubbo 数据结构 算法 实际场景提 BAT真实面试题System.out.println(3|9) &amp; &amp;&amp; ； | || | 和|| 的区别两者都可以是逻辑运算符 | 两边的东西都会运算 || 先判断左边 再去判断右边 了解 2进制 10进制Forward 和 Redirect的区别转发是服务器的行为 重定向是客户端的行为 重定向 是用服务器的返回的状态码来实现的 301 302 新的网址请求这个资源 1 地址显示来说 forward 是服务器请求资源 ，服务器请求地址的资源 服务器直接访问目标地址的url，然后把这些内容发送给浏览器 浏览器根本不知道内容是从哪里来的 redirect是服务端根据逻辑，发送一个状态码，高速浏览器去重新请求那个地址，所以地址显示的是新的url 2 数据共享 ​ foward 转发页面和转发到的额页面乐意共享request中的数据 3 应用 forward 用户登录 redirect 用户退出 注销登录的时候 4 效率上说 forward 高 redirect低 最后面试失败 乃是常事]]></content>
      <categories>
        <category>javaEE</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JUC并发编程]]></title>
    <url>%2Fblog4%2F2019%2F10%2F10%2FJUC%2F</url>
    <content type="text"><![CDATA[1 JUC是什么java.util.current 在并发编程中使用的工具类 进程和线程的回顾1 进程是什么 线程是什么进程是操作系统动作执行的基本单元 在传统的操作系统中，进程既是基本的分配单元，也是基本的执行单元 独立功能的程序关于某个数据集合的一次性活动 线程一个进程可以包含若干个线程，一个进程中至少有一个线程， 不然没有存在的意义 线程可以利用进程拥有的资源 进程作为分配资源的基本单位 线程作为独立运行和独立调度的基本单位，基本上不拥有系统资源， 因此对它的调度所付出的开销就小得多 2 进程和线程的例子3 线程的状态new runnable blocked waiting Timed_waiting terminated 4 wait/sleep的区别wait 线程暂停 wait 放开手去睡 放开手里的锁 sleep握紧手去睡 行了手里还有锁 5 什么是并发 什么是并行并发 同一个时刻在访问同一个 资源 并行 多项工作一起执行 之后再汇总 Lock接口复习synchronized线程操作资源类 高内聚 低耦合 实现步骤1 创建资源类 2 资源类创建同步方法，同步代码 example 买票程序 Lock Lock接口实现 可重入锁怎么用 1234567891011121314class X&#123; private final ReetrantLock lock=new ReetrantLock(); //.... public void m() &#123; lock.lock(); try&#123; //....nethod body &#125;finally&#123; lock.unlock(); &#125; &#125;&#125; synchronized和lock的区别1 首先synchronized是java的内置关键字，在jvm层面，lock是一个java类 2 synchronized无法判断是否获取锁的状态，Lock可以判断是否被获取到锁 3 首先synchronized是java的内置关键字，在jvm层面，lock是一个java类 synchronized无法判断是否获取锁的状态，Lock可以判断是否被获取到锁 4 用synchronized关键字的两个线程线程1 和线程2 ，如果线程1 获得锁 线程2 等待 。如果线程1阻塞线程2 会一致等待下去，Lock就不一定会等待下去，如果一致获取不到锁的话，线程可以不用一直等待就结束了 5 synchronized的锁可重入 不可中断，非公平 而Lock锁可重入，可判断， 可公平（两者皆可） 6Lock锁适合大量同步代码的同步问题 synchronized适合代码量少的同步问题 创建线程的方式集成Thread 实现Runnable方法 新建类实现Runnnable接口 12ckass MyThread umplements Runnablenew Thread() 匿名内部类 123456new Thread(new Runnable()&#123; @Override public void run()&#123;&#125;&#125;) lambda表达式 123new Thread(()-&gt;&#123; &#125;,"your thread name").start() Lambda表达式lambda是一个匿名函数 我们可以把lambda表达式理解为可以传递的代码 左侧 lambda表达式需要的参数 右侧 lambda题 lambda表达式要执行的功能 Runnable接口为什么可以使用lambda表达式]]></content>
      <categories>
        <category>javaEE</category>
      </categories>
      <tags>
        <tag>java高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git的学习]]></title>
    <url>%2Fblog4%2F2019%2F10%2F10%2Fgit%2F</url>
    <content type="text"><![CDATA[一、Git的基础1 认识git有了SVN 为什么还需要git svn增量式管理 GIT采取文件系统快照的方式 svn是集中式管理 git是分布式管理 其他的优势 大部分的操作在本地不需要互联网，只有pull和push时需要 2 git的安装1 Linux的环境./config make sudo make install gt config –global user.name “YourName” 我们注意到了global参数 用了这个参数，表示这台加的所有的git仓库都会使用这个配置 3 git的结构工作区 git add的暂存区 （临时存储） git commit 本地库（历史版本） 很多人喜欢直接commit到本地库 是因为用svn比较多的缘故 工作区 (.git)=.svn git的版本库里面包含了很多的东西，其中最重要的就是成为stage的暂存区， 还有Git为我们自动创建的第一个分支master 以及指向master的一个指针叫Head svn是没有暂存区的 文件git的版本库里添加的时候 分两步执行 实际上就是把文件修改添加到暂存区 4 Git和代码托管中心局域网 - GitLab服务器 外网- GitLab 、 码云 本地库与远程协作的方式 1 团队的内部协作 2 跨团队的协作 5 Git的命令行操作克隆远程仓库12git clone https://github.com/malingzhao/tuchaung.gitcd 本地仓库 本地库的初始化git add 设置签名作用 区分开发人员的身份 设置的额签名和远程 登录的仓库的账号和密码没有关系 命令 123git config user.name tom_progit config user.email [good_pro@126.com](mailto:good_pro@126.com) cd .git 观察配置信息 项目的优先于用户的优先级 二 命令行的常用的操作1 上传文件git clone cd 仓库地址 gti add 文件名 git commit -m “描述” git push orign master 第一次出现user.name user.email 配置输入即可// 查看路径echo $HOME 查看路径git config –global credential.helper store 创建此文件时一依据此路径]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>技能</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据面试]]></title>
    <url>%2Fblog4%2F2019%2F10%2F09%2FbigdataInterview%2F</url>
    <content type="text"><![CDATA[Michael_PK 大数据团队面试 spark Hadoop 的公有云和私有云 剑指大数据面试概述互联网寒冬 简历投递的活跃度高 大数据岗位 够硬的技术功底 表现能力 拓展和提升大数据相应的能力 java Linux的基础知识 具有一定的项目经 1 开篇就业趋势 高效Offer 2 大数据处理架构总体架构 系统设计 3 小文件引发的血案篇hdfs 小文件问题 4 SQL on Hadoop架构层面的调优 语法层面调优 执行框架底层调优 sql的实战案例 5 数据倾斜篇什么是数据倾斜及产生的原因 大数据中的shuffle 产生数据倾斜的场景 数据倾斜的解决方案 6 spark的调优篇算子的合理选择给计算性能带来的深远影响 合理的序列化整合saprk使用为性能提速 如何保证流处理过程的零数据丢失 流处理数据Sink到目的地的N中错误的操作剖析 案例实战： 如何基于Spark定制外部数据源 7 java篇javaSE 多线程 生产者和消费者 jvm 8 jvm篇ClassLoader机制 内存模型 垃圾回收 垃圾回收算法 垃圾回收器 9 其他篇 zookeeper Linux 10 技巧篇2小文件引发的血案篇1 HDFS架构 学习一个新框架 ​ 百度 文档 不推荐 ​ 开源框架存在问题 推荐 官网+源码 ​ 跪在坚持 ​ hadoop ​ spark ​ flink ​ org.apache.xxxxx hadoop :HDFS/YARN/MapReduce3 HDFS ​ NameNode ​ DataNode ​ SecondaryNameNode ​ 概念 Client NN: 一个，Single Problem Of Failure ====&gt; HA metadata： 谁 权限 文件对应block的信息 文件名 副本信息（生产环境一般3个） 副本机制 ======》》》 增加容错 DN： 多个 存储数据 和NN之间是有心跳的 Block : File存入HDFS 按照block进行拆分 128M 2 HDFS的读写流程HDFS的写流程 HDFS的读流程 请求最近的NameNode 最终 3HDFS HA NameNode挂掉 SecondaryNameNode Active Standby 同步NameNode的状态 共享 快速的切换 两个独立的机器上 一个活动状态一个备份状态 Zookeeper 实现高可用调度系统调度机 zookeeper进行管理持久性的 临时性的 HA 主 和 备的 切换 各自的进程监控zookeeper状态的切换Active Standby 数据的同步 共享 。。。。很多 Yarn的架构 ResourceManager HA NN HA 也有这个问题？ 什么导致 小文件问题RM HA 遇到的问题： 起不来NN HA 也有这个问题？ 什么导致 小文件问题 RM HA 遇到的问题： 起不来 SLA 如何保证？？ 99.99% 99.95% 4 小文件是什么小文件的定义 小文件 明显小于block size的文件 80% 129M 128M + 1M 为什么产生小文件1 批处理 离线计算 小文件 特别是spark 2 数据拷贝到HDFS 没有关注 数据搬迁（手工 Flume采集）没有做很好的设置 3 MapReduce 作业 Reduce 没有做设置 Reduce 决定了输出文件的多少 shuffle合理的设置 hadoop的目录 文件 blk是以元数据的方式存储下来的 200字节 5 小文件给hadoop集群带来的瓶颈问题 100个小文件 IO n多个小文件的IO1 IO开销大2 Map task Reduce Task 的开启和销毁 （task）jvm的启动销毁3 资源有限 3 SQL on Hadoop1 SQL on Hadoop的常用框架常用的SQL on hadoop框架 Hive sql =====》》 对应的sql转换成对应的执行引擎的作业： MapReduce/Spark/Tez Impala： 内存 Presto: 京东 Drill: 跨数据源的查询 Phoenix： HBase（RowKey） 性能高 API + 命令行 sql查询hbase的东西 Spark SQL： 查询结构化数据 MetaStore 存储表的元数据信息 框架之间是共享元数据信息的 Hive on Spark: Hive社区 MR Spark TezSpark SQL： Spark社区 Spark on Hive : X 错误的说法 2 行式存储 vs 列式存储 列式存储 带来很大的性能提升 分表SQL on Hadoop的调优策略 架构层面调优 架构调优之分表 spark ETL操作 Flume ===&gt;&gt; HDFS===&gt;&gt; spark EL ===&gt;&gt; SQL==&gt;&gt;Spark SQL ==&gt; NoSQL 前提 1 行式 2 每分钟2亿条数据 3 500个作业访问这个大表 分区表系统用户日志： who done 语法调优jvm重用 MapTask/ReduceTask都是以进程存在的 ，有多少个task就有多少个jvm 推测执行1 集群中的机器负载是不同的 2 集群中机器的配置不同 3 数据倾斜 一个job100个reduce 99个很快运行完， 只有最后一个话费很撑的执行时间，那么这个job它的运行速度是取决于最慢的一个task 长尾作业 并行执行 默认没有开启 并行的前提多个task之间是没有依赖的 jvm重用MapTask 和ReduceTask 都是以进程的形式存在的，有多少个task就有多少个jvm 当task运行完成结束之后，jvm就会被销毁，jvm的启动和销毁需要开销，每个jvm可以执行多个task 本章总结框架 MetaStore 行式存储 列式存储 调优策略 4 Spark调优篇其他篇分布式锁什么是分布式锁： 公共资源： 同步、加锁 下订单 zookeeper 实现分布式锁 创建一个springboot工程 订单： id itemid 条目： id name counts 1 Spark 10 2 Hadoop 6 3 Flink 3 Spring Data 操作数据库 springdatajpa + mysql 12345jpa: hibernate: ddl-auto: update database: mysql 同一个业务的执行 两个请求一块发起 两个订单同时创建成功 案例演示 自动创建表的配置 套路 dao service controller domain 业务层 代码详情https://github.com/malingzhao/bigdata Linu的内容1 检索： 内容​ grep pk1.txt pk2.txt grep “oop” pk*|grep “Spark” 管道操作符 | 多个命令 多个指令 连接起来 前一个指令的结果作为下一个指令的输入 ps -ef ps -ef| grep zookeeper ps -ef|grep zookeeper |grep -v “grep” -v过滤东西 2 对内容的统计awk 获得第一列和第二列 tab键分割的处理 awk ‘{print $1,$2}’ file.txt 拿到头 awk “$1=8888 || NR=1” emp.txt 逗号 awk -F “,” ‘{print $1}’ sales.csv 拿出来 -F 指定分割符 awk -F “,” ‘{print $0}’一行所有的数据 3 对内容的替换sed java的格式转换成scala的 sed ‘s/String/val/‘ test.txt 第一个原来的 替换成 val sed -i ‘s/String/val’ test.txt 完成第二步的操作 sed -i ‘s/;/ /‘ test.txt 默认替换第一个 sed -i ‘s/Hadoop/hadoop/‘ test.txt 想要全局替换 sed -i ‘s/Hadoop/hadoop/g’ test.txt 技巧篇技术一票决定的权利公司的价值观 你的职业规划为什么要离职薪资问题 加班（抗压能力） 周边的同事 领导 项目 ==》》》》》》》》 个人的发展： 加分 个人的成长空间 离职原因不是个人原因 不是频繁的离职 找到更适合自己的平台 家庭的原因： 对象 你对加班的看法常态 公司基本上加班是早上11点 住的地方就在公司附近 有家 单身 为什么选择我们公司公司 海投 打电话 为什么选择你们公司 建议 对于个人特别想去的 建立一定要差异化 投不同的公司 投不一样的简历 编简历 简历的包装 行业 公司 岗位的描述 喜欢 胜任 你的优缺点目的 你这个人对自己有没有对自己的清晰的认识 自我认知 实事求是 优点： 全力以赴 适应能力 缺点： 推辞能力 在意别人的看法 问问题 自己加班加点===》》（乐于助人，人缘，关系融洽）]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kylin的学习]]></title>
    <url>%2Fblog4%2F2019%2F10%2F09%2FKylin%2F</url>
    <content type="text"><![CDATA[1 概述1.1 Kylin定义开源 分布式 分析引擎 提供基于Hadoop/spark的sql查询和多维分析 OLAP能力（联机分析处理） 亚秒内查询巨大的Hive表 1.2 架构 1 RestServer Rsultful接口 提供查询 2 查询引擎 解析用户查询 3 路由器 在发行版是默认关闭的 体验不好 Hive的速度和Kylin的速度相差加大 4 元数据管理工具 元数据驱动应用程序 kylin的元数据存储在hbase中 5 分析引擎 处理离线任务 shell脚本 java API MapReduce任务 任务引擎需要对kylin中的任务进行协调和管理 1.3特点SQL接口 超大规模数据集 亚秒级的响应 ​ 很多复杂的计算 连接 聚合 离线的预计算过程就完成了 可伸缩性和高吞吐率 搭建集群 每秒70个查询 BI ODBC tableau JDBC RestPI Kylin 2 环境搭建官方文档即可 3 入门4 Cube构建原理5 cube构建优化6 BI集成]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper的学习]]></title>
    <url>%2Fblog4%2F2019%2F10%2F09%2Fzookeeper%2F</url>
    <content type="text"><![CDATA[1 zookeeper入门1.1概述开源 为分布式应用提供协调服 Zookeeper=文件系统+通知机制 1.2 特点Leader follower 半数机制 每个server保存的数据一致（数据备份） 实时性 更新请求顺序执行 1.3 数据结构unix文件系统 形结构 1.4 应用场景统一的配置管理和配置服务 1.5下载地址https://zookeeper.apache.org/ 2 zookeeper的安装2.1 本地模式zoo.cfg bin/zksSrver.sh start|status|stop 出现bash4.1 .bash_profile 每次登陆 .bashrc 每次进入新的bash环境 .bash_logout 每次退出登录 .bash_history 每用户注销前使用的命令 进入root用户 cp .bash_profile .bashrc .bash_logout .bash_profile /home/user 问题得到解决 2.2 配置参数的解读tickTime 2000 initLimit 10 syncLimit dataDir clientPort 3 Zookeeper的内部的原理3.1 选举机制半数机制，适合安装奇数台服务器 没有mater和slave，但是zookeeper在工作的时候 一个leader 其他的都是follower 4 zookeeper的实战分布式部署安装zoo.cfg myid zkData 12345dataDir=/opt/module/zookeeper-3.4.10/zkData#######################cluster##########################server.2=hadoop102:2888:3888server.3=hadoop103:2888:3888server.4=hadoop104:2888:3888 server.A=B:C:D A myid B 主机名称 C 与Leader交换信息 D 一但leader挂掉 选举出新的leader 4.2 API操作5 面试zookeeper的选举机制zookeeper的监听原理main方法 connect线程和listener线程 connect 将监听的事件发送给zookeeper zookeeper在监控l列表上添加事件 listener线程监听到事件变化 zookeeper调用process方法 zookeeper的部署的方式本地部署 分布式部署 zookeeper的常用命令ls create get delete set]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[电商推荐系统]]></title>
    <url>%2Fblog4%2F2019%2F10%2F09%2Fdianshangtuijianxitong%2F</url>
    <content type="text"><![CDATA[1 电商推荐系统的简介1.1 项目的架构设计1.1.2 亚马逊推荐系统的贡献 比例达到了20%到25% 在亚马逊的页面上 推荐列表占了很大的比例 真实的业务架构 构建真正的系统 基于统计的模块 实时 自定义的推荐模块 设计到机器学习 和推荐系统的相关知识 电影推荐系统 切换不同的业务场景 一通百通 对大数据的工具有一个更深刻的理解、 做的是电商推荐系统 推荐系统的具体的应用 1.1.2 分析项目框架 数据源解析 统计推荐模块+ 基于LFM的离线推荐模块 基于自定义模型的实时推荐模块 其他形式的离线相似推荐 基于内容的推荐和 ​ 基于物品的协同过滤模块 1.1.3 数据的生命周期 数据源 三大类 图片 视频 非结构化的数据 日志数据 半结构化的数据 结构化数据 关系数据 数据源 数据采集 数据存储 数据计算 数据应用 用的数据库 mongodb 运算处理 （Mahout） hadoop的 storm的流式处理 spark flink 大数据的计算框架 算完之后 存储到数据库里 分析 Echarts 等数据可视化展示 Cassandra NoSQL 1.2 大数据的处理流程 左边的实时的处理 网站 APP 前端页面 用户接口 —》》》》 http请求 —-》》》 业务系统的后台—-》》》》 调用相应的服务 响应—–》》 埋点收集日志 –》》 记录用户的行为 —》》 日志的采集 （Flume） 数据总线 —》》 KAfka （做存储） 实时消费 flume的数据 —-》》 sparkStreaming 实时计算 —-》》 数据的可视化展示 右边的离线处理的流程 业务系统 日志文件 flume 日志采集 sink 配置成hdfs存储 日志清洗ETL 做数据清洗的操作 数据仓库 最后对这些数据进行计算 对数据进行离线计算 相关的业务数据库 最后做可视化展示 推荐系统 —》》 大数据的典型应用 1.3 我们的目标 不同的地方显示不同的推荐的结果 商品的详情 评分 xxx 然后下面相似的商品推荐出来 混合推荐 典型分区混合 1.4 项目的系统架构用户可视化 Angular JS 前端 后台spring 不是我们主要考虑的地方 数据 存储 MongoDB 一些重要的数据 缓存到Redis里面 并不是一定要存MongoDB mongoDB大数据平台很主流的数据库 读写性能 支持很大的数据量 mongo是一个文档型数据库 json串 存储在里面 很多的特征 redis缓存常规操作 ES 模糊查询 条件查询 离线的推荐服务 统计服务 个性化统计服务 在线 实时 Flume kafka 缓冲 sparkStreaming 实时的推荐处理 Redis缓存的数据 写回到mongo里 用户可视化 推荐 1.4.1 离线数据加载服务 数据放到mongo里面 离线统计服务 sparkSQL 写回表 对应的查询 个性化推荐 隐语义模型的推荐模块 基于内容的推荐 最终的结果写进mongo 1.4.2 实时log Flume kafka 消息缓冲 对应的消息处理 spark Streaming对数据过滤 最后做实时的计算 redis里面拿到最近的评分数据 ‘ 商品检索 mongo es也是可以的 1.5 数据源的解析1.5.1 信息介绍 商品信息 1.用户评分信息 基于商品的信息 做基于内容推荐 用户的评分数据 隐语义模型 协同过滤的推荐 1.6主要的数据模型商品相似度 为了实时推荐做基础 实时 1.6 统计推荐模块商品相似度 1.6.1 历史热门商品统计什么样的商品是热门的 评分的多少 1select productId ,count(productId) as count from ratings group by productId order by count desc RatingMoreProducts 1.6.2近期热门商品的统计UDF函数 changeDate 时间戳 转换为年月的格式 分解成一个月的的热门商品 1.6.3商品的平均得分的统计统计的指标 相应的实现 1.7 离线推荐模块ALS算法进行隐语义模型训练 ALS.train() lambda 正则化参数 iterations 迭代次数 隐语义模型定义的隐特征的个数 涉及到了模型评估和参数调整 RMSE 考察预测评分和实际评分的误差 得到选取什么样的参数是最好的 1.7.1 计算用户推荐矩阵 user的RDD 和product的RDD做了一个笛卡尔积 物品两两匹配得到的结果 model.predict uid 聚合 sortBy（“score”.take(20) sparkSession.write 1.7.2计算商品相似度矩阵特征向量 矩阵分解 用户的特征向量矩阵 商品的特征向量矩阵 笛卡尔积 给实时推荐做基础 1.8 基于模型的实时推荐模块 1.8.1 需求计算速度快 结果可以不是特别精确 有预先设计好的推荐模型 评分数据 flume kafka log mongo 里面 redis里 结果写回mongo 1.8.2推荐优先级的计算刚看的商品 差评的物品 综合考虑相似度和评分 对剑优先级 计算公式 .推荐的基础评分项 奖励 惩罚 incount 评分里面的高分项 AB 高分 奖励 C 低分 惩罚 前面的基础项加权 lg 2 AB高分 C是低分 1.9 其他形式的离线相似推荐点开用户的商品详情页 出现相似的内容 基于用户购买了哪些商品 1.9.1 基于内容的推荐与A有相同的标签的商品 TF-IDF算法 根据UGC的特征提取item-CF算法 喜欢商品A的人还喜欢哪些商品 TF 词频 每一个标签 词语 每一个商品获得的一个标签 提取出物品的特征向量 余弦相似度 1.9.3 基于物品的协同过滤 1.10 混合推荐分区推荐 同时购买了商品i和商品j 2 环境的搭建2.1 安装monhgodb1234567891011wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel62-3.4.3.tgztar xvf 到 ./cluster然后mv /usr/local/mongodbcd /usr/local/mongodb创建 /usr/local/mongodb/data/usr/local/mongodb/data/db/usr/local/mongodb/data/logs/usr/local/mongodb/data/logs/mongodb.log/usr/local/mongodb/data/mongodb.conf mongodb.conf 1234567891011#端口号port = 27017#数据目录dbpath = /usr/local/mongodb/data/db#日志目录logpath = /usr/local/mongodb/data/logs/mongodb.log#设置后台运行fork = true#日志输出方式logappend = true#开启认证#auth = true 2.1.1 mongodb的启动1sudo /usr/local/mongodb/bin/mongod -config /usr/local/mongodb/data/mongodb.conf 2.1.2 mongodb的访问1/usr/local/mongodb/bin/mongo 2.1.3 mongodb的停止1/usr/local/mongodb/bin/mongo 2.2 安装Redis2.2.1 Redis的基本安装获得安装包 12wget http://download.redis.io/releases/redis-4.0.2.tar.gz 解压 tar xvf xxx ./clsuetr cd redis 1sudo yum install gcc 编译源代码 1make MALLOC=libc 编译安装 1sudo make install 创建配置文件 1sudo cp ./redis-4.0.2/redis.conf /etc/redis.conf 修改配置文件 12345daemonize yes #37行 #是否以后台daemon方式运行，默认不是后台运行pidfile /var/run/redis/redis.pid #41行 #redis的PID文件路径（可选）bind 0.0.0.0 #64行 #绑定主机IP，默认值为127.0.0.1，我们是跨机器运行，所以需要更改logfile /var/log/redis/redis.log #104行i #定义log文件位置，模式log信息定向到stdout，输出到/dev/null（可选）dir “/usr/local/rdbfile” #188行 #本地数据库存放路径，默认为./，编译安装默认存在在/usr/local/bin下（可选） 2.2.2 Redis的基本使用启动redis服务器 1redis-server /etc/redis.conf 连接redis服务器 1redis-cli 停止redis服务器 1redis-cli shutdown 2.3 安装Spark（单节点） 上传安装包 ./cluster 配置slaves 添加主机名 hadoop101 配置spark的主机名称和端口号 spark-env.sh 12SPARK_MASTER_HOST=linux #添加spark master的主机名SPARK_MASTER_PORT=7077 #添加spark master的端口号 14. 启动 1sbin/start-all.sh 2.4 安装 Zookeeper 单节点2.5 安装Flume-ng 单节点 上传安装包 1wget http://www.apache.org/dyn/closer.lua/flume/1.8.0/apache-flume-1.8.0-bin.tar.gz ​ 2. 解压 即可 2.6 安装kafka 单节点1 解压 2 配置 server.properties 3 kafka节点的使用 3 项目的搭建3.1 新建maven项目com.xxx 最外层pom.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889 &lt;properties&gt; &lt;log4j.version&gt;1.2.17&lt;/log4j.version&gt; &lt;slf4j.version&gt;1.7.22&lt;/slf4j.version&gt; &lt;mongodb-spark.version&gt;2.0.0&lt;/mongodb-spark.version&gt; &lt;casbah.version&gt;3.1.1&lt;/casbah.version&gt; &lt;redis.version&gt;2.9.0&lt;/redis.version&gt; &lt;kafka.version&gt;0.10.2.1&lt;/kafka.version&gt; &lt;spark.version&gt;2.1.1&lt;/spark.version&gt; &lt;scala.version&gt;2.11.8&lt;/scala.version&gt; &lt;jblas.version&gt;1.2.1&lt;/jblas.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;jcl-over-slf4j&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;!--maven项目 各种各样的声明周期编译的插件--&gt; &lt;build&gt; &lt;!--声明并引入子项目共有的插件--&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.6.1&lt;/version&gt; &lt;!--所有的编译用JDK1.8--&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;!--maven的打包插件--&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!--该插件用于将scala代码编译成class文件--&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt; &lt;executions&gt; &lt;!--绑定到maven的编译阶段--&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;/build&gt; 2.2reoommender 的pom.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- 引入Spark相关的Jar包 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-mllib_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-graphx_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;build&gt; &lt;plugins&gt; &lt;!-- 父项目已声明该plugin，子项目在引入的时候，不用声明版本和已经声明的配置 --&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 2.3DataLoader 的搭建pom.xml 123456789101112131415161718192021222324252627&lt;dependencies&gt; &lt;!-- Spark的依赖引入 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 引入Scala --&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 加入MongoDB的驱动 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;casbah-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;casbah.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt; &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;mongodb-spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; log4j.properties的配置 1234log4j.rootLogger=info, stdoutlog4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; %5p --- [%50t] %-80c(line:%5L) : %m%n 导入数据集 新建类 样例类 搭建基本的程序框架 隐式类的加入 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154package com.atguigu.recommenderimport com.mongodb.casbah.commons.MongoDBObjectimport com.mongodb.casbah.&#123;MongoClient, MongoClientURI&#125;import org.apache.spark.SparkConfimport org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;/* * @创建人: MaLingZhao * @创建时间: 2019/9/29 * @描述： 推荐系统的加载服务 *//*Producct 数据集21643^ 商品ID 0巴黎欧莱雅男士劲能醒肤露 8重功效50ml^ 商品名称 1916,502,352 商品的分类ID 不需要 2B0010MBKHO^ 亚马逊ID 不需要 3Y300_QL70_.jpg^ 商品的图URL 为了和业务系统相关联小说|文学艺术|图书音像 商品分类 5男士乳液/面霜|男士护肤| 商品的UGC标签 6*/case class Product(productId :Int,name:String, imageUrl:String,categories:String,tags:String)/*Rating数据集407423, 用户ID457976, 商品ID5.0, 评分数据1379001600 时间戳 *///Int 类型case class Rating(userId:Int, productId:Int,score:Double,timestamp:Int)/*uri mongoDb的ueldb 要操作的db *///mongo的配置封装样例类case class MongoConfig(uri:String,db:String)object DataLoader &#123; //定义数据文件路径 val PRODUCT_DATA_PATH="D:\\xiangmu\\ECommerceRecommendSystem\\recommender\\DataLoader\\src\\main\\resources\\products.csv" val RATING_DATA_PATH="D:\\xiangmu\\ECommerceRecommendSystem\\recommender\\DataLoader\\src\\main\\resources\\ratings.csv" //定义mongoDB中存储的表名 val MONGODB_PRODUCT_COLLECTION="Product" val MONGODB_RATING_COLLECTION="Rating" def main(args: Array[String]): Unit = &#123; val config=Map( "spark.cores" -&gt; "local[*]", "mongo.uri" -&gt; "mongodb://linux:27017/recommender", "mongo.db" -&gt;"recommender" ) //创建一个spark Config val sparkConf=new SparkConf().setMaster(config("spark.cores")).setAppName("DataLoader"); //创建sparkSession val spark=SparkSession.builder().config(sparkConf).getOrCreate() //隐式转换的包 import spark.implicits._ //加载数据 val productRDD= spark.sparkContext.textFile(PRODUCT_DATA_PATH) //转换成表结构 val productDF=productRDD.map(item =&gt;&#123; //第一个反斜杠 转义后面的反斜杠 数据通过^分割 val attr=item.split("\\^") //转换成product类 最后一行代表代码的返回 Product(attr(0).toInt,attr(1).trim,attr(4).trim,attr(5).trim,attr(6).trim) &#125;).toDF() val ratingRDD=spark.sparkContext.textFile(RATING_DATA_PATH) val ratingDF=ratingRDD.map(item=&gt;&#123; //逗号不用转义 val attr=item.split("," ) Rating(attr(0).toInt,attr(1).toInt,attr(2).toDouble,attr(3).toInt) &#125;).toDF() //调用多次的活 不用每次传入参数 implicit val mongoConfig=MongoConfig(config("mongo.uri"),config("mongo.db")) storeDataInMongoDB(productDF,ratingDF) //spark.stop()、 &#125; //隐式参数的使用 //以后调用的参数 def storeDataInMongoDB(productDF:DataFrame,raingDF:DataFrame)(implicit mongoConfig: MongoConfig): Unit = &#123; //新建和一个mongoDb的连接 客户端 val mongoClient = MongoClient(MongoClientURI(mongoConfig.uri)) //定义要操作的mongoDb中的表 val productCollection = mongoClient(mongoConfig.db)(MONGODB_PRODUCT_COLLECTION) val ratingCollection = mongoClient(mongoConfig.db)(MONGODB_RATING_COLLECTION) //如果表存在的话 删除 productCollection.dropCollection() ratingCollection.dropCollection() //将当前的数据存入对应的表中 productDF.write .option("uri", mongoConfig.uri) .option("collection", MONGODB_PRODUCT_COLLECTION) .mode("overwrite") .format("com.mongodb.spark.sql") .save() raingDF.write .option("uri", mongoConfig.uri) .option("collection", MONGODB_RATING_COLLECTION) .mode("overwrite") .format("com.mongodb.spark.sql") .save() //创建索引 //字段 productCollection.createIndex(MongoDBObject("productId" -&gt; 1)) ratingCollection.createIndex(MongoDBObject("productId" -&gt; 1)) ratingCollection.createIndex(MongoDBObject("userId" -&gt; 1)) mongoClient.close() &#125; &#125; 运行程序 测试代码吗 启动本地的mongo的测试 4 离线推荐服务 代码4.1 创建maven项目任务调度工具 离线统计服务 pom.xml 123456789101112131415161718192021222324252627&lt;dependencies&gt; &lt;!-- Spark的依赖引入 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 引入Scala --&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 加入MongoDB的驱动 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;casbah-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;casbah.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt; &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;mongodb-spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 统计学习模块 没有MLib log4j 复制过来 建立类 4.2 创建离线统计 服务 StatisticsRecommender代码部分 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142package com.atguigu.statisticsimport java.text.SimpleDateFormatimport java.util.Dateimport org.apache.spark.SparkConfimport org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;/* * @创建人: MaLingZhao * @创建时间: 2019/9/30 * @描述： *//*商品的信息不是特别的重要只需要评分的数据集 */case class Rating(userId:Int, productId:Int,score:Double,timestamp:Int)/*uri mongoDb的ueldb 要操作的db *///mongo的配置封装样例类case class MongoConfig(uri:String,db:String)object StatisticsRecommender &#123; val MONGODB_RATING_COLLECTION="Rating" //统计的表的名称 val RATE_MORE_PRODUCTS = "RateMoreProducts" val RATE_MORE_RECENTLY_PRODUCTS = "RateMoreRecentlyProducts" val AVERAGE_PRODUCTS = "AverageProducts" def main(args: Array[String]): Unit = &#123; val config = Map( "spark.cores" -&gt; "local[*]", "mongo.uri" -&gt; "mongodb://linux:27017/recommender", "mongo.db" -&gt; "recommender" ) //创建一个spark Config val sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("StatisticsRecommender"); //创建sparkSession val spark = SparkSession.builder().config(sparkConf).getOrCreate() //隐式转换的包 import spark.implicits._ //调用自己的包里面的 //调用多次的活 不用每次传入参数 implicit val mongoConfig=MongoConfig(config("mongo.uri"),config("mongo.db")) //加载数据 val ratingDF = spark.read .option("uri",mongoConfig.uri) .option("collection",MONGODB_RATING_COLLECTION) .format("com.mongodb.spark.sql") .load() //转换为定义的数据结构 .as[Rating] .toDF() //创建一张ratings的临时表 ratingDF.createOrReplaceTempView("ratings") //TODO 用spark sql 去做不同的统计推荐 //1. 历史热门商品 ，按照评分个数统计 //id 聚合 降序排列 productId count val rateMoreProductsDF =spark.sql("select productId,count(productId) as count from ratings group by productId order by count desc") //一一对应 storeDFInMongoDB(rateMoreProductsDF,RATE_MORE_PRODUCTS) //2. 近期热门商品，把时间戳转换成yyyyMM 年月格式进行评分个数统计 最终得到的是 productId count yearmonth // 创建一个日期的格式化工具 val simpleDateFormat=new SimpleDateFormat("yyyyMM") //注册UDF，将timestamp转化为年月格式yyyyMM //x为毫秒 x的格式为长整型 spark.udf.register("changeDate",(x:Int)=&gt;simpleDateFormat.format(new Date(x*1000L)).toInt) //把ratings 数据转换成想要的格式 productId，score yearmonth val ratingOfYearMonthDF=spark.sql("select productId,score,changeDate(timestamp) as yearmonth from ratings") ratingOfYearMonthDF.createOrReplaceTempView("ratingOfMonth") val rateMoreRecentlyProductDF =spark.sql("select productId, count(productId) as count ,yearmonth from ratingOfMonth group by yearmonth,productId order by yearmonth desc,count desc ") //把DF保存到mongoDB storeDFInMongoDB(rateMoreProductsDF,RATE_MORE_RECENTLY_PRODUCTS) //3. 优质商品统计 ，商品的平均得分 val averageProductsDF=spark.sql("select productId,avg(score) as avg from ratings group by productId order by avg desc ") storeDFInMongoDB(averageProductsDF,AVERAGE_PRODUCTS) spark.stop() &#125; def storeDFInMongoDB(df: DataFrame, collection_name: String)(implicit mongoConfig: MongoConfig): Unit = &#123; df.write .option("uri",mongoConfig.uri) .option("collection",collection_name) .mode("overwrite") .format("com.mongodb.spark.sql") .save()&#125;&#125;测试代码 4.3 基于隐语义模型的推荐评分矩阵 分解成两矩阵 用户商品推荐列表 ALS训练出来的Model 计算当前用户推荐列表 userId 和productId 产生(userId,productId)的分组 预测评分 评分排序 返回分支最大的K个商品 作为当前用户的推荐列表 UserRevcs 4.3.2 商品相似度矩阵 存储结构 4.3.4 创建模块 pom.xml 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scalanlp&lt;/groupId&gt; &lt;artifactId&gt;jblas&lt;/artifactId&gt; &lt;version&gt;$&#123;jblas.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Spark的依赖引入 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-mllib_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 引入Scala --&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 加入MongoDB的驱动 --&gt; &lt;!-- 用于代码方式连接MongoDB --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;casbah-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;casbah.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 用于Spark和MongoDB的对接 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt; &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;mongodb-spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; log4j配置文件的复制 4.3.5ALS算法代码部分 包装样例类 12345678910//定义标准推荐对象case class Recommendation(productId:Int,scre:Double) //定义用户的推荐列表case class UserRecs(userId:Int,recs:Seq[Recommendation]) //定义商品相似度列表case class ProductRecs(productId:Int,recs:Seq[Recommendation]) scala代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165case class ProductRating(userId:Int, productId:Int,score:Double,timestamp:Int)//定义标准推荐对象case class Recommendation(productId:Int,scre:Double)//定义用户的推荐列表case class UserRecs(userId:Int,recs:Seq[Recommendation])//定义商品相似度列表case class ProductRecs(productId:Int,recs:Seq[Recommendation])/*uri mongoDb的ueldb 要操作的db *///mongo的配置封装样例类case class MongoConfig(uri:String,db:String)object OfflineRecommender &#123; //定义mongodb中存储的表名 val MONGODB_RATING_COLLECTION = "Rating" //用户的推荐列表 val USER_RECS = "UserRecs" //商品相似度表 val PRODUCT_RECS = "ProductRecs" //最大的返回数量 val USER_MAX_RECOMMENDATION = 20 def main(args: Array[String]): Unit = &#123; val config = Map( "spark.cores" -&gt; "local[*]", "mongo.uri" -&gt; "mongodb://linux:27017/recommender", "mongo.db" -&gt; "recommender" ) //创建一个spark Config val sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("StatisticsRecommender"); //创建sparkSession val spark = SparkSession.builder().config(sparkConf).getOrCreate() //隐式转换的包 import spark.implicits._ //调用自己的包里面的 //调用多次的活 不用每次传入参数 implicit val mongoConfig = MongoConfig(config("mongo.uri"), config("mongo.db")) //加载数据 RDD ALS训练的时候使用 val ratingRDD = spark.read .option("uri", mongoConfig.uri) .option("collection", MONGODB_RATING_COLLECTION) .format("com.mongodb.spark.sql") .load() //转换为定义的数据结构 .as[ProductRating] .rdd .map( rating =&gt; (rating.userId, rating.productId, rating.score) ) .cache() //避免RDD的重复计算 // 提取出用户和商品的所有数据集 val userRDD = ratingRDD.map(_._1).distinct() val productRDD = ratingRDD.map(_._2).distinct() //TODO 核心计算过程 //1 训练隐语义模型 val trainData = ratingRDD.map(x =&gt; Rating(x._1, x._2, x._3)) //定义模型训练参数 rank 隐语义的隐特征的个数 iterations 迭代次数 lambda 正则化项系数 val (rank, iterations, lambda) = (5, 10, 0.01) val model = ALS.train(trainData, rank, iterations, lambda) //2 获得预测评分矩阵，得到用户的推荐列表 //userRDD 和productRDD 做笛卡尔积 得到空的userProductRDD表示的评分矩阵 val userProducts = userRDD.cartesian(productRDD) val preRating = model.predict(userProducts) //得到从预测评分矩阵提得到用户推荐列表 val userRecs = preRating.filter(_.rating &gt; 0) //每个用户id 物品对应的id .map( rating =&gt; (rating.user, (rating.product, rating.rating))) .groupByKey() .map &#123; case (userId, recs) =&gt; //降序排列 sort UserRecs(userId, recs.toList.sortWith(_._2 &gt; _._2).take(USER_MAX_RECOMMENDATION) .map(x =&gt; Recommendation(x._1, x._2))) &#125; .toDF() //写回到mongoDB 数据库中 userRecs.write .option("uri", mongoConfig.uri) .option("collection", USER_RECS) .mode("overwrite") .format("com.mongodb.spark.sql") .save() //3 利用商品的特征向量，计算商品的相似度列表 // 3. 利用商品的特征向量，计算商品的相似度列表 val productFeatures = model.productFeatures.map&#123; case (productId, features) =&gt; ( productId, new DoubleMatrix(features) ) &#125; // 两两配对商品，计算余弦相似度 val productRecs = productFeatures.cartesian(productFeatures) .filter&#123; case (a, b) =&gt; a._1 != b._1 &#125; // 计算余弦相似度 .map&#123; case (a, b) =&gt; val simScore = consinSim( a._2, b._2 ) ( a._1, ( b._1, simScore ) ) &#125; .filter(_._2._2 &gt; 0.4) .groupByKey() .map&#123; case (productId, recs) =&gt; ProductRecs( productId, recs.toList.sortWith(_._2&gt;_._2).map(x=&gt;Recommendation(x._1,x._2)) ) &#125; .toDF() productRecs.write .option("uri", mongoConfig.uri) .option("collection", PRODUCT_RECS) .mode("overwrite") .format("com.mongodb.spark.sql") .save() spark.stop() &#125; def consinSim(product1: DoubleMatrix, product2: DoubleMatrix): Double = &#123; product1.dot(product2)/(product1.norm2()) * product2.norm2()&#125;&#125; 4.3.3 模型参数评估选取最优的参数 RMSE 求得误差 考察 12 ALSTrainer 代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071 object ALSTrainer&#123;def main(args: Array[String]): Unit = &#123; val config = Map( "spark.cores" -&gt; "local[*]", "mongo.uri" -&gt; "mongodb://localhost:27017/recommender", "mongo.db" -&gt; "recommender" ) // 创建一个spark config val sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("OfflineRecommender") // 创建spark session val spark = SparkSession.builder().config(sparkConf).getOrCreate() import spark.implicits._ implicit val mongoConfig = MongoConfig( config("mongo.uri"), config("mongo.db") ) // 加载数据 val ratingRDD = spark.read .option("uri", mongoConfig.uri) .option("collection", MONGODB_RATING_COLLECTION) .format("com.mongodb.spark.sql") .load() .as[ProductRating] .rdd .map( rating =&gt; Rating(rating.userId, rating.productId, rating.score) ).cache() // 数据集切分成训练集和测试集 val splits = ratingRDD.randomSplit(Array(0.8, 0.2)) val trainingRDD = splits(0) val testingRDD = splits(1) // 核心实现：输出最优参数 adjustALSParams( trainingRDD, testingRDD ) spark.stop()&#125; def adjustALSParams(trainData: RDD[Rating], testData: RDD[Rating]): Unit =&#123; // 遍历数组中定义的参数取值 val result = for( rank &lt;- Array(5, 10, 20, 50); lambda &lt;- Array(1, 0.1, 0.01) ) yield &#123; val model = ALS.train(trainData, rank, 10, lambda) val rmse = getRMSE( model, testData ) ( rank, lambda, rmse )&#125; // 按照rmse排序并输出最优参数 println(result.minBy(_._3))&#125; def getRMSE(model: MatrixFactorizationModel, data: RDD[Rating]): Double = &#123; //构建UserProducts， 得到预测的评分矩阵 val userProducts: RDD[(Int, Int)] = data.map(item=&gt;(item.user,item.product)) val predictRating: RDD[Rating] = model.predict(userProducts) //按照公式计算 RMSE，首先把预测评分和实际评分表做一个连接 以（userID和productID）做一个连接 val observed : RDD[((Int, Int), Double)] = data.map(item=&gt;((item.user,item.product),item.rating)) val predict: RDD[((Int, Int), Double)] = predictRating.map(item=&gt;((item.user,item.product),item.rating)) sqrt(observed.join(predict).map&#123; case((userId,productId),(actual,pre)) =&gt; val error = actual -pre error*error &#125;.mean()) &#125;&#125; 5 实时推荐模块5.1 分析用户最近的偏好 之前买过什么商品 基于这样的思想 构建模型 不需要那么精确 基于模型的实时架构 mongo redis 日志 评分数据 userId productId 时间戳 flume kafka Stream 过滤 recommender spark streaming 定义实时推荐算法 用户最近的评分 redis 推荐优先级的计算 基本原理 用户最近一段时间的口味是相似的 最近的k次评分 好评 差评 5.1.2 推荐优先级计算 5.1.3 实现创建项目 pom.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&lt;dependencies&gt; &lt;!-- Spark的依赖引入 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 引入Scala --&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 加入MongoDB的驱动 --&gt; &lt;!-- 用于代码方式连接MongoDB --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;casbah-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;casbah.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 用于Spark和MongoDB的对接 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt; &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;mongodb-spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- redis --&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- kafka --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.10.2.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 实时系统的搭建 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213package com.atguigu.onlineimport com.mongodb.casbah.commons.MongoDBObjectimport com.mongodb.casbah.&#123;MongoClient, MongoClientURI&#125;import org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.SparkConfimport org.apache.spark.sql.SparkSessionimport org.apache.spark.streaming.kafka010.&#123;ConsumerStrategies, KafkaUtils, LocationStrategies&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import redis.clients.jedis.Jedis// 定义一个连接助手对象，建立到redis和mongodb的连接object ConnHelper extends Serializable&#123; // 懒变量定义，使用的时候才初始化 lazy val jedis = new Jedis("localhost") lazy val mongoClient = MongoClient(MongoClientURI("mongodb://linux:27017/recommender"))&#125;case class MongoConfig( uri: String, db: String )// 定义标准推荐对象case class Recommendation( productId: Int, score: Double )// 定义用户的推荐列表case class UserRecs( userId: Int, recs: Seq[Recommendation] )// 定义商品相似度列表case class ProductRecs( productId: Int, recs: Seq[Recommendation] )object OnlineRecommender &#123; // 定义常量和表名 val MONGODB_RATING_COLLECTION = "Rating" val STREAM_RECS = "StreamRecs" val PRODUCT_RECS = "ProductRecs" val MAX_USER_RATING_NUM = 20 val MAX_SIM_PRODUCTS_NUM = 20 def main(args: Array[String]): Unit = &#123; val config = Map( "spark.cores" -&gt; "local[*]", "mongo.uri" -&gt; "mongodb://localhost:27017/recommender", "mongo.db" -&gt; "recommender", "kafka.topic" -&gt; "recommender" ) // 创建spark conf val sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("OnlineRecommender") val spark = SparkSession.builder().config(sparkConf).getOrCreate() val sc = spark.sparkContext val ssc = new StreamingContext(sc, Seconds(2)) import spark.implicits._ implicit val mongoConfig = MongoConfig( config("mongo.uri"), config("mongo.db") ) // 加载数据，相似度矩阵，广播出去 val simProductsMatrix = spark.read .option("uri", mongoConfig.uri) .option("collection", PRODUCT_RECS) .format("com.mongodb.spark.sql") .load() .as[ProductRecs] .rdd // 为了后续查询相似度方便，把数据转换成map形式 .map&#123;item =&gt; ( item.productId, item.recs.map( x=&gt;(x.productId, x.score) ).toMap ) &#125; .collectAsMap() // 定义广播变量 val simProcutsMatrixBC = sc.broadcast(simProductsMatrix) // 创建kafka配置参数 val kafkaParam = Map( "bootstrap.servers" -&gt; "linux:9092", "key.deserializer" -&gt; classOf[StringDeserializer], "value.deserializer" -&gt; classOf[StringDeserializer], "group.id" -&gt; "recommender", "auto.offset.reset" -&gt; "latest" ) // 创建一个DStream val kafkaStream = KafkaUtils.createDirectStream[String, String]( ssc, LocationStrategies.PreferConsistent, ConsumerStrategies.Subscribe[String, String]( Array(config("kafka.topic")), kafkaParam ) ) // 对kafkaStream进行处理，产生评分流，userId|productId|score|timestamp val ratingStream = kafkaStream.map&#123;msg=&gt; var attr = msg.value().split("\\|") ( attr(0).toInt, attr(1).toInt, attr(2).toDouble, attr(3).toInt ) &#125; // 核心算法部分，定义评分流的处理流程 ratingStream.foreachRDD&#123; rdds =&gt; rdds.foreach&#123; case ( userId, productId, score, timestamp ) =&gt; println("rating data coming!&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;") // TODO: 核心算法流程 // 1. 从redis里取出当前用户的最近评分，保存成一个数组Array[(productId, score)] val userRecentlyRatings = getUserRecentlyRatings( MAX_USER_RATING_NUM, userId, ConnHelper.jedis ) // 2. 从相似度矩阵中获取当前商品最相似的商品列表，作为备选列表，保存成一个数组Array[productId] val candidateProducts = getTopSimProducts( MAX_SIM_PRODUCTS_NUM, productId, userId, simProcutsMatrixBC.value ) // 3. 计算每个备选商品的推荐优先级，得到当前用户的实时推荐列表，保存成 Array[(productId, score)] val streamRecs = computeProductScore( candidateProducts, userRecentlyRatings, simProcutsMatrixBC.value ) // 4. 把推荐列表保存到mongodb saveDataToMongoDB( userId, streamRecs ) &#125; &#125; // 启动streaming ssc.start() println("streaming started!") ssc.awaitTermination() &#125; /** * 从redis里获取最近num次评分 */ import scala.collection.JavaConversions._ def getUserRecentlyRatings(num: Int, userId: Int, jedis: Jedis): Array[(Int, Double)] = &#123; // 从redis中用户的评分队列里获取评分数据，list键名为uid:USERID，值格式是 PRODUCTID:SCORE jedis.lrange( "userId:" + userId.toString, 0, num ) .map&#123; item =&gt; val attr = item.split("\\:") ( attr(0).trim.toInt, attr(1).trim.toDouble ) &#125; .toArray &#125; // 获取当前商品的相似列表，并过滤掉用户已经评分过的，作为备选列表 def getTopSimProducts(num: Int, productId: Int, userId: Int, simProducts: scala.collection.Map[Int, scala.collection.immutable.Map[Int, Double]]) (implicit mongoConfig: MongoConfig): Array[Int] =&#123; // 从广播变量相似度矩阵中拿到当前商品的相似度列表 val allSimProducts = simProducts(productId).toArray // 获得用户已经评分过的商品，过滤掉，排序输出 val ratingCollection = ConnHelper.mongoClient( mongoConfig.db )( MONGODB_RATING_COLLECTION ) val ratingExist = ratingCollection.find( MongoDBObject("userId"-&gt;userId) ) .toArray .map&#123;item=&gt; // 只需要productId item.get("productId").toString.toInt &#125; // 从所有的相似商品中进行过滤 allSimProducts.filter( x =&gt; ! ratingExist.contains(x._1) ) .sortWith(_._2 &gt; _._2) .take(num) .map(x=&gt;x._1) &#125; // 计算每个备选商品的推荐得分 def computeProductScore(candidateProducts: Array[Int], userRecentlyRatings: Array[(Int, Double)], simProducts: scala.collection.Map[Int, scala.collection.immutable.Map[Int, Double]]) : Array[(Int, Double)] =&#123; // 定义一个长度可变数组ArrayBuffer，用于保存每一个备选商品的基础得分，(productId, score) val scores = scala.collection.mutable.ArrayBuffer[(Int, Double)]() // 定义两个map，用于保存每个商品的高分和低分的计数器，productId -&gt; count val increMap = scala.collection.mutable.HashMap[Int, Int]() val decreMap = scala.collection.mutable.HashMap[Int, Int]() // 遍历每个备选商品，计算和已评分商品的相似度 for( candidateProduct &lt;- candidateProducts; userRecentlyRating &lt;- userRecentlyRatings )&#123; // 从相似度矩阵中获取当前备选商品和当前已评分商品间的相似度 val simScore = getProductsSimScore( candidateProduct, userRecentlyRating._1, simProducts ) if( simScore &gt; 0.4 )&#123; // 按照公式进行加权计算，得到基础评分 scores += ( (candidateProduct, simScore * userRecentlyRating._2) ) if( userRecentlyRating._2 &gt; 3 )&#123; increMap(candidateProduct) = increMap.getOrDefault(candidateProduct, 0) + 1 &#125; else &#123; decreMap(candidateProduct) = decreMap.getOrDefault(candidateProduct, 0) + 1 &#125; &#125; &#125; // 根据公式计算所有的推荐优先级，首先以productId做groupby scores.groupBy(_._1).map&#123; case (productId, scoreList) =&gt; ( productId, scoreList.map(_._2).sum/scoreList.length + log(increMap.getOrDefault(productId, 1)) - log(decreMap.getOrDefault(productId, 1)) ) &#125; // 返回推荐列表，按照得分排序 .toArray .sortWith(_._2&gt;_._2) &#125; def getProductsSimScore(product1: Int, product2: Int, simProducts: scala.collection.Map[Int, scala.collection.immutable.Map[Int, Double]]): Double =&#123; simProducts.get(product1) match &#123; case Some(sims) =&gt; sims.get(product2) match &#123; case Some(score) =&gt; score case None =&gt; 0.0 &#125; case None =&gt; 0.0 &#125; &#125; // 自定义log函数，以N为底 def log(m: Int): Double = &#123; val N = 10 math.log(m)/math.log(N) &#125; // 写入mongodb def saveDataToMongoDB(userId: Int, streamRecs: Array[(Int, Double)])(implicit mongoConfig: MongoConfig): Unit =&#123; val streamRecsCollection = ConnHelper.mongoClient(mongoConfig.db)(STREAM_RECS) // 按照userId查询并更新 streamRecsCollection.findAndRemove( MongoDBObject( "userId" -&gt; userId ) ) streamRecsCollection.insert( MongoDBObject( "userId" -&gt; userId, "recs" -&gt; streamRecs.map(x=&gt;MongoDBObject("productId"-&gt;x._1, "score"-&gt;x._2)) ) ) &#125;&#125; 6 实时系统的联调KafkaStreaming模块的搭建 pom.xml 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-streams&lt;/artifactId&gt; &lt;version&gt;0.10.2.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.10.2.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;kafkastream&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.atguigu.kafkastream.Application&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 代码的实现 Application 12345678910111213141516171819202122232425262728293031323334353637383940package com.atguigu.kafkastream; import org.apache.kafka.streams.KafkaStreams;import org.apache.kafka.streams.StreamsConfig;import org.apache.kafka.streams.processor.TopologyBuilder;import java.util.Properties;public class Application &#123; public static void main(String[] args) &#123; String brokers = "linux:9092"; String zookeepers = "linux:2181"; // 定义输入和输出的topic String from = "log"; String to = "recommender"; // 定义kafka stream 配置参数 Properties settings = new Properties(); settings.put(StreamsConfig.APPLICATION_ID_CONFIG, "logFilter"); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, brokers); settings.put(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeepers); // 创建kafka stream 配置对象 StreamsConfig config = new StreamsConfig(settings); // 定义拓扑构建器 TopologyBuilder builder = new TopologyBuilder(); builder.addSource("SOURCE", from) .addProcessor("PROCESSOR", ()-&gt;new LogProcessor(), "SOURCE") .addSink("SINK", to, "PROCESSOR"); // 创建kafka stream KafkaStreams streams = new KafkaStreams( builder, config ); streams.start(); System.out.println("kafka stream started!"); &#125;&#125; LogProcessor 12345678910111213141516171819202122232425262728293031323334353637383940package com.atguigu.kafkastream;import org.apache.kafka.streams.processor.Processor;import org.apache.kafka.streams.processor.ProcessorContext;/** * @ClassName: LogProcessor * @Description: * @Author: wushengran on 2019/4/28 15:08 * @Version: 1.0 */public class LogProcessor implements Processor&lt;byte[], byte[]&gt;&#123; private ProcessorContext context; @Override public void init(ProcessorContext processorContext) &#123; this.context = processorContext; &#125; @Override public void process(byte[] dummy, byte[] line) &#123; // 核心处理流程 String input = new String(line); // 提取数据，以固定前缀过滤日志信息 if( input.contains("PRODUCT_RATING_PREFIX:") )&#123; System.out.println("product rating data coming! " + input); input = input.split("PRODUCT_RATING_PREFIX:")[1].trim(); context.forward("logProcessor".getBytes(), input.getBytes()); &#125; &#125; @Override public void punctuate(long l) &#123; &#125; @Override public void close() &#123; &#125;&#125; 6.2 配置启动Flumeflume的conf 目录下 123456789101112131415161718192021222324agent.sources = exectailagent.channels = memoryChannelagent.sinks = kafkasink# For each one of the sources, the type is definedagent.sources.exectail.type = exec# 下面这个路径是需要收集日志的绝对路径，改为自己的日志目录agent.sources.exectail.command = tail –f/mnt/d/Projects/BigData/ECommerceRecommenderSystem/businessServer/src/main/log/agent.logagent.sources.exectail.interceptors=i1agent.sources.exectail.interceptors.i1.type=regex_filter# 定义日志过滤前缀的正则agent.sources.exectail.interceptors.i1.regex=.+PRODUCT_RATING_PREFIX.+# The channel can be defined as follows.agent.sources.exectail.channels = memoryChannel# Each sink's type must be definedagent.sinks.kafkasink.type = org.apache.flume.sink.kafka.KafkaSinkagent.sinks.kafkasink.kafka.topic = logagent.sinks.kafkasink.kafka.bootstrap.servers = localhost:9092agent.sinks.kafkasink.kafka.producer.acks = 1agent.sinks.kafkasink.kafka.flumeBatchSize = 20#Specify the channel the sink should use 12 启动flume 1./bin/flume-ng agent -c ./conf/ -f ./conf/log-kafka.properties -n agent -Dflume.root.logger=INFO,console 启动zookeeper 1234bin/zkServer.sh start启动kafkabin/kafka-server-start.sh -daemon ./config/server.properties 测试实时推荐模块 连接的东西 先启动zookeeper 和kafka 在启动redis 然后redis的客户端 6 冷启动问题的解决实际项目中遇到的问题 冷启动的问题 算法是基于隐语义模型的 冷启动问题的处理 进来注册的时候 让你勾选项 了解即可 7 其他形式的离线相似推荐买了这个商品的他、用户 跟商品的相似 对应的相似的产品作出推荐 推荐算法分类的时候 用户画像 基于商品内容推荐 7.1 基于内容的相似度推荐id 名称 图片url 分类 ugc标签 主要基于UGC标签 创建项目 pom文件 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scalanlp&lt;/groupId&gt; &lt;artifactId&gt;jblas&lt;/artifactId&gt; &lt;version&gt;$&#123;jblas.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Spark的依赖引入 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-mllib_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 引入Scala --&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 加入MongoDB的驱动 --&gt; &lt;!-- 用于代码方式连接MongoDB --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;casbah-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;casbah.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 用于Spark和MongoDB的对接 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt; &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;mongodb-spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 实现代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108package com.atguigu.contentimport org.apache.spark.SparkConfimport org.apache.spark.ml.feature.&#123;HashingTF, IDF, Tokenizer&#125;import org.apache.spark.ml.linalg.SparseVectorimport org.apache.spark.sql.SparkSessionimport org.jblas.DoubleMatrixcase class Product( productId: Int, name: String, imageUrl: String, categories: String, tags: String )case class MongoConfig( uri: String, db: String )// 定义标准推荐对象case class Recommendation( productId: Int, score: Double )// 定义商品相似度列表case class ProductRecs( productId: Int, recs: Seq[Recommendation] )object ContentRecommender &#123; // 定义mongodb中存储的表名 val MONGODB_PRODUCT_COLLECTION = "Product" val CONTENT_PRODUCT_RECS = "ContentBasedProductRecs" def main(args: Array[String]): Unit = &#123; val config = Map( "spark.cores" -&gt; "local[*]", "mongo.uri" -&gt; "mongodb://linux:27017/recommender", "mongo.db" -&gt; "recommender" ) // 创建一个spark config val sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("ContentRecommender") // 创建spark session val spark = SparkSession.builder().config(sparkConf).getOrCreate() import spark.implicits._ implicit val mongoConfig = MongoConfig( config("mongo.uri"), config("mongo.db") ) // 载入数据，做预处理 val productTagsDF = spark.read .option("uri", mongoConfig.uri) .option("collection", MONGODB_PRODUCT_COLLECTION) .format("com.mongodb.spark.sql") .load() .as[Product] .map( x =&gt; ( x.productId, x.name, x.tags.map(c=&gt; if(c=='|') ' ' else c) ) ) .toDF("productId", "name", "tags") .cache() // TODO: 用TF-IDF提取商品特征向量 // 1. 实例化一个分词器，用来做分词，默认按照空格分 val tokenizer = new Tokenizer().setInputCol("tags").setOutputCol("words") // 用分词器做转换，得到增加一个新列words的DF val wordsDataDF = tokenizer.transform(productTagsDF) // 2. 定义一个HashingTF工具，计算频次 val hashingTF = new HashingTF().setInputCol("words").setOutputCol("rawFeatures").setNumFeatures(800) val featurizedDataDF = hashingTF.transform(wordsDataDF) // 3. 定义一个IDF工具，计算TF-IDF val idf = new IDF().setInputCol("rawFeatures").setOutputCol("features") // 训练一个idf模型 val idfModel = idf.fit(featurizedDataDF) // 得到增加新列features的DF val rescaledDataDF = idfModel.transform(featurizedDataDF) // 对数据进行转换，得到RDD形式的features val productFeatures = rescaledDataDF.map&#123; row =&gt; ( row.getAs[Int]("productId"), row.getAs[SparseVector]("features").toArray ) &#125; .rdd .map&#123; case (productId, features) =&gt; ( productId, new DoubleMatrix(features) ) &#125; // 两两配对商品，计算余弦相似度 val productRecs = productFeatures.cartesian(productFeatures) .filter&#123; case (a, b) =&gt; a._1 != b._1 &#125; // 计算余弦相似度 .map&#123; case (a, b) =&gt; val simScore = consinSim( a._2, b._2 ) ( a._1, ( b._1, simScore ) ) &#125; .filter(_._2._2 &gt; 0.4) .groupByKey() .map&#123; case (productId, recs) =&gt; ProductRecs( productId, recs.toList.sortWith(_._2&gt;_._2).map(x=&gt;Recommendation(x._1,x._2)) ) &#125; .toDF() productRecs.write .option("uri", mongoConfig.uri) .option("collection", CONTENT_PRODUCT_RECS) .mode("overwrite") .format("com.mongodb.spark.sql") .save() spark.stop() &#125; def consinSim(product1: DoubleMatrix, product2: DoubleMatrix): Double =&#123; product1.dot(product2)/ ( product1.norm2() * product2.norm2() ) &#125;&#125; 7.2 基于ItemCF的推荐算法 123456789101112131415161718192021222324252627282930&lt;dependencies&gt; &lt;!-- Spark的依赖引入 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 引入Scala --&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 加入MongoDB的驱动 --&gt; &lt;!-- 用于代码方式连接MongoDB --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;casbah-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;casbah.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 用于Spark和MongoDB的对接 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt; &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;mongodb-spark.version&#125;&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109package com.atguigu.itemcfimport org.apache.spark.SparkConfimport org.apache.spark.sql.SparkSessioncase class ProductRating( userId: Int, productId: Int, score: Double, timestamp: Int )case class MongoConfig( uri: String, db: String )// 定义标准推荐对象case class Recommendation( productId: Int, score: Double )// 定义商品相似度列表case class ProductRecs( productId: Int, recs: Seq[Recommendation] )object ItemCFRecommender &#123; // 定义常量和表名 val MONGODB_RATING_COLLECTION = "Rating" val ITEM_CF_PRODUCT_RECS = "ItemCFProductRecs" val MAX_RECOMMENDATION = 10 def main(args: Array[String]): Unit = &#123; val config = Map( "spark.cores" -&gt; "local[*]", "mongo.uri" -&gt; "mongodb://linux:27017/recommender", "mongo.db" -&gt; "recommender" ) // 创建一个spark config val sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("ItemCFRecommender") // 创建spark session val spark = SparkSession.builder().config(sparkConf).getOrCreate() import spark.implicits._ implicit val mongoConfig = MongoConfig( config("mongo.uri"), config("mongo.db") ) // 加载数据，转换成DF进行处理 val ratingDF = spark.read .option("uri", mongoConfig.uri) .option("collection", MONGODB_RATING_COLLECTION) .format("com.mongodb.spark.sql") .load() .as[ProductRating] .map( x =&gt; ( x.userId, x.productId, x.score ) ) .toDF("userId", "productId", "score") .cache() // TODO: 核心算法，计算同现相似度，得到商品的相似列表 // 统计每个商品的评分个数，按照productId来做group by val productRatingCountDF = ratingDF.groupBy("productId").count() // 在原有的评分表上rating添加count val ratingWithCountDF = ratingDF.join(productRatingCountDF, "productId") // 将评分按照用户id两两配对，统计两个商品被同一个用户评分过的次数 val joinedDF = ratingWithCountDF.join(ratingWithCountDF, "userId") .toDF("userId","product1","score1","count1","product2","score2","count2") .select("userId","product1","count1","product2","count2") // 创建一张临时表，用于写sql查询 joinedDF.createOrReplaceTempView("joined") // 按照product1,product2 做group by，统计userId的数量，就是对两个商品同时评分的人数 val cooccurrenceDF = spark.sql( """ |select product1 |, product2 |, count(userId) as cocount |, first(count1) as count1 |, first(count2) as count2 |from joined |group by product1, product2 """.stripMargin ).cache() // 提取需要的数据，包装成( productId1, (productId2, score) ) val simDF = cooccurrenceDF.map&#123; row =&gt; val coocSim = cooccurrenceSim( row.getAs[Long]("cocount"), row.getAs[Long]("count1"), row.getAs[Long]("count2") ) ( row.getInt(0), ( row.getInt(1), coocSim ) ) &#125; .rdd .groupByKey() .map&#123; case (productId, recs) =&gt; ProductRecs( productId, recs.toList .filter(x=&gt;x._1 != productId) .sortWith(_._2&gt;_._2) .take(MAX_RECOMMENDATION) .map(x=&gt;Recommendation(x._1,x._2)) ) &#125; .toDF() // 保存到mongodb simDF.write .option("uri", mongoConfig.uri) .option("collection", ITEM_CF_PRODUCT_RECS) .mode("overwrite") .format("com.mongodb.spark.sql") .save() spark.stop() &#125; // 按照公式计算同现相似度 def cooccurrenceSim(coCount: Long, count1: Long, count2: Long): Double =&#123; coCount / math.sqrt( count1 * count2 ) &#125;&#125;]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>项目实战</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java面试]]></title>
    <url>%2Fblog4%2F2019%2F10%2F08%2Fjava%2F</url>
    <content type="text"><![CDATA[1 各大厂的面试题1.1 蚂蚁花呗一个小时 1.2美团的一面 垃圾回收器 1.3 百度 java集合类 synchronized 什么是对象锁 什么是全局锁 1.4 头条 1.5 美团的面试汇总 1.6 蚂蚁金服二面 1.7 讲解1 关于2018.12 月份 ，。互联网公司大规模的缩招 裁员 缩招不是不招聘 而是招聘更多的优质的咖啡啊工程师 2 将最近半年的大厂面试题进行了整理和划分 1 1.8 3 1 个人 1.8倍的工资 干三个人的活 3 第一次 提出高频最多的常见笔试面试题目 ArrayList HashMap 底层是什么东西 4 JVM/GC 多线程与高并发 1.8 技术框架 大厂的面试题 90% 1.9 redis的相关题目 哪些数据存mysql 哪些 redis 如何保持移植性 redis缓存给了多大的总内存 命中率多高 超大Value打满网卡的问题 1.10 消息中间件MQrabibtMQ 消息中间件 消息积压了两个小时 消息中间件只有一个 挂掉 影响业务 1.11 JVM+GC的解析 oom 的东西 了解 Out of Memory Error java内存溢出 四大引用 强 软 弱 虚 水平 泯然众人与 性能检测工具 1.12 JUC多线程及高并发1.13 面试重点jvm + GC JUC多线程高并发 本次讲解 互联网笔试题第二季 JVM/GC的知识 JUC的前提只是 超级熟悉java8的与、新特性 （Stream+lambdaExpress+函数接口+方法引用） 2 JUC多线程及高并发 current 并发 高并发 秒杀 多个线程访问 统一个资源 并行 各种事情一路并行去做 节水 道调料 atomic 院原子性 AtomicInteger 原子引用 2.1 请你谈谈你对volatile的理解 同步 synchronized 轻量级 什么是轻 三大特性 volatile是轻量级的同步机制 不是文科 理解 jMM关于同步的 2.1.1 JMM值内存可见性 volatile 可见性1 理论线程 —-》》》 工作内存 —-》》 每个线程的私有内存区域 变量 —-》》 主内存 —》》 共享内存区域 线程可以访问 t1改成37 了 t2 t3不知道 1 t1 拷走25 2 t1改成37 3 把37 写回主内存 线程2 和线程3 不知道 只要有一个线程修改完自己的工作空间值之后写回主内存 及时通知其他的线程 这种机制 JMM之内存模型的第一个特性可见性 主内存的值只要被修改 其他的线程马上获得通知 改课 干活 —》》线程 自己的工作内存 私有数据 new 3 个线程 java内存模型规定所有的变量在主内训 主内存 拷贝到自己的工作内存空间 不同的线程无法访问对方的工作内存 线程间的通信（传值） 必须靠主内存来完成 拷贝共享变量的初始值 线程的工作内存里面 西城区和东城区的售票员 电话确认不可能 线程运算完 写回主内存 可见性 有了最新消息 第一个通知 论证 2 代码灭有volatile的操作 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/8 * @描述： */import java.util.concurrent.TimeUnit;class MyData&#123; int number=0; public void addTO60() &#123; this.number=60; &#125;&#125;/** 1 验证volatile的可见性* 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰** */public class VolatileDemo &#123; public static void main(String[] args) &#123; //main是一切方法的运行入口 MyData myData=new MyData();//资源类 new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+"\t come in"); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; myData.addTO60(); System.out.println(Thread.currentThread().getName()+ "\t updated number value:"+ myData.number); &#125;,"AAA").start(); //第二个线程就是我们的main线程 while (myData.number==0) &#123; //main线程一直在这里等待循环 直到这个number不在等于0 没有可见性 &#125; System.out.println(Thread.currentThread().getName()+"\t misson is over"); &#125;&#125; 最后一句没有打印 没有人通知 main线程傻傻的等待 没有人通知我改了 我不知道 加上volatile的代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/8 * @描述： */import java.util.concurrent.TimeUnit;class MyData&#123; volatile int number=0; public void addTO60() &#123; this.number=60; &#125;&#125;/** 1 验证volatile的可见性* 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰** */public class VolatileDemo &#123; public static void main(String[] args) &#123; //main是一切方法的运行入口 MyData myData=new MyData();//资源类 new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+"\t come in"); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; myData.addTO60(); System.out.println(Thread.currentThread().getName()+ "\t updated number value:"+ myData.number); &#125;,"AAA").start(); //第二个线程就是我们的main线程 while (myData.number==0) &#123; //main线程一直在这里等待循环 直到这个number不在等于0 没有可见性 &#125; System.out.println(Thread.currentThread().getName()+"\t misson is over"+"main get value:"+myData.number); &#125;&#125; 可见性证明 缓存 JMM的一种内存抽象机制 抽象的概念 再次阅读 轻量级 乞丐版的synchronized 2.1.2 JMM之原子性 volatile不保证原子性volatile是不保证原子性的 1 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/8 * @描述： */import java.util.concurrent.TimeUnit;class MyData&#123; volatile int number=0; public void addTO60() &#123; this.number=60; &#125; /* 此时number的前面是加了voatile关键字的修饰的，volatile不保证原子性 */public void addPlus()&#123; number++;&#125;&#125;/** 1 验证volatile的可见性* 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰** */public class VolatileDemo &#123;//main一切方法的入口 public static void main(String[] args) &#123; MyData myData=new MyData(); for (int i = 1; i &lt;=20 ; i++) &#123; new Thread(()-&gt;&#123; for (int j = 1; j &lt;=1000 ; j++) &#123; myData.addPlus(); &#125; &#125;,String.valueOf(i) ).start(); &#125; //等待上面的20个线程全部计算完成 再用main线程取得的最终的结果值是什么 //后台的线程 /*1 main 2 后台GC线程 * */ while(Thread.activeCount()&gt;2) &#123; Thread.yield(); &#125; System.out.println(Thread.currentThread().getName()+"\t finally n umber value: "+ myData.number); &#125; /*volatile可以保证可见性 可以通知其线程主物理内存的值已经被修改 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰 2 验证volatile不保证原子性 2.1 原子性指的是什么意思？ 不可分割 完整性 也即某个线程在做某个具体的业务的时候 中间不可以被加塞或者分割 需要整体完整 要么同时成功 要么同时失败 一段时间内只允许一个操作 2.2 volatile 是否可以保证原子性 */ public static void seeOkByVolatile() &#123; MyData myData=new MyData();//资源类 new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+"\t come in"); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; myData.addTO60(); System.out.println(Thread.currentThread().getName()+ "\t updated number value:"+ myData.number); &#125;,"AAA").start(); //第二个线程就是我们的main线程 while (myData.number==0) &#123; //main线程一直在这里等待循环 直到这个number不在等于0 没有可见性 &#125; System.out.println(Thread.currentThread().getName()+"\t misson is over"+"main get value:"+myData.number); &#125;&#125; 出来的不是2万 说明volatile不能够保证原子性 有没有可能加到2万 但是有极其特殊的情况是可能达到2万的 加了synchronized 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/8 * @描述： */import java.util.concurrent.TimeUnit;class MyData&#123; volatile int number=0; public void addTO60() &#123; this.number=60; &#125; /* 此时number的前面是加了voatile关键字的修饰的，volatile不保证原子性 */public synchronized void addPlus()&#123; number++;&#125;&#125;/** 1 验证volatile的可见性* 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰** */public class VolatileDemo &#123;//main一切方法的入口 public static void main(String[] args) &#123; MyData myData=new MyData(); for (int i = 1; i &lt;=20 ; i++) &#123; new Thread(()-&gt;&#123; for (int j = 1; j &lt;=1000 ; j++) &#123; myData.addPlus(); &#125; &#125;,String.valueOf(i) ).start(); &#125; //等待上面的20个线程全部计算完成 再用main线程取得的最终的结果值是什么 //后台的线程 /*1 main 2 后台GC线程 * */ while(Thread.activeCount()&gt;2) &#123; Thread.yield(); &#125; System.out.println(Thread.currentThread().getName()+"\t finally n umber value: "+ myData.number); &#125; /*volatile可以保证可见性 可以通知其线程主物理内存的值已经被修改 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰 2 验证volatile不保证原子性 2.1 原子性指的是什么意思？ 不可分割 完整性 也即某个线程在做某个具体的业务的时候 中间不可以被加塞或者分割 需要整体完整 要么同时成功 要么同时失败 一段时间内只允许一个操作 2.2 volatile 不保证原子性的案例的演示 */ public static void seeOkByVolatile() &#123; MyData myData=new MyData();//资源类 new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+"\t come in"); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; myData.addTO60(); System.out.println(Thread.currentThread().getName()+ "\t updated number value:"+ myData.number); &#125;,"AAA").start(); //第二个线程就是我们的main线程 while (myData.number==0) &#123; //main线程一直在这里等待循环 直到这个number不在等于0 没有可见性 &#125; System.out.println(Thread.currentThread().getName()+"\t misson is over"+"main get value:"+myData.number); &#125;&#125; 所以说volatile是 轻量级的同步机制 java c java verbose 12345678910public class T1 &#123;volatile int n =0;public void add()&#123; n++;&#125;&#125; 写覆盖 1 1 1 操作了3次 加1 各自的工作内存加1 由于synchronized 不能保证原子性 getfield iadd putfield 你先写 你先写 原子性 没有写完的时候 另外一个线程已经被唤醒 putfield 可能线程写入 丢失 后面的线程可能会把前面的线程写覆盖掉 JMM内存模型要求保证原子性 volatile不保证原子性 运行程序保证是2万 方法1 synchronized xxx 功能|+性能 原子包装的整型类 AtomicInteger的引入 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/8 * @描述： */import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicInteger;class MyData&#123; //MyData.java ===&gt; MyData.class =====&gt; jvm字节码 volatile int number=0; public void addTO60() &#123; this.number=60; &#125; /* 此时number的前面是加了voatile关键字的修饰的，volatile不保证原子性 */public void addPlus()&#123; number++;&#125;AtomicInteger atomicInteger=new AtomicInteger(); public void addAtomic() &#123;atomicInteger.getAndIncrement(); &#125;&#125;/** 1 验证volatile的可见性* 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰** */public class VolatileDemo &#123;//main一切方法的入口 public static void main(String[] args) &#123; MyData myData=new MyData(); for (int i = 1; i &lt;=20 ; i++) &#123; new Thread(()-&gt;&#123; for (int j = 1; j &lt;=1000 ; j++) &#123; myData.addPlus(); myData.addAtomic(); &#125; &#125;,String.valueOf(i) ).start(); &#125; //等待上面的20个线程全部计算完成 再用main线程取得的最终的结果值是什么 //后台的线程 /*1 main 2 后台GC线程 * */ while(Thread.activeCount()&gt;2) &#123; Thread.yield(); &#125; System.out.println(Thread.currentThread().getName()+"\t finally n umber value: "+ myData.number); System.out.println(Thread.currentThread().getName()+"\t AtomicInteger type finally n umber value: "+ myData.atomicInteger); &#125; /*volatile可以保证可见性 可以通知其线程主物理内存的值已经被修改 1.1 加入 int number=0，number变量之前根本没有添加volatile关键字修饰 2 验证volatile不保证原子性 2.1 原子性指的是什么意思？ 不可分割 完整性 也即某个线程在做某个具体的业务的时候 中间不可以被加塞或者分割 需要整体完整 要么同时成功 要么同时失败 一段时间内只允许一个操作 2.2 volatile 不保证原子性的案例的演示 */ public static void seeOkByVolatile() &#123; MyData myData=new MyData();//资源类 new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+"\t come in"); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; myData.addTO60(); System.out.println(Thread.currentThread().getName()+ "\t updated number value:"+ myData.number); &#125;,"AAA").start(); //第二个线程就是我们的main线程 while (myData.number==0) &#123; //main线程一直在这里等待循环 直到这个number不在等于0 没有可见性 &#125; System.out.println(Thread.currentThread().getName()+"\t misson is over"+"main get value:"+myData.number); &#125;&#125; 2.1.3 volatile指令重排 编译器 和处理器 常常做指令重排 源代码 编译器的优化的重排 指令并行的重排 内存系统的重排 最终执行的命令 数据依赖性 单线程 —-》》 多线程 源代码 12345678 底层 不一定是按照这个顺序 而是多线程环境 答题的顺序和 重排1 语句四存在数据的依赖性不能排到第一条 重排2 非计算机的了解即可 volatile禁止指令重排 线程的安全得到保证 a的前面加不加volatile a拿到的不是1 是 0 单线程环境不用担心指令重排 多线程 单线程编译器优化 编译器优化 指令并行重排的优化 内存系统的重排 3个重排 volatile可见性 Volatile的三大特性的讲解 JUC的包里面大规模使用到了单例模式 2.1.4 volatile的单例模式1单机版的单线程 12345678910111213141516171819202122232425262728293031323334353637383940package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/9 * @描述： */import javafx.scene.SnapshotParameters;/** 单机版的单线程* */public class SingleDemo &#123; private static SingleDemo instance=null; private SingleDemo()&#123; System.out.println(Thread.currentThread().getName()+"\t 我是构造方法Singledemo()" ); &#125; public static SingleDemo getInstance() &#123; if(instance==null) &#123; instance=new SingleDemo(); &#125; return instance; &#125; public static void main(String[] args) &#123; System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance()); System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance()); System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance()); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/9 * @描述： */import javafx.scene.SnapshotParameters;/** 单机版的单线程* */public class SingleDemo &#123; private static SingleDemo instance=null; private SingleDemo()&#123; System.out.println(Thread.currentThread().getName()+"\t 我是构造方法Singledemo()" ); &#125; public static SingleDemo getInstance() &#123; if(instance==null) &#123; instance=new SingleDemo(); &#125; return instance; &#125; public static void main(String[] args) &#123;// System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance());// System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance());// System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance());//// System.out.println();// System.out.println();// System.out.println(); for (int i = 0; i &lt;10 ; i++) &#123; new Thread(()-&gt; &#123; SingleDemo.getInstance(); &#125;,String.valueOf(i)).start(); &#125; &#125;&#125; 多线程 10个现在变成 6条 加上synchronized 解决问题 synchronized 整个代码都锁了 加上synchronized单例volatile的解析DCL（双端检测）介绍DCL的单例模式 高并发的环境下企业推崇的 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/9 * @描述： */import javafx.scene.SnapshotParameters;/** 单机版的单线程* */public class SingleDemo &#123; /* * 保证在多线程的指令不重排 * */ private static volatile SingleDemo instance=null; private SingleDemo()&#123; System.out.println(Thread.currentThread().getName()+"\t 我是构造方法Singledemo()" ); &#125; //DCL 模式 Double Check Lock 双端检索机制 //加锁前判断 //加锁判断 //在多线程的条件下 底层有指令重排 //如果没有控制好指令重排 public static SingleDemo getInstance() &#123; if (instance == null) &#123; synchronized (SingleDemo.class) &#123; if (instance == null) &#123; instance = new SingleDemo(); &#125; &#125; &#125; return instance; &#125; public static void main(String[] args) &#123;// System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance());// System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance());// System.out.println(SingleDemo.getInstance()==SingleDemo.getInstance());//// System.out.println();// System.out.println();// System.out.println(); for (int i = 0; i &lt;10 ; i++) &#123; new Thread(()-&gt; &#123; SingleDemo.getInstance(); &#125;,String.valueOf(i)).start(); &#125; &#125;&#125; 先获得 再去加 i++ 2.2 CAS1 比较并交换什么是比较和交换呢 期望值和物理内存的真实值一样 修改为更新值 期望值和物理内存的真实值不一样 重新获得真实值 CASDemo源码123456789101112131415161718192021222324252627282930package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/9 * @描述： *//** CAS 是什么？ ==》》 compare and set* 先比较后交换* 比较并交换** */import java.util.concurrent.atomic.AtomicInteger;public class CASDemo &#123; public static void main(String[] args) &#123; AtomicInteger atomicInteger=new AtomicInteger(5); //劳动成果写进主物理内存 System.out.println(atomicInteger.compareAndSet(5,2019 )+"\t current data:"+atomicInteger.get()); System.out.println(); System.out.println(atomicInteger.compareAndSet(5,1024 )+"\t current data:"+atomicInteger.get()); System.out.println(); &#125;&#125; 结果 compare and set 同 修改成功 不同 修改失败 为什么用synchronized 不用CAS CAS原理 谈谈unsafe类1.UnSafe 是CAS的核心类 由于Java 方法无法直接访问底层 ,需要通过本地(native)方法来访问,UnSafe相当于一个后面,基于该类可以直接操作特额定的内存数据.UnSafe类在于sun.misc包中,其内部方法操作可以向C的指针一样直接操作内存,因为Java中CAS操作的助兴依赖于UNSafe类的方法. 2.变量ValueOffset,便是该变量在内存中的偏移地址,因为UnSafe就是根据内存偏移地址获取数据的 3.变量value和volatile修饰,保证了多线程之间的可见性. 什么是CASCAS的全称为Compare-And-Swap ,它是一条CPU并发原语.它的功能是判断内存某个位置的值是否为预期值,如果是则更新为新的值,这个过程是原子的. CAS并发原语提现在Java语言中就是sun.miscUnSaffe类中的各个方法.调用UnSafe类中的CAS方法,JVM会帮我实现CAS汇编指令.这是一种完全依赖于硬件 功能,通过它实现了原子操作,再次强调,由于CAS是一种系统原语,原语属于操作系统用于范畴,是由若干条指令组成,用于完成某个功能的一个过程,并且原语的执行必须是连续的,在执行过程中不允许中断,也即是说CAS是一条原子指令,不会造成所谓的数据不一致的问题. ABA问题 集合内的线程不安全问题 CAS的缺点 时间开销大 有个do whie 只能保证一个共享变量的原子性 ​ 一个变量可以保证原子性，多个变量可以靠锁来保证原子性 引发出的ABA问题 2.3 原子AtomicInteger的ABA问题 谈谈ABA问题的产生CAS算法实现一个重要的前提取出内存中的某个时刻的数据并比较替换，那么在这个时间差里会导致数据的变化 线程one 在位置V取出A 线程 two在V位置取出A two，线程B进行了一些操作将数值变成了B，然后线程two又将V位置的数据编程A， 这个时候线程one 进CAS操作发现内存中仍然是A，然后one操作成功 原子引用 AtomicReference12345678910111213141516171819202122232425262728293031323334353637import lombok.AllArgsConstructor;import lombok.Getter;import lombok.Setter;import lombok.ToString;import java.util.concurrent.atomic.AtomicReference;@Setter@Getter@AllArgsConstructor@ToStringclass User &#123; String userName; int age;&#125;public class AtomicReferenceDemo &#123; public static void main(String[] args) throws InterruptedException &#123; //建立用户zs User zs = new User("zs", 22); //用户ls User ls = new User("ls", 25); //原子引用 AtomicReference&lt;User&gt; userAtomicReference=new AtomicReference&lt;&gt;(); userAtomicReference.set(zs); System.out.println(userAtomicReference.compareAndSet(zs,ls)+"\t"+userAtomicReference.get().toString()); // System.out.println(userAtomicReference.compareAndSet(zs, ls)+"\t"+userAtomicReference.get().toString()); &#125;&#125; 时间戳原子引用AtomicStampedReference中间不知道修改了多少次 修改的时候加时间 新增一种机制 修改版本号（类似时间戳） T1 100 1 T2 100 1 101 2 100 3 ABA 乐观锁 自己的值总是不会被改变 自己很自信 JUC工具包 AtomicStampedReference ABA问题的解决1234/** * An &#123;@code AtomicStampedReference&#125; maintains an object reference * along with an integer "stamp", that can be updated atomically. */ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115package com.mlz.study.read;/* * @创建人: MaLingZhao * @创建时间: 2019/10/10 * @描述： *//*开始是100到101101 到* */import org.omg.PortableInterceptor.INACTIVE;import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicReference;import java.util.concurrent.atomic.AtomicStampedReference;public class ABADemo &#123; static AtomicReference&lt;Integer&gt; atomicReference = new AtomicReference&lt;&gt;(100); static AtomicStampedReference&lt;Integer&gt; atomicStampedReference = new AtomicStampedReference&lt;&gt;(100, 1); public static void main(String[] args) &#123; System.out.println("=====以下是ABA问题的产生======================"); new Thread(() -&gt; &#123; //期望值真实值一样改成101 atomicReference.compareAndSet(100, 101); atomicReference.compareAndSet(101, 100); &#125;, "t1").start(); new Thread(() -&gt; &#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //暂停一秒钟 保证上面的1线程完成了一次ABA操作 System.out.println(atomicReference.compareAndSet(100, 2019) + "\t" + atomicReference.get()); &#125;, "t2").start(); //暂停一会进程 try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("======以下是ABA问题的解决========="); /* * * 100 101 100 * 1 2 3 * */ new Thread(() -&gt; &#123; int stamp = atomicStampedReference.getStamp(); System.out.println(Thread.currentThread().getName() + "\t第1次版本号" + stamp); //暂停1s t3线程 try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //期望值 更新值 期望的版本号 更新的版本号 atomicStampedReference.compareAndSet(100,101,atomicStampedReference.getStamp(),atomicStampedReference.getStamp()+1); System.out.println(Thread.currentThread().getName() + "\t第2次版本号" + atomicStampedReference.getStamp()); atomicStampedReference.compareAndSet(101,100,atomicStampedReference.getStamp(),atomicStampedReference.getStamp()+1); System.out.println(Thread.currentThread().getName() + "\t第3次版本号" +atomicStampedReference.getStamp()); &#125;, "t3").start(); /* * * 期望值100 2019 * * 1 想改成2 * */ new Thread(() -&gt; &#123; int stamp = atomicStampedReference.getStamp(); System.out.println(Thread.currentThread().getName() + "\t第1次版本号" + stamp); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; boolean result=atomicStampedReference.compareAndSet(100,2019,stamp,stamp+1); System.out.println(Thread.currentThread().getName() + "\t修改成功否：" +result+"\t当前最新实际版本号"+atomicStampedReference.getStamp()); System.out.println(Thread.currentThread().getName()+"当前实际最新值:"+atomicStampedReference.getReference()); &#125;,"t4").start(); &#125;&#125; 2.4 ArryList是线程不安全的，请大家编写一个 3 集合类ArrayListArrayList 前身是Vector vector 有synchronized Collections接口 Collection API的熟练运用程度 123public HashSet(int initialCapacity) &#123; map = new HashMap&lt;&gt;(initialCapacity); &#125; hashSet的底层是Map 为什么一个装1 一个装2 因为 hasSet添加的时候前面是key 后面是一个Object对象 Object对象是null的 在多线程的情况下的时候 我们使用ArrayList会出现 java.util.ConcurrentMoidificationException 这个异常 我们的解决策略有3个 1 new Vector Vector在ArrayList之前就出现了 那么ArrayList出现的意义是什么呢 Vector 保证了安全性 但是却使用了synchronized加锁，虽然保证了一定的安全性，但是并发性却下降了 2 Collections.synchronize3dList(new ArrayList) Collection 集合的接口 Collections 集合接口的一些补充 有高并发的条件下为ArrayList实现线程安全的类 写时复制 3 new CopyOnWriteArrayList() 集合类不安全之Set集合类不安全之Map2.4 Transfervalue醒脑小练习栈运行 堆存储 main方法的20复印了一份 去执行栈中的方法changeValue1 最后存在main方法中的值还是20 方法的作域和jvm的分布 基本类型传的是复印件 引用类型传递的是地址 person的值被改过了 String str=”abc”; test.changeValue3(“String…….”+str) String的特殊性 一种是字符串常量池 去池子里面检查有没有abc String特殊 String去找xxx 没有新建xxx 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/10 * @描述： */import com.mlz.study.entity.Person;import java.util.concurrent.TransferQueue;public class TestTransferValue &#123; public void changeValue1(int age) &#123; age = 30; &#125; private void changeValue2(Person str) &#123; str.setPersonName("xxx"); &#125; private void changeValue3(String str) &#123; str = "xxx"; &#125; /** * 栈运行 * 堆 存储 */ public static void main(String[] args) &#123; TestTransferValue test = new TestTransferValue(); int age = 20; test.changeValue1(age); System.out.println("age-----" + age); Person person = new Person("abc"); test.changeValue2(person); System.out.println("personName-----" + person.getPersonName()); String str = "abc"; test.changeValue3(str); System.out.println("String........." + str); &#125;&#125; 分析图片 2.5 java锁之公平锁和非公平锁公平锁/非公平锁/可重入锁/递归锁/自旋锁谈谈你的理解?请手写一个自旋锁 order policy 排序策略 公平锁 队列 先来后到 课间休息 默认非公平 加塞加上 加不上 非公平锁 加塞提问 快 定义公平锁 是指多个线程按照申请锁的顺序来获取锁类似排队打饭 先来后到非公平锁 是指在多线程获取锁的顺序并不是按照申请锁的顺序,有可能后申请的线程比先申请的线程优先获取到锁,在高并发的情况下,有可能造成优先级反转或者饥饿现象 反转 饥饿的理解 两者的区别公平锁性能下降 公平锁/非公平锁 并发包ReentrantLock的创建可以指定构造函数的boolean类型来得到公平锁或者非公平锁 默认是非公平锁 公平锁 先来后到 非公平锁 先抢先得 性能提升 题外话Java ReentrantLock而言,通过构造哈数指定该锁是否是公平锁 默认是非公平锁 非公平锁的优点在于吞吐量必公平锁大. 对于synchronized而言 也是一种非公平锁. 2.6 可重入锁（递归锁）是什么 京津冀黔 生活 12345678910111213public sync void method01()&#123;method02()&#125;public sync void method02()&#123;&#125; method01() 就是大门 但是里面的锁是不上的 默认是非公平的可重入锁 method01（）本身是同步 method2（） 又是同步 内层递归函数能获取method01的方法 ReentrantLock/synchronized就是一个典型的可重入锁知识代码说明 代码部分 synchronized 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/10 * @描述： *//* * 线程操作资源类 * *//** 同一个线程访问两个同步方法 但是他们确实访问同一把锁* 外层synchronized 内层的synchronized* */class Phone &#123; public synchronized void sendSMS() throws Exception &#123; System.out.println(Thread.currentThread().getName() + "\t invoked sendSMS"); sendEmail(); &#125; public synchronized void sendEmail() throws Exception &#123; System.out.println(Thread.currentThread().getName() + "\t #####invoked sendEmail"); &#125;&#125;/* * 外层函数获得锁之后 内层递归函数仍然能够获取该锁的代码 * * */public class ReenterLockDemo &#123; public static void main(String[] args) &#123; Phone phone = new Phone(); new Thread(() -&gt; &#123; try &#123; phone.sendSMS(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, "t1").start(); new Thread(() -&gt; &#123; try &#123; phone.sendSMS(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, "t2").start(); &#125;&#125; ReentrantLock 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/10 * @描述： *//* * 线程操作资源类 * */import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/* * 同一个线程访问两个同步方法 但是他们确实访问同一把锁 * 外层synchronized 内层的synchronized * * case two * * ReentLock就是一个典型的可重入锁 * */class Phone implements Runnable &#123; public synchronized void sendSMS() throws Exception &#123; System.out.println(Thread.currentThread().getName() + "\t invoked sendSMS"); sendEmail(); &#125; public synchronized void sendEmail() throws Exception &#123; System.out.println(Thread.currentThread().getName() + "\t #####invoked sendEmail"); &#125; Lock lock = new ReentrantLock(); @Override public void run() &#123; get(); &#125; public void get() &#123; lock.lock(); try &#123; System.out.println(Thread.currentThread().getName() + "\t invoked get()"); set(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void set() &#123; lock.lock(); lock.lock(); try &#123; System.out.println(Thread.currentThread().getName() + "\t #####invoked set()"); &#125; finally &#123; lock.unlock(); lock.unlock(); &#125; &#125;&#125;/* * 外层函数获得锁之后 内层递归函数仍然能够获取该锁的代码 * * */public class ReenterLockDemo &#123; public static void main(String[] args) &#123; Phone phone = new Phone(); new Thread(() -&gt; &#123; try &#123; phone.sendSMS(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, "t1").start(); new Thread(() -&gt; &#123; try &#123; phone.sendSMS(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, "t2").start(); /* * 只要锁时匹配的有配对就行 * 两把锁 * 3把锁 * n把锁 * */ System.out.println(); System.out.println(); System.out.println(); //暂停一会 try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; /*phone实现了Runnable接口 * */ Thread t3 = new Thread(phone); Thread t4 = new Thread(phone); t3.start(); t4.start(); &#125;&#125; 可重入锁最大的作用就是避免死锁配对 加两对 编译 2.7 自旋锁理论讲解循环替代了阻塞 久安少线程的上下文切换 循环会消耗CPU 多次回来循环调用 循环调查的方式就叫做自旋 空闲 获得锁 忙 干自己的事情 好处： 不会造成阻塞 坏处：长时间 的话会 降低系统的性能 代码验证1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/11 * @描述： *//* * 题目： 实现一个自旋锁 * 循环锁的好处： * 循环比较直到成功为止 * */import com.mlz.study.read.SingleDemo;import java.util.SortedMap;import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicInteger;import java.util.concurrent.atomic.AtomicReference;public class SpinLockDemo &#123; /* * 原子引用 * */ /* * AtomicInteger 0 * 引用 null * */ AtomicReference&lt;Thread&gt; atomicReference = new AtomicReference&lt;&gt;(); public void mylock() &#123; Thread thread = Thread.currentThread(); System.out.println(Thread.currentThread().getName() + "\t come in O(n_n)O"); while (!atomicReference.compareAndSet(null, thread)) &#123; &#125; &#125; public void myunlock() &#123; Thread thread = Thread.currentThread(); atomicReference.compareAndSet(thread, null); System.out.println(Thread.currentThread().getName() + "\t invoked myunlock()"); &#125; public static void main(String[] args) &#123; /* * 泛型 原子引用线程 * */ SpinLockDemo spinLockDemo = new SpinLockDemo(); new Thread(() -&gt; &#123; spinLockDemo.mylock(); //暂停一会线程 try &#123; TimeUnit.SECONDS.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; spinLockDemo.myunlock(); &#125;, "AA").start(); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; new Thread(() -&gt; &#123; spinLockDemo.mylock(); //暂停一会线程 try &#123; TimeUnit.SECONDS.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; spinLockDemo.myunlock(); &#125;, "BB").start(); &#125;&#125; 2.8 java锁之读写锁独占锁（写锁） 共享锁 sync —–&gt;&gt; lock —&gt;&gt; lock 上卫生间 unlock ReentrantReadWriteLock 读写分离 数据的一致性 并发性读写 demo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/11 * @描述： *//*多个线程同时读写一个资源类没有任何问题 所以为了满足并发量 读取共享资源应该可以同时进行但是如果一个线程相当于读取共享资源来，就不应该再有其他线程对该资源进行读或者写技术人员思维方式After before读写锁Lock为啥读读能共存读写能共存写写不能共存写操作 原子 + 独占中间的真个过程必须是一个完整的统一体中间不许被分割 不许被打断* *//* * 资源类 * */import java.util.HashMap;import java.util.Map;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.ReentrantLock;import java.util.concurrent.locks.ReentrantReadWriteLock;class MyCache &#123; private volatile Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); private ReentrantReadWriteLock rwLock = new ReentrantReadWriteLock();//private Lock lock=new ReentrantLock(); //Redis 读 写 清空 缓存框架 public void put(String key, Object value) &#123; rwLock.writeLock().lock(); try &#123; System.out.println(Thread.currentThread().getName() + "\t 正在写入:" + key); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; map.put(key, value); System.out.println(Thread.currentThread().getName() + "\t 写入完成:"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; rwLock.writeLock().unlock(); &#125; &#125; public void get(String key) &#123; rwLock.readLock().lock(); try &#123; System.out.println(Thread.currentThread().getName() + "\t 正在读取:"); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; Object result = map.get(key); System.out.println(Thread.currentThread().getName() + "\t 读取完成:" + result); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; rwLock.readLock().unlock(); &#125; &#125;&#125;public class ReadWriteDemo &#123; public static void main(String[] args) &#123; MyCache myCache = new MyCache(); for (int i = 1; i &lt;= 5; i++) &#123; final int tempInt = i; new Thread(() -&gt; &#123; myCache.put(tempInt + "", tempInt + ""); &#125;, String.valueOf(i)).start(); &#125; for (int i = 1; i &lt;= 5; i++) &#123; final int tempInt = i; new Thread(() -&gt; &#123; myCache.get(tempInt + ""); &#125;, String.valueOf(i)).start(); &#125; &#125;&#125; idea设置代码模块化trylock 自动生成方法设置template alt + crtl +s 读写分离 严格控制 完整独立性 读写分离代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/11 * @描述： *//*多个线程同时读写一个资源类没有任何问题 所以为了满足并发量 读取共享资源应该可以同时进行但是如果一个线程相当于读取共享资源来，就不应该再有其他线程对该资源进行读或者写技术人员思维方式After before读写锁Lock为啥读读能共存读写能共存写写不能共存写操作 原子 + 独占中间的真个过程必须是一个完整的统一体中间不许被分割 不许被打断* *//* * 资源类 * */import java.util.HashMap;import java.util.Map;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.ReentrantLock;import java.util.concurrent.locks.ReentrantReadWriteLock;class MyCache &#123; private volatile Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); private ReentrantReadWriteLock rwLock = new ReentrantReadWriteLock();//private Lock lock=new ReentrantLock(); //Redis 读 写 清空 缓存框架 public void put(String key, Object value) &#123; rwLock.writeLock().lock(); try &#123; System.out.println(Thread.currentThread().getName() + "\t 正在写入:" + key); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; map.put(key, value); System.out.println(Thread.currentThread().getName() + "\t 写入完成:"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; rwLock.writeLock().unlock(); &#125; &#125; public void get(String key) &#123; rwLock.readLock().lock(); try &#123; System.out.println(Thread.currentThread().getName() + "\t 正在读取:"); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; Object result = map.get(key); System.out.println(Thread.currentThread().getName() + "\t 读取完成:" + result); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; rwLock.readLock().unlock(); &#125; &#125;&#125;public class ReadWriteDemo &#123; public static void main(String[] args) &#123; MyCache myCache = new MyCache(); for (int i = 1; i &lt;= 5; i++) &#123; final int tempInt = i; new Thread(() -&gt; &#123; myCache.put(tempInt + "", tempInt + ""); &#125;, String.valueOf(i)).start(); &#125; for (int i = 1; i &lt;= 5; i++) &#123; final int tempInt = i; new Thread(() -&gt; &#123; myCache.get(tempInt + ""); &#125;, String.valueOf(i)).start(); &#125; &#125;&#125; 加锁和不加锁的区别 12 2.9 CountDownLatch前提任务完成之后 再去完成最后的任务 做减法 定义 让一些线程阻塞直到另外一些完成后才唤醒 CountDownLatch主要有两个方法 当一个或者多个线程调用await方法的时候，调用线程会被阻塞，其他线程调用，countDown方法 计数器减1 （调用CountDown方法的时候，线程不会被阻塞），当计数器的值变为0的时候调用await（）方法被阻塞的线程被唤醒，继续执行 demo11234567891011121314151617181920212223242526272829303132333435363738394041424344package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/12 * @描述： */import java.util.concurrent.CountDownLatch;import java.util.concurrent.TimeUnit;public class CountDownLatchDemo &#123; /* * 没有用 CountdownLatch * * */ public static void main(String[] args) throws InterruptedException &#123; CountDownLatch countDownLatch = new CountDownLatch(6); for (int i = 1; i &lt;= 6; i++) &#123; new Thread(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + "\t 上完自习,离开教室"); countDownLatch.countDown(); &#125;, String.valueOf(i)).start(); &#125; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; countDownLatch.await(); System.out.println(Thread.currentThread().getName() + "\t ****************************班长最后关门走人"); &#125;&#125; demo2一统天下 灭六国 枚举类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.mlz.study.Interview.enums;/* * @创建人: MaLingZhao * @创建时间: 2019/10/12 * @描述： */import lombok.Getter;public enum CountryEnum &#123; /* * 美剧 ONE TWO * * 代码版本的mysql数据库 * * ONE * */ ONE(1, "齐"), TWO(2, "楚"), THREE(3, "燕"), FOUR(4, "韩"), FIVE(5, "赵"), SIX(6, "魏"); @Getter private Integer retCode; @Getter private String retMessage; CountryEnum(Integer retCode, String retMessage) &#123; this.retCode = retCode; this.retMessage = retMessage; &#125; public static CountryEnum forEach_CountryEnum(int index) &#123; CountryEnum[] myArray = CountryEnum.values(); for (CountryEnum element : myArray) &#123; if (index == element.getRetCode()) &#123; return element; &#125; &#125; return null; &#125;&#125; Demo 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/12 * @描述： */import com.mlz.study.Interview.enums.CountryEnum;import java.util.concurrent.CountDownLatch;import java.util.concurrent.TimeUnit;public class CountDownLatchDemo &#123; /* * 没有用 CountdownLatch * * */ public static void main(String[] args) throws InterruptedException &#123; CountDownLatch countDownLatch = new CountDownLatch(6); for (int i = 1; i &lt;= 6; i++) &#123; new Thread(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + "\t 国被灭"); countDownLatch.countDown(); &#125;, CountryEnum.forEach_CountryEnum(i).getRetMessage()).start(); &#125; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; countDownLatch.await(); System.out.println(Thread.currentThread().getName() + "\t ****************************秦国一统天下"); &#125;&#125; 枚举类使用更加方便 1234System.out.println(CountryEnum.ONE); System.out.println(CountryEnum.ONE.getRetCode());; System.out.println(CountryEnum.ONE.getRetMessage()); CyclicBarrier集齐7颗龙珠就能召唤神龙 人到齐了才能开会 demo案例123456789101112131415161718192021222324252627282930313233343536package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/12 * @描述： */import java.util.concurrent.BrokenBarrierException;import java.util.concurrent.CyclicBarrier;public class CyclicBarrierDemo &#123; public static void main(String[] args) &#123; CyclicBarrier cyclicBarrier = new CyclicBarrier(7, () -&gt; System.out.println("**********召唤神龙")); for (int i = 1; i &lt;= 7; i++) &#123; final int tempInt = i; new Thread(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + "\t 收集到第:" + tempInt + "龙珠"); try &#123; cyclicBarrier.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; &#125;, String.valueOf(i)).start(); &#125; &#125;&#125; Semaphore信号灯 小米的秒杀接口 基础 高级 synchronized 小米秒杀 放出手机 下降99 多个线程抢多份资源 来一辆 走一辆 代替synchronized和lock 定义两个目的 共享资源的互斥使用 另一个用于并发线程数的控制 demo案例123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/12 * @描述： */import java.util.concurrent.Semaphore;import java.util.concurrent.TimeUnit;public class SemaphoreDemo &#123; public static void main(String[] args) &#123; //模拟3个停车位 Semaphore semaphore = new Semaphore(3); for (int i = 1; i &lt;= 6; i++) &#123;//模拟6辆车 new Thread(() -&gt; &#123; try &#123; semaphore.acquire(); System.out.println(Thread.currentThread().getName() + "\t抢到车位"); //暂停一会线程 try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + "\t停车3秒离开车位"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; semaphore.release(); &#125; &#125;, String.valueOf(i)).start(); &#125; &#125;&#125; 2.9 阻塞队列理论定义阻塞队列,顾名思义,首先它是一个队列,而一个阻塞队列在数据结构中所起的作用大致如图所示: 线程1往阻塞队列中添加元素二线程2从队列中移除元素当阻塞队列是空时,从队列中获取元素的操作将会被阻塞.当阻塞队列是满时,往队列中添加元素的操作将会被阻塞.同样试图往已满的阻塞队列中添加新圆度的线程同样也会被阻塞,知道其他线程从队列中移除一个或者多个元素或者全清空队列后使队列重新变得空闲起来并后续新增. 队列 有序 生产线程 消费线程 队列 空 获取元素操作被阻塞 队列满 队列添加元素操作被阻塞 卖蛋糕的问题 柜台空 xxx 柜台满 xxx 请写一个阻塞队列 为什么用在多线程领域:所谓阻塞,在某些情况下会挂起线程(即线程阻塞),一旦条件满足,被挂起的线程优惠被自动唤醒 为什么需要使用BlockingQueue 好处是我们不需要关心什么时候需要阻塞线程,什么时候需要唤醒线程,因为BlockingQueue都一手给你包办好了 在concurrent包 发布以前,在多线程环境下,我们每个程序员都必须自己去控制这些细节,尤其还要兼顾效率和线程安全,而这会给我们的程序带来不小的复杂度. 架构 BlockQueue 消息系统 底层就是这个 ArrayBlockingQueue 数组组成的有界阻塞队列 LinkedBlockingQueue 链表结构组成的有界阻塞队列 Integer.MAX_VALUE SynchronousQueue 不存储元素的阻塞队列，也即是单个元素的阻塞队列 生产一个， 消费一个 定制版的 下单开工 Deque 双端队列 阻塞队列的核心方法抛出异常 当阻塞队列满时,再往队列里面add插入元素会抛IllegalStateException: Queue full当阻塞队列空时,再往队列Remove元素时候回抛出NoSuchElementException特殊值 插入方法,成功返回true 失败返回false移除方法,成功返回元素,队列里面没有就返回null一直阻塞 当阻塞队列满时,生产者继续往队列里面put元素,队列会一直阻塞直到put数据or响应中断退出当阻塞队列空时,消费者试图从队列take元素,队列会一直阻塞消费者线程直到队列可用.超时退出 当阻塞队列满时,队列会阻塞生产者线程一定时间,超过后限时后生产者线程就会退出 热身123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/12 * @描述： */import java.util.ArrayList;import java.util.List;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;/** 队列* 阻塞队列* 2.1 阻塞队列有没有好的一面** 2.2 不得不阻塞，你如何管理** 银行排队* */public class BlockingQueueDemo &#123; public static void main(String[] args) &#123; //你用过什么List //你除了ArrayList和LinkedList 你还用过什么 //CopyOnWriteArrayList //List list=new ArrayList(); /* * * 队列只有3个位置 * */ BlockingQueue&lt;String&gt; blockingQueue=new ArrayBlockingQueue&lt;&gt;(3); System.out.println(blockingQueue.add("a")); System.out.println(blockingQueue.add("b")); System.out.println(blockingQueue.add("c")); /* * 队首元素 * */ System.out.println(blockingQueue.element()); /* * 不合法的队列 队列满 添加元素 * */ //System.out.println(blockingQueue.add("d")); System.out.println(blockingQueue.remove()); System.out.println(blockingQueue.remove()); System.out.println(blockingQueue.remove()); /* * 空的时候报异常 * */ // System.out.println(blockingQueue.remove()); System.out.println( Integer.MAX_VALUE); &#125;&#125; api的布尔返回值12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/12 * @描述： */import java.util.ArrayList;import java.util.List;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;/* * 队列 * 阻塞队列 * 2.1 阻塞队列有没有好的一面 * * 2.2 不得不阻塞，你如何管理 * * 银行排队 * */public class BlockingQueueDemo &#123; public static void main(String[] args) &#123; //你用过什么List //你除了ArrayList和LinkedList 你还用过什么 //CopyOnWriteArrayList //List list=new ArrayList(); BlockingQueue&lt;String&gt; blockingQueue = new ArrayBlockingQueue&lt;&gt;(3); System.out.println(blockingQueue.offer("a")); System.out.println(blockingQueue.offer("b")); System.out.println(blockingQueue.offer("c")); System.out.println(blockingQueue.offer("x" )); System.out.println(blockingQueue.peek()); System.out.println(blockingQueue.poll()); System.out.println(blockingQueue.poll()); System.out.println(blockingQueue.poll()); System.out.println(blockingQueue.poll()); &#125;&#125; 老子堵着你1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/12 * @描述： */import java.util.ArrayList;import java.util.List;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;/* * 队列 * 阻塞队列 * 2.1 阻塞队列有没有好的一面 * * 2.2 不得不阻塞，你如何管理 * * 银行排队 * */public class BlockingQueueDemo &#123; public static void main(String[] args) throws InterruptedException &#123; //你用过什么List //你除了ArrayList和LinkedList 你还用过什么 //CopyOnWriteArrayList //List list=new ArrayList(); BlockingQueue&lt;String&gt; blockingQueue = new ArrayBlockingQueue&lt;&gt;(3); blockingQueue.put("a"); blockingQueue.put("a"); blockingQueue.put("a"); System.out.println("===================="); //blockingQueue.put("a"); /* * 堵着 * */ blockingQueue.take(); blockingQueue.take(); blockingQueue.take(); blockingQueue.take(); &#125;&#125; 心情好 放你走吧123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/12 * @描述： */import java.util.ArrayList;import java.util.List;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;import java.util.concurrent.TimeUnit;/* * 队列 * 阻塞队列 * 2.1 阻塞队列有没有好的一面 * * 2.2 不得不阻塞，你如何管理 * * 银行排队 * */public class BlockingQueueDemo &#123; public static void main(String[] args) throws InterruptedException &#123; //你用过什么List //你除了ArrayList和LinkedList 你还用过什么 //CopyOnWriteArrayList //List list=new ArrayList(); BlockingQueue&lt;String&gt; blockingQueue = new ArrayBlockingQueue&lt;&gt;(3); System.out.println(blockingQueue.offer("a",2L, TimeUnit.SECONDS)); System.out.println(blockingQueue.offer("a",2L, TimeUnit.SECONDS)); System.out.println(blockingQueue.offer("a",2L, TimeUnit.SECONDS)); System.out.println(blockingQueue.offer("a",2L, TimeUnit.SECONDS)); &#125;&#125; SynchronousBlcokQueue专属定制版 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package com.mlz.study.Interview;/* * @创建人: MaLingZhao * @创建时间: 2019/10/12 * @描述： */import java.util.concurrent.BlockingQueue;import java.util.concurrent.SynchronousQueue;import java.util.concurrent.TimeUnit;public class SynchronousQueueDemo &#123; public static void main(String[] args) &#123; BlockingQueue&lt;Integer&gt; blockingQueue=new SynchronousQueue&lt;&gt;(); new Thread(()-&gt;&#123; try &#123; System.out.println(Thread.currentThread().getName() + "\t put 1"); blockingQueue.put(1); System.out.println(Thread.currentThread().getName() + "\t put 2"); blockingQueue.put(2); System.out.println(Thread.currentThread().getName() + "\t put 3"); blockingQueue.put(3); &#125;catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;,"AAA").start(); new Thread(()-&gt;&#123; try &#123; try &#123; TimeUnit.SECONDS.sleep(5); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName()+"\t"+blockingQueue.take()); try &#123; TimeUnit.SECONDS.sleep(5); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName()+"\t"+blockingQueue.take()); try &#123; TimeUnit.SECONDS.sleep(5); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName()+"\t"+blockingQueue.take()); &#125;catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;,"BBB").start(); &#125;&#125;]]></content>
      <tags>
        <tag>面试</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实时项目]]></title>
    <url>%2Fblog4%2F2019%2F08%2F12%2FRealTimeProjectsparkES%2F</url>
    <content type="text"><![CDATA[12 第一章1 项目的架构1.1 需求的特点离线需求 一般是根据前一日的数据生成报表等数据，虽然统计指标、报表繁多，但是对时效性不敏感。 实时需求 主要侧重于对当日数据的实时监控，通常业务逻辑相对离线需求简单一下，统计指标也少一些，但是更注重数据的时效性，以及用户的交互性。 实时架构 离线架构 2 框架的搭建2.1 框架的架构 2.2 框架的搭建代码部分呢 1建立父工程gmall1205-dw 引入pom.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100 &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.10.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt;&lt;properties&gt; &lt;spark.version&gt;2.1.1&lt;/spark.version&gt; &lt;scala.version&gt;2.11.8&lt;/scala.version&gt; &lt;log4j.version&gt;1.2.17&lt;/log4j.version&gt; &lt;slf4j.version&gt;1.7.22&lt;/slf4j.version&gt; &lt;fastjson.version&gt;1.2.47&lt;/fastjson.version&gt; &lt;httpclient.version&gt;4.5.5&lt;/httpclient.version&gt; &lt;httpmime.version&gt;4.3.6&lt;/httpmime.version&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;!--此处放日志包，所有项目都要引用--&gt; &lt;!-- 所有子项目的日志框架 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;jcl-over-slf4j&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 具体的日志实现 --&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;version&gt;$&#123;httpclient.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpmime&lt;/artifactId&gt; &lt;version&gt;$&#123;httpmime.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;$&#123;fastjson.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-hive_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 2 建立子模块 公共模块gmall1205-common pom.xml文件 12345678910111213141516&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpmime&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 建立子模块 模拟数据 gmall1205-mock 加入公共模块的依赖 gmall1205-common pom 4.1 建立类RandomDate 包名 com.atguigu.gmall1205.mock.util 这是工具类 123456789101112131415161718192021222324import java.util.Date;import java.util.Random;public class RandomDate &#123; Long logDateTime =0L;// int maxTimeStep=0 ; public RandomDate (Date startDate , Date endDate,int num) &#123; Long avgStepTime = (endDate.getTime()- startDate.getTime())/num; this.maxTimeStep=avgStepTime.intValue()*2; this.logDateTime=startDate.getTime(); &#125; public Date getRandomDate() &#123; int timeStep = new Random().nextInt(maxTimeStep); logDateTime = logDateTime+timeStep; return new Date( logDateTime); &#125;&#125; 4.2 RanOpt 1234567891011121314151617public class RanOpt&lt;T&gt;&#123; T value ; int weight; public RanOpt ( T value, int weight )&#123; this.value=value ; this.weight=weight; &#125; public T getValue() &#123; return value; &#125; public int getWeight() &#123; return weight; &#125; &#125; 4.3RandomOptionGroup 12345678910111213141516171819202122232425262728293031323334353637383940import java.util.ArrayList;import java.util.List;import java.util.Random;public class RandomOptionGroup&lt;T&gt; &#123; int totalWeight=0; List&lt;RanOpt&gt; optList=new ArrayList(); public RandomOptionGroup(RanOpt&lt;T&gt;... opts) &#123; for (RanOpt opt : opts) &#123; totalWeight+=opt.getWeight(); for (int i = 0; i &lt;opt.getWeight() ; i++) &#123; optList.add(opt); &#125; &#125; &#125; public RanOpt&lt;T&gt; getRandomOpt() &#123; int i = new Random().nextInt(totalWeight); return optList.get(i); &#125; public static void main(String[] args) &#123; RanOpt[] opts= &#123;new RanOpt("zhang3",20),new RanOpt("li4",30),new RanOpt("wang5",50)&#125;; RandomOptionGroup randomOptionGroup = new RandomOptionGroup(opts); for (int i = 0; i &lt;10 ; i++) &#123; System.out.println(randomOptionGroup.getRandomOpt().getValue()); &#125; &#125; &#125; *4.4 RandomNum * 1234567public class RandomNum &#123; public static final int getRandInt(int fromNum,int toNum)&#123; return fromNum+ new Random().nextInt(toNum-fromNum+1); &#125;&#125; 1 5新建日志发送工具 发送到采集系统的web端口 LogUploader 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import org.apache.http.HttpEntity;import org.apache.http.NameValuePair;import org.apache.http.client.ClientProtocolException;import org.apache.http.client.config.RequestConfig;import org.apache.http.client.entity.UrlEncodedFormEntity;import org.apache.http.client.methods.CloseableHttpResponse;import org.apache.http.client.methods.HttpPost;import org.apache.http.entity.mime.MultipartEntityBuilder;import org.apache.http.entity.mime.content.FileBody;import org.apache.http.impl.client.CloseableHttpClient;import org.apache.http.impl.client.HttpClients;import org.apache.http.message.BasicNameValuePair;import org.apache.http.util.EntityUtils;import java.io.File;import java.io.IOException;import java.io.OutputStream;import java.io.UnsupportedEncodingException;import java.net.HttpURLConnection;import java.net.URL;import java.util.ArrayList;import java.util.List;public class LogUploader &#123; public static void sendLogStream(String log)&#123; try&#123; //不同的日志类型对应不同的URL URL url =new URL("http://logserver/log"); HttpURLConnection conn = (HttpURLConnection) url.openConnection(); //设置请求方式为post conn.setRequestMethod("POST"); //时间头用来供server进行时钟校对的 conn.setRequestProperty("clientTime",System.currentTimeMillis() + ""); //允许上传数据 conn.setDoOutput(true); //设置请求的头信息,设置内容类型为JSON conn.setRequestProperty("Content-Type", "application/x-www-form-urlencoded"); System.out.println("upload" + log); //输出流 OutputStream out = conn.getOutputStream(); out.write(("log="+log).getBytes()); out.flush(); out.close(); int code = conn.getResponseCode(); System.out.println(code); &#125; catch (Exception e)&#123; e.printStackTrace(); &#125; &#125;&#125; http://logserver/log?{json} 6 日志生成类 com.atguigu.gmall1205.mock 包名 startup和wvwnt两种事件日志 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175import com.alibaba.fastjson.JSON;import com.alibaba.fastjson.JSONObject;import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Date;import java.util.Random;public class JsonMocker &#123; int startupNum=100000; int eventNum=200000 ; RandomDate logDateUtil= null; RanOpt[] osOpts= &#123;new RanOpt("ios",3),new RanOpt("andriod",7) &#125;; RandomOptionGroup&lt;String&gt; osOptionGroup= new RandomOptionGroup(osOpts); Date startTime= null; Date endTime= null; RanOpt[] areaOpts= &#123;new RanOpt("beijing",10), new RanOpt("shanghai",10),new RanOpt("guangdong",20),new RanOpt("hebei",5), new RanOpt("heilongjiang",5),new RanOpt("shandong",5),new RanOpt("tianjin",5), new RanOpt("shan3xi",5),new RanOpt("shan1xi",5),new RanOpt("sichuan",5) &#125;; RandomOptionGroup&lt;String&gt; areaOptionGroup= new RandomOptionGroup(areaOpts); String appId="gmall1205"; RanOpt[] vsOpts= &#123;new RanOpt("1.2.0",50),new RanOpt("1.1.2",15), new RanOpt("1.1.3",30), new RanOpt("1.1.1",5) &#125;; RandomOptionGroup&lt;String&gt; vsOptionGroup= new RandomOptionGroup(vsOpts); RanOpt[] eventOpts= &#123;new RanOpt("addFavor",10),new RanOpt("addComment",30), new RanOpt("addCart",20), new RanOpt("clickItem",40) &#125;; RandomOptionGroup&lt;String&gt; eventOptionGroup= new RandomOptionGroup(eventOpts); RanOpt[] channelOpts= &#123;new RanOpt("xiaomi",10),new RanOpt("huawei",20), new RanOpt("wandoujia",30), new RanOpt("360",20), new RanOpt("tencent",20) , new RanOpt("baidu",10), new RanOpt("website",10) &#125;; RandomOptionGroup&lt;String&gt; channelOptionGroup= new RandomOptionGroup(channelOpts); RanOpt[] quitOpts= &#123; new RanOpt(true,20),new RanOpt(false,80)&#125;; RandomOptionGroup&lt;Boolean&gt; isQuitGroup= new RandomOptionGroup(quitOpts); public JsonMocker( ) &#123; &#125; public JsonMocker(String startTimeString ,String endTimeString,int startupNum,int eventNum) &#123; try &#123; startTime= new SimpleDateFormat("yyyy-MM-dd").parse(startTimeString); endTime= new SimpleDateFormat("yyyy-MM-dd").parse(endTimeString); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; logDateUtil= new RandomDate(startTime,endTime,startupNum+eventNum); &#125; String initEventLog(String startLogJson)&#123; /*`type` string COMMENT '日志类型', `mid` string COMMENT '设备唯一 表示', `uid` string COMMENT '用户标识', `os` string COMMENT '操作系统', `appid` string COMMENT '应用id', `area` string COMMENT '地区' , `evid` string COMMENT '事件id', `pgid` string COMMENT '当前页', `npgid` string COMMENT '跳转页', `itemid` string COMMENT '商品编号', `ts` bigint COMMENT '时间',*/ JSONObject startLog = JSON.parseObject(startLogJson); String mid= startLog.getString("mid"); String uid= startLog.getString("uid"); String os= startLog.getString("os"); String appid=this.appId; String area=startLog.getString("area"); String evid = eventOptionGroup.getRandomOpt().getValue(); int pgid = new Random().nextInt(50)+1; int npgid = new Random().nextInt(50)+1; int itemid = new Random().nextInt(50); // long ts= logDateUtil.getRandomDate().getTime(); JSONObject jsonObject = new JSONObject(); jsonObject.put("type","event"); jsonObject.put("mid",mid); jsonObject.put("uid",uid); jsonObject.put("os",os); jsonObject.put("appid",appid); jsonObject.put("area",area); jsonObject.put("evid",evid); jsonObject.put("pgid",pgid); jsonObject.put("npgid",npgid); jsonObject.put("itemid",itemid); return jsonObject.toJSONString(); &#125; String initStartupLog( )&#123; /*`type` string COMMENT '日志类型', `mid` string COMMENT '设备唯一标识', `uid` string COMMENT '用户标识', `os` string COMMENT '操作系统', , `appId` string COMMENT '应用id', , `vs` string COMMENT '版本号', `ts` bigint COMMENT '启动时间', , `area` string COMMENT '城市' */ String mid= "mid_"+ RandomNum.getRandInt(1,500); String uid=""+ RandomNum.getRandInt(1,500); String os=osOptionGroup.getRandomOpt().getValue(); String appid=this.appId; String area=areaOptionGroup.getRandomOpt().getValue(); String vs = vsOptionGroup.getRandomOpt().getValue(); //long ts= logDateUtil.getRandomDate().getTime(); String ch=os.equals("ios")?"appstore": channelOptionGroup.getRandomOpt().getValue(); JSONObject jsonObject = new JSONObject(); jsonObject.put("type","startup"); jsonObject.put("mid",mid); jsonObject.put("uid",uid); jsonObject.put("os",os); jsonObject.put("appid",appid); jsonObject.put("area",area); jsonObject.put("ch",ch); jsonObject.put("vs",vs); return jsonObject.toJSONString(); &#125;public static void genLog() &#123; JsonMocker jsonMocker = new JsonMocker(); jsonMocker.startupNum = 1000000; for (int i = 0; i &lt; jsonMocker.startupNum; i++) &#123; String startupLog = jsonMocker.initStartupLog(); jsonMocker.sendLog(startupLog); while (!jsonMocker.isQuitGroup.getRandomOpt().getValue()) &#123; String eventLog = jsonMocker.initEventLog(startupLog); jsonMocker.sendLog(eventLog); &#125; try &#123; Thread.sleep(20); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; public void sendLog(String log) &#123; LogUploader.sendLogStream(log); &#125; public static void main(String[] args) &#123; genLog(); &#125;&#125; 2.3开始接收日志怎么接受日志呢 用springBoot 2.3.1 springBoot是干什么的简介 Spring Boot 是由 Pivotal 团队提供的全新框架，其设计目的是用来简化新 Spring 应用的初始搭建以及开发过程。 该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。 有了springboot 我们就可以…内嵌Tomcat,不再需要外部的Tomcat不再需要那些千篇一律，繁琐的xml文件。 更方便的和各个第三方工具（mysql,redis,elasticsearch,dubbo等等整合），而只要维护一个配置文件即可。 2.3.2 springBoot和ssm的关系​ springboot整合了springmvc ，spring等核心功能。也就是说本质上实现功能的还是原有的spring ,springmvc的包，但是springboot单独包装了一层，这样用户就不必直接对springmvc， spring等，在xml中配置。 2.3.3 没有xml我们打哪里配置1 springboot实际上就是把以前需要用户手工配置的部分，全部作为默认项。除非用户需要额外更改不然不用配置。这就是所谓的：“约定大于配置” 2 如果需要特别配置的时候，去修改application.properties 2.3.4 快速上手快速搭建模块 整合第三方 框架的版本里面可以修改 springboot的配置 logger 两个都想继承 多了一个springBoot的parent 怎么办 pom.xml 修改 springboot的版本将这个模块的依赖剪切到父模块中 123456&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.10.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; 同时让springboot继承父模块 在logger的xml中添加 12345&lt;parent&gt; &lt;artifactId&gt;gmall1205-parent&lt;/artifactId&gt; &lt;groupId&gt;com.atguigu.gmall1205&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; 3 日志保存配置logserver 反向代理 application.properties 代理本机 80 两款软件可以使用 修改 post方式改成get请求 能通 12345678910111213141516171819202122232425262728293031323334package com.atguigu.gmall1205.logger.controller;/* * @创建人: MaLingZhao * @创建时间: 2019/9/25 * @描述： */import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.*;@RestController//@Controller//RestConrtoller=Controller+ ResponseBondy 不用每个方法前加ResponseBiondy了public class LoggerController &#123; //下面的两个配置是完全等价的 //@RequestMapping(value="/log",method= RequestMethod.POST) //@PostMapping("/log") //@ResponseBody @RequestMapping("/log") public String dolog(@RequestParam("log") String logJson) &#123; //logJson直接注入到变量 //字符串当做返回的模板的名字的jsp 加上返回ResponseBondy 返回的是字符串 return logJson; &#125;&#125; 修改logger 打通关系 123456789101112131415161718192021222324252627282930313233343536package com.atguigu.gmall1205.logger.controller;/* * @创建人: MaLingZhao * @创建时间: 2019/9/25 * @描述： */import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.*;@RestController//@Controller//RestConrtoller=Controller+ ResponseBondy 不用每个方法前加ResponseBiondy了public class LoggerController &#123; //下面的两个配置是完全等价的 //@RequestMapping(value="/log",method= RequestMethod.POST) @PostMapping("/log") //@ResponseBody //@RequestMapping("/log") public String dolog(@RequestParam("log") String logJson) &#123; //logJson直接注入到变量 //字符串当做返回的模板的名字的jsp 加上返回ResponseBondy 返回的是字符串 System.out.println(logJson); return logJson; &#125;&#125; 两个模块启动 pom.xml 12345678910111213141516 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-log4j&lt;/artifactId&gt; &lt;version&gt;1.3.8.RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 不使用spring-boot-starter-logging 使用spring-boot-starter-log4j exclusions 排斥在外的 添加log.properties private static final org.slf4j.Logger logger = LoggerFactory.getLogger(LogJsonController.class) ; logger.info 1234567891011121314151617181920212223242526# 自定义格式log4j.appender.atguigu.MyConsole=org.apache.log4j.ConsoleAppender#log4j.appender.atguigu.MyConsole.target=System.err# 这里配置的是输出错误的内容 我们需要配置System.outlog4j.appender.atguigu.MyConsole.target=System.outlog4j.appender.atguigu.MyConsole.layout=org.apache.log4j.PatternLayout log4j.appender.atguigu.MyConsole.layout.ConversionPattern=%d&#123;yyyy-MM-dd HH:mm:ss&#125; %10p (%c:%M) - %m%n log4j.appender.atguigu.File=org.apache.log4j.DailyRollingFileAppender# 具体输出到那个文件里面log4j.appender.atguigu.File.file=d:/applog/gmall1205/log/app.log# 扩展名# 生成的新的日志 重新起名字 app.log_2018_5xxxxxlog4j.appender.atguigu.File.DatePattern=&apos;.&apos;yyyy-MM-ddlog4j.appender.atguigu.File.layout=org.apache.log4j.PatternLayoutlog4j.appender.atguigu.File.layout.ConversionPattern=%m%n#最后的类追加 输出的类log4j.logger.com.atguigu.gmall1205.logger.controller.LoggerController=info,atguigu.File,atguigu.MyConsole#trace debug info warn error fatal 毁灭性的#中间四个常用# error# 级别越低 输出的内容越多 级别 trace debug info warn error fatal 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.atguigu.gmall1205.logger.controller;/* * @创建人: MaLingZhao * @创建时间: 2019/9/25 * @描述： */import com.alibaba.fastjson.JSON;import com.alibaba.fastjson.JSONObject;import com.fasterxml.jackson.databind.util.JSONPObject;import org.slf4j.LoggerFactory;import org.springframework.web.bind.annotation.*;@RestController//@Controller//RestConrtoller=Controller+ ResponseBondy 不用每个方法前加ResponseBiondy了public class LoggerController &#123; private static final org.slf4j.Logger logger = LoggerFactory.getLogger(LoggerController.class) ; //下面的两个配置是完全等价的 //@RequestMapping(value="/log",method= RequestMethod.POST) @PostMapping("/log") //@ResponseBody //@RequestMapping("/log") public String dolog(@RequestParam("log") String logJson) &#123; //logJson直接注入到变量 //字符串当做返回的模板的名字的jsp 加上返回ResponseBondy 返回的是字符串 /* 在什么地方封装时间戳 这样的话时间会更加的准确 在收集 的时候 给 从日志到springBoot */ // 补时间戳 //fastjson转字符串 JSONObject jsonpObject =JSON.parseObject(logJson); jsonpObject.put("ts",System.currentTimeMillis()); //洛盘到logfile log4j logger.info(jsonpObject.toJSONString()); //发送kafka System.out.println(logJson); return logJson; &#125;&#125; 完成日志的输出洛盘 到目录查看日志文件 3 日志服务器集群3.1 发送到kafkakafka已经进行了引入 1@Autowired KafkaTemplate&lt;String,String&gt; kafkaTemplate; kafkTemplate.sen() 发送 到什么地方 配置application.properties 123456789101112#包含所有第三方的配置server.port=80# 配置kafka集群spring.kafka.bootstrap-servers= hadoop102:9092,hadoop103:9092,hadoop104:9092#配置编码格式# 指定消息key和消息体的编解码方式spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializerspring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer 在hosts的方案中最好选择第二种 在common做一个常量类 com.atguigu.gmall1205.common.constant包 12 12345678910package com.atguigu.gmall.dw.constant;public class GmallConstants &#123; public static final String KAFKA_TOPIC_STARTUP="GMALL_STARTUP"; public static final String KAFKA_TOPIC_EVENT="GMALL_EVENT"; &#125; loggercontroller的修改 12 @RestController // Controller+Responsebodypublic class LoggerController { 1234567@AutowiredKafkaTemplate&lt;String,String&gt; kafkaTemplate;private static final org.slf4j.Logger logger = LoggerFactory.getLogger(LoggerController.class) ;//@RequestMapping(value = &quot;/log&quot;,method = RequestMethod.POST) =&gt;@PostMapping(&quot;/log&quot;)public String dolog(@RequestParam(&quot;log&quot;) String logJson)&#123; 1234567891011121314 // 补时间戳 JSONObject jsonObject = JSON.parseObject(logJson); jsonObject.put("ts",System.currentTimeMillis()); // 落盘到logfile log4j logger.info(jsonObject.toJSONString()); // 发送kafka if("startup".equals(jsonObject.getString("type")) )&#123; kafkaTemplate.send(GmallConstant.KAFKA_TOPIC_STARTUP,jsonObject.toJSONString()); &#125;else&#123; kafkaTemplate.send(GmallConstant.KAFKA_TOPIC_EVENT,jsonObject.toJSONString()); &#125; return "success";&#125; } 3.2 打jar包1 添加logger install 注意安装的父项目 3台机器依次执行 1569413890196](RealTimeProjectsparkES\1569413890196.png) 对应的log4j 上传jar包 java -jar 命令出现异常 非root用户只能使用1024以上的端口号 端口问题 改变端口 修改UpLoader 上传日志 产生日志 后台启动 3.2.3 日志的生成的脚本12 12345678910111213141516171819202122232425#!/bin/bashJAVA_BIN=/opt/module/jdk1.8.0_144/bin/javaPROJECT=gmall1205APPNAME=gmall1205-logger-0.0.1-SNAPSHOT.jarSERVER_PORT=8080 case $1 in "start") &#123; for i in hadoop102 hadoop103 hadoop104 do echo "========启动日志服务: $i===============" ssh $i "$JAVA_BIN -Xms32m -Xmx64m -jar /applog/$PROJECT/$APPNAME --server.port=$SERVER_PORT &gt;/dev/null 2&gt;&amp;1 &amp;" done &#125;;; "stop") &#123; for i in hadoop102 hadoop103 hadoop104 do echo "========关闭日志服务: $i===============" ssh $i "ps -ef|grep $APPNAME |grep -v grep|awk '&#123;print \$2&#125;'|xargs kill" &gt;/dev/null 2&gt;&amp;1 done &#125;;; esac source /etc/profile 写全路径 添加执行权限 4 nginx4.1 nginx的定义定义： Nginx (“engine x”) 是一个高性能的HTTP和反向代理服务器,特点是占有内存少，并发能力强，事实上nginx的并发能力确实在同类型的网页服务器中表现较好，中国大陆使用nginx网站用户有：百度、京东、新浪、网易、腾讯、淘宝等。 4.2 nginx与tomcat的关系1除了tomcat以外，apache,nginx,jboss,jetty等都是http服务器。 但是nginx和apache只支持静态页面和CGI协议的动态语言，比如perl、php等，但是nginx不支持java。Java程序只能通过与tomcat配合完成。 1nginx与tomcat 配合，为tomcat集群提供反向代理服务、负载均衡等服务 nginx处理请求 替代不了tomcat 处理不了java php nginx可以处理+ 4.3 nginx的三大功能4.3.1 反向代理反向代理什么是反向代理？先看什么是正向代理 再看什么是反向代理 ​ 4.3.2 负载均衡4.3.3 动静分离 4.4 安装nginx4.4.1 安装依赖1sudo yum -y install openssl openssl-devel pcre pcre-devel zlib zlib-devel gcc gcc-c++ 4.4.2 安装包解压缩nginx-xx.tar.gz包。进入解压缩目录，执行./configure –prefix=/home/atguigu/nginx make &amp;&amp; make install 路径在 赋予权限 nginx占用80端口，默认情况下非root用户不允许使用1024以下端口 执行这个命令 sudo setcap cap_net_bind_service=+eip /home/atguigu/nginx/sbin/nginx 如果报错的话 ln -s /usr/local/lib/libpcre.so.1 /lib64 修改 conf/nginx.conf文件 负载均衡策略 使用80端口 123456789101112131415161718192021http&#123; .......... upstream logserver&#123; server hadoop102:8080 weight=1; server hadoop103:8080 weight=1; server hadoop104:8080 weight=1; &#125; server &#123; listen 80; server_name logserver; location / &#123; root html; index index.html index.htm; proxy_pass http://logserver; proxy_connect_timeout 10; &#125; ..........&#125; nginx**的命令**启动 启动命令: 在/usr/local/nginx/sbin目录下执行 ./nginx 关闭 关闭命令: 在/usr/local/nginx/sbin目录下执行 ./nginx -s stop 重新加载 重新加载命令: 在/usr/local/nginx/sbin目录下执行 ./nginx -s reload nginx的地址是哪里 三台服务器处理数据 如何体现 4.3 启动nginx进行测试windows发送模拟日志 nginx负责路由 日志服务复杂接收 更新集群启动脚本logger-cluster.sh 123456789101112131415161718192021222324252627282930#!/bin/bashJAVA_BIN=/opt/module/jdk1.8.0_144/bin/javaPROJECT=gmall1205APPNAME=gmall1205-logger-0.0.1-SNAPSHOT.jarSERVER_PORT=8080 case $1 in "start") &#123; for i in hadoop102 hadoop103 hadoop104 do echo "========: $i===============" ssh $i "$JAVA_BIN -Xms32m -Xmx64m -jar /applog/$PROJECT/$APPNAME --server.port=$SERVER_PORT &gt;/dev/null 2&gt;&amp;1 &amp;" done echo "========NGINX===============" /home/atguigu/nginx/sbin/nginx &#125;;; "stop") &#123; echo "======== NGINX===============" /home/atguigu/nginx/sbin/nginx -s stop for i in hadoop102 hadoop103 hadoop104 do echo "========: $i===============" ssh $i "ps -ef|grep $APPNAME |grep -v grep|awk '&#123;print \$2&#125;'|xargs kill" &gt;/dev/null 2&gt;&amp;1 done &#125;;; esac source /etc/profile cat /etc/profiel &gt;~/.bashrc 全路径 12345/bin/kafka-console-consumer.sh --bootstrap-server hadoop02:9092,hadoop103:9092,hadoop104:9092 --topic GMALL_STARTUP --from-beginning #启动log-cluster.sh#最后tail -20f 接收文件的目录 端口号访问hadoop102/也行 或者添加主机的映射 nginx的使用指南 对应的host文件的配置 到了这 第二章 日活DAU1 搭建实时处理模块1.1 思路1 消费kafka 2 过滤当日已经计入的日活设备 3 把每次新增的当日活动信息保存到ES 4 ES查询数据 发布成数据接口 洛盘文件 kafka sparkStreaming 消费kafka 新建项目 pom.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.atguigu.gmall1205&lt;/groupId&gt; &lt;artifactId&gt;gmall1205-common&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.10.2.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.searchbox&lt;/groupId&gt; &lt;artifactId&gt;jest&lt;/artifactId&gt; &lt;version&gt;5.3.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.java.dev.jna&lt;/groupId&gt; &lt;artifactId&gt;jna&lt;/artifactId&gt; &lt;version&gt;4.5.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.codehaus.janino&lt;/groupId&gt; &lt;artifactId&gt;commons-compiler&lt;/artifactId&gt; &lt;version&gt;2.7.8&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; spark redis 和 ES DauApp 1.2 工具类 1.2.1 MyKafkaUtil1234567891011121314151617181920212223242526272829303132333435363738import org.apache.kafka.common.serialization.StringDeserializerimport java.util.Propertiesimport org.apache.kafka.clients.consumer.ConsumerRecordimport org.apache.spark.streaming.StreamingContextimport org.apache.spark.streaming.dstream.InputDStreamimport org.apache.spark.streaming.kafka010.&#123;ConsumerStrategies, KafkaUtils, LocationStrategies&#125;object MyKafkaUtil &#123; private val properties: Properties = PropertiesUtil.load("config.properties") val broker_list = properties.getProperty("kafka.broker.list") // kafka消费者配置 val kafkaParam = Map( "bootstrap.servers" -&gt; broker_list,//用于初始化链接到集群的地址 "key.deserializer" -&gt; classOf[StringDeserializer], "value.deserializer" -&gt; classOf[StringDeserializer], //用于标识这个消费者属于哪个消费团体 "group.id" -&gt; "gmall_consumer_group", //如果没有初始化偏移量或者当前的偏移量不存在任何服务器上，可以使用这个配置属性 //可以使用这个配置，latest自动重置偏移量为最新的偏移量 "auto.offset.reset" -&gt; "latest", //如果是true，则这个消费者的偏移量会在后台自动提交,但是kafka宕机容易丢失数据 //如果是false，会需要手动维护kafka偏移量 "enable.auto.commit" -&gt; (true: java.lang.Boolean) ) // 创建DStream，返回接收到的输入数据 // LocationStrategies：根据给定的主题和集群地址创建consumer // LocationStrategies.PreferConsistent：持续的在所有Executor之间分配分区 // ConsumerStrategies：选择如何在Driver和Executor上创建和配置Kafka Consumer // ConsumerStrategies.Subscribe：订阅一系列主题 def getKafkaStream(topic: String,ssc:StreamingContext): InputDStream[ConsumerRecord[String,String]]=&#123; val dStream = KafkaUtils.createDirectStream[String,String](ssc, LocationStrategies.PreferConsistent,ConsumerStrategies.Subscribe[String,String](Array(topic),kafkaParam)) dStream &#125;&#125; 1.2.2 PropertiesUtil1234567891011121314151617181920212223242526272829303132333435363738import org.apache.kafka.common.serialization.StringDeserializerimport java.util.Propertiesimport org.apache.kafka.clients.consumer.ConsumerRecordimport org.apache.spark.streaming.StreamingContextimport org.apache.spark.streaming.dstream.InputDStreamimport org.apache.spark.streaming.kafka010.&#123;ConsumerStrategies, KafkaUtils, LocationStrategies&#125;object MyKafkaUtil &#123; private val properties: Properties = PropertiesUtil.load("config.properties") val broker_list = properties.getProperty("kafka.broker.list") // kafka消费者配置 val kafkaParam = Map( "bootstrap.servers" -&gt; broker_list,//用于初始化链接到集群的地址 "key.deserializer" -&gt; classOf[StringDeserializer], "value.deserializer" -&gt; classOf[StringDeserializer], //用于标识这个消费者属于哪个消费团体 "group.id" -&gt; "gmall_consumer_group", //如果没有初始化偏移量或者当前的偏移量不存在任何服务器上，可以使用这个配置属性 //可以使用这个配置，latest自动重置偏移量为最新的偏移量 "auto.offset.reset" -&gt; "latest", //如果是true，则这个消费者的偏移量会在后台自动提交,但是kafka宕机容易丢失数据 //如果是false，会需要手动维护kafka偏移量 "enable.auto.commit" -&gt; (true: java.lang.Boolean) ) // 创建DStream，返回接收到的输入数据 // LocationStrategies：根据给定的主题和集群地址创建consumer // LocationStrategies.PreferConsistent：持续的在所有Executor之间分配分区 // ConsumerStrategies：选择如何在Driver和Executor上创建和配置Kafka Consumer // ConsumerStrategies.Subscribe：订阅一系列主题 def getKafkaStream(topic: String,ssc:StreamingContext): InputDStream[ConsumerRecord[String,String]]=&#123; val dStream = KafkaUtils.createDirectStream[String,String](ssc, LocationStrategies.PreferConsistent,ConsumerStrategies.Subscribe[String,String](Array(topic),kafkaParam)) dStream &#125;&#125; 1.2.3 工具类 RedisUtil1234567891011121314151617181920212223242526object RedisUtil &#123; var jedisPool:JedisPool=null def getJedisClient: Jedis = &#123; if(jedisPool==null)&#123;// println("开辟一个连接池") val config = PropertiesUtil.load("config.properties") val host = config.getProperty("redis.host") val port = config.getProperty("redis.port") val jedisPoolConfig = new JedisPoolConfig() jedisPoolConfig.setMaxTotal(100) //最大连接数 jedisPoolConfig.setMaxIdle(20) //最大空闲 jedisPoolConfig.setMinIdle(20) //最小空闲 jedisPoolConfig.setBlockWhenExhausted(true) //忙碌时是否等待 jedisPoolConfig.setMaxWaitMillis(500)//忙碌时等待时长 毫秒 jedisPoolConfig.setTestOnBorrow(true) //每次获得连接的进行测试 jedisPool=new JedisPool(jedisPoolConfig,host,port.toInt) &#125;// println(s"jedisPool.getNumActive = $&#123;jedisPool.getNumActive&#125;") // println("获得一个连接") jedisPool.getResource &#125;&#125; 1.3 配置1.3.1 config.properties123456# Kafka配置kafka.broker.list=hadoop102:9092,hadoop103:9092,hadoop104:9092# Redis配置redis.host=hadoop102redis.port=6379 1.4 样例类 case class Startup 123456789101112131415case class StartUpLog(mid:String, uid:String, appid:String, area:String, os:String, ch:String, logType:String, vs:String, var logDate:String, var logHour:String, var logHourMinute:String, var ts:Long ) &#123;&#125; 1.5 业务类RealtimeStartupApp 12345678910111213141516object DauAPP &#123; def main(args: Array[String]): Unit = &#123; val conf: SparkConf = new SparkConf().setAppName("Dau").setMaster("local[*]") val ssc = new StreamingContext(conf,Seconds(5)) val inputStream: InputDStream[ConsumerRecord[String, String]] = MyKafkaUtil.getKafkaStream(GmallConstant.KAFKA_TOPIC_STARTUP,ssc) inputStream.foreachRDD(rdd=&gt; println( rdd.map(_.value()).collect().mkString("\n")))ssc.start() ssc.awaitTermination() &#125;&#125; 启动jsonMocker 和 logger-cluster.sh 观看结果 redis string list set hash zset uv user visit dau daily active user hash 存两个值 zset 排序 list 和set list 可以重复 set不可以重复 type set key dau：2019-06-03 value： mids redis 过滤日活的策略 redis的安装配置 电影推荐系统 设计原则 尽可能的较少redis的打开关闭的次数 redis的对接检查 redis的去重12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152object DauApp &#123; def main(args: Array[String]): Unit = &#123; val sparkConf: SparkConf = new SparkConf().setAppName("dau_app").setMaster("local[*]") val ssc = new StreamingContext(sparkConf,Seconds(5)) val inputDstream: InputDStream[ConsumerRecord[String, String]] = MyKafkaUtil.getKafkaStream(GmallConstant.KAFKA_TOPIC_STARTUP,ssc) // inputDstream.foreachRDD&#123;rdd=&gt; // println(rdd.map(_.value()).collect().mkString("\n")) // &#125; // 转换处理 val startuplogStream: DStream[Startuplog] = inputDstream.map &#123; record =&gt; val jsonStr: String = record.value() val startuplog: Startuplog = JSON.parseObject(jsonStr, classOf[Startuplog]) val date = new Date(startuplog.ts) val dateStr: String = new SimpleDateFormat("yyyy-MM-dd HH:mm").format(date) val dateArr: Array[String] = dateStr.split(" ") startuplog.logDate = dateArr(0) startuplog.logHour = dateArr(1).split(":")(0) startuplog.logHourMinute = dateArr(1) startuplog &#125; startuplogStream.foreachRDD&#123;rdd=&gt; //executor rdd.foreachPartition &#123; startuplogStr =&gt; val jedis: Jedis = RedisUtil.getJedisClient for (startuplog &lt;- startuplogStr) &#123; val key = "dau:" + startuplog.logDate val value = startuplog.mid jedis.sadd(key, value) println(startuplog) &#125; jedis.close() &#125; &#125; ssc.start() ssc.awaitTermination() &#125;&#125; 2 ES的安装2.1 为什么用ES最强的是sql 很复杂的业务 Hive比mysql更强 spark mr rdd mysql sql写不出来 可以写 最差的是redis和hbase Es有自己的语言 ES的安装 为什么用yml 用户自定义函数 全文检索 ES比较强 关联查询 join sql 关系型数据库 ES索引 ES 复杂的查询 mysql 元数据细 写入的速度 redis最快 hbase Hive mysql es 最慢 建索引 工作复杂 不直接写到磁盘 缓冲区到磁盘 其实有延迟 数据块但是放弃了一致性 业务操作 关系型数据库容量低 严谨NoSql 严谨 mysql 数据存储 老二 Oracle老大 明细 对数据的容量的要求 英语单词的学习 英语如何学习 hbase es 更好 集群 单机 ES 3台机器里 6.8 不是6.8 装演示安装 6.6 的版本 之前的版本 5 和6 重要的不通电 解压缩 cluster-name 集群名称 天然集群 es的三个颜色 是不是集群 yellow 亚健康状态 green状态 搭建集群最简单的 最复杂的集群是mysql 官方的集群 redis的集群 说明集群的名称是一样的 一家的名字 名字不能一样 每台机器的真实的ip地址 三台机子 es的安装说明2.2将数据保存到es字段的类型 a b c name:: zhangsan string json { “name”:”zhangsan” } kibana的安装与使用 添加的字段 如何使用 1234567891011121314151617181920es5.x ex6.x ex7.x database indextable type index docrow documentcolumn field名字 彼此之间的区分es documentfield6 系列 表的结构的概念取消了7 的时候type 就了document和field不纠结5的架构了string&#123;&#125; 聚合的字段必须使用 keyword es查询之前准确的定义字段 不能精确的推断 第三章 cannal1 为什么使用canal1.1 作用同步sql 做拉表 更新redis 某些情况无法从日志中获取信息 ，同时也无法利用sqoopp、等ETL工具实现对数据的实时监控 1.2 canal的工作原理工作原理简单 把自己伪装成salve 从master复制信息 1.3 了解一下mysql的binlogmysql二进制日志可以说是mysql最重要的日志了 他记录了dml 和ddl语句，以事件的形式记录，还包括语 语句所消耗的时间 开启二进制日志会造成1%的性能的消耗 1.3..2二进制文件的应用的场景Mysql Replication 在Master端开启binlog master把这个文件传递给slaves 达到master-slaves 数据一致的目的 其二 自然就是数据恢复了 通过mysqlbinlog工具来恢复数据 、二进制日志包括两类文件 二进制索引文件 二进制日志文件 ddl语句和dml语句 数据导入到 kibina的使用 es和mysql是正好相反 es速度快 所有的字段 mysql不会给你建立索引 字段的分类 建立索引但是不分词 jest jedis 操作es jest 操作es的 说一下es ES的保存2 mysql的安装3 canal的安装4 es的保存2批量插入ES 12 ES查询总数数据通过接口发布出来 java 日活的总数 分时间点的日活 ES中怎么写 12 查询的数据 将查询的数据发布成接口 发布数据的模块 新建工程spring 项目 com.atguigu.gmall1205 建立 **** es的分时查询]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>项目实战</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[社交项目]]></title>
    <url>%2Fblog4%2F2019%2F06%2F17%2FSocialProject%2F</url>
    <content type="text"><![CDATA[1项目介绍1.1 扯淡需要搭建成服务的功能制作镜像—-》》 docekr 镜像做容器微服务与容器的关系springCloud 实现微服务之间的通信spring全家桶按照传统的方式 UML 建模语句大的体系统一建模语言 power designer 建模数据库干什么用的 谈业务用的 公司 做了开发 什么也不用管开发出来 投资方 讲架构 UML建模语言 所有的类 powerdesigner 电脑安装一个 建模相关的东西 他们用这个谈公司 谈投资 1.2 系统架构设计简化了开发 专注于开发就好 nginx apache公司优化的发布软件的服务 nginx 解压可用 解压到没有空格的目录 json格式数据 返回json数据 前后端的约定 传这个返回这个 自己设定的状态码 1.3 RESTful的开发风格为了restful而生 spring全家桶 在spring全家桶用的越来越多 增删改查 4钟方法 get put delete post get 安全 幂等 获取表示 变更时获取表示 Post 不安全 且不幂等 插入 数据库来了一条 做了重复操作 Put 不安全 幂等 同一条更新语句同时执行了两次 Delete 不安全 幂等 删除资源 Docker 创建mysql 开发微服务 连一个公用的mysql mysql 下载镜像 pull即可 mq消息队列 dock pull 。。。。 配置国内的源 国内的服务器有3个 目前的还行 连接 配置ip地址 1.3 创建mysql的微服务测试工具PostMan 父工程 1.4 配置商品sringboot的仓库1234567891011121314151617181920212223242526272829303132333435363738&lt;!-- 配置maven的默认仓库--&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;spring-snapshots&lt;/id&gt; &lt;name&gt;Spring Snapshots&lt;/name&gt; &lt;url&gt;https://repo.spring.io/snapshot&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;spring-milestones&lt;/id&gt; &lt;name&gt;Spring Milestones&lt;/name&gt; &lt;url&gt;https://repo.spring.io/milestone&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;spring-snapshots&lt;/id&gt; &lt;name&gt;Spring Snapshots&lt;/name&gt; &lt;url&gt;https://repo.spring.io/snapshot&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;pluginRepository&gt; &lt;id&gt;spring-milestones&lt;/id&gt; &lt;name&gt;Spring Milestones&lt;/name&gt; &lt;url&gt;https://repo.spring.io/milestone&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; 12 ResponseBoby javabean –&gt;&gt;json RequestBody json—&gt;&gt;javaBean Result类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566&lt;!-- springboot最基本的架构--&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;!-- 配置maven的默认仓库--&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;spring-snapshots&lt;/id&gt; &lt;name&gt;Spring Snapshots&lt;/name&gt; &lt;url&gt;https://repo.spring.io/snapshot&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;spring-milestones&lt;/id&gt; &lt;name&gt;Spring Milestones&lt;/name&gt; &lt;url&gt;https://repo.spring.io/milestone&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;spring-snapshots&lt;/id&gt; &lt;name&gt;Spring Snapshots&lt;/name&gt; &lt;url&gt;https://repo.spring.io/snapshot&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;pluginRepository&gt; &lt;id&gt;spring-milestones&lt;/id&gt; &lt;name&gt;Spring Milestones&lt;/name&gt; &lt;url&gt;https://repo.spring.io/milestone&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; common模块 1.5 封装对象Result1234567public class Result &#123; private boolean flag; private Integer code; private String message;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859多了object类public class Result &#123; private boolean flag; private Integer code; private String message; private Object data; public Result(boolean flag, Integer code, String message) &#123; this.flag = flag; this.code = code; this.message = message; &#125; public Result(boolean flag, Integer code, String message, Object data) &#123; this.flag = flag; this.code = code; this.message = message; this.data = data; &#125; public Result() &#123; &#125; public boolean isFlag() &#123; return flag; &#125; public void setFlag(boolean flag) &#123; this.flag = flag; &#125; public Integer getCode() &#123; return code; &#125; public void setCode(Integer code) &#123; this.code = code; &#125; public String getMessage() &#123; return message; &#125; public void setMessage(String message) &#123; this.message = message; &#125; public Object getData() &#123; return data; &#125; public void setData(Object data) &#123; this.data = data; &#125;&#125; 注意是什么有参构造 1.6 分页对象1234567891011121314151617181920212223242526272829303132PageResultpublic class PageResult&lt;T&gt;&#123; private long total; private List&lt;T&gt; rows; public PageResult() &#123; &#125; public long getTotal() &#123; return total; &#125; public void setTotal(long total) &#123; this.total = total; &#125; public List&lt;T&gt; getRows() &#123; return rows; &#125; public void setRows(List&lt;T&gt; rows) &#123; this.rows = rows; &#125; public PageResult(long total, List&lt;T&gt; rows) &#123; this.total = total; this.rows = rows; &#125;&#125; 2的12次方 雪花算法 提供了分布式ID生成器 IdWorkder 基础微服务 增删改查 1.7 跨预处理CrossJoin123456789@SpringBootApplication@CrossOrigin //跨预处理public class BaseAppApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(BaseAppApplication.class); &#125;&#125; 1.8 docker 安装mysql5.6前提 centos7 123456docker search mysqldocker pull mysql:5.6docker run -p 3306:3306 --name mysql -v /mysql/conf:/etc/mysql/conf.d -v /mysql/logs:/logs -v /mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.6 含义的讲解 -v 代表挂载前面的是Linux主机 后面的是 对应mysql的端口 启动容器的命令 编写简单的CURD 运行镜像 docker exec -it MySQL运行成功后的容器ID /bin/bash 1.9 根据标签编写CRUD代码分布式开发必须加序列化 使用IO流在不同的平台传输 PostMan 异常处理 controllerAdvice postMan的使用方法 发送消息 接受信息 访问 异常处理的类 添加 int i=1/0 在postman上显示false 内部用 不会在其他的模块用 service dao pojo 三个模块就够了 @PathValiable 12@RestControllerAdvice@ExceptionHandler(value = Exception.class) 1234@RequestMapping(method = RequestMethod.POST) @RequestMapping(value="/&#123;labelId&#125;",method = RequestMethod.GET) @RequestMapping(value="/&#123;labelId&#125;",method = RequestMethod.GET) public Result findById(@PathVariable("labelId") String labelId) 第二天 查询与缓存2 .1基础微服务 条件查询2.1.1 启动docker1systemctl start docker docker start id name 2.1.2 写的方法 先写service 再写dao.+ 2.2 分页条件查询12345678 @RequestMapping(value="/label/search/&#123;page&#125;/&#123;size&#125;",method = RequestMethod.POST)public Result pageQuery(@RequestBody Label label,@PathVariable int page,@PathVariable int size)&#123; Page&lt;Label&gt; pageData=labelService.pageQuery(label,page,size); return new Result(true,StatusCode.OK,"查询成功",new PageResult&lt; Label&gt;(pageData.getTotalElements(),pageData.getContent()));&#125; 招聘代码的生成 、什么是DockFile DockFile的镜像 构建卷积神经网络 热门企业 hql jpql业务说明 hql的差别 热门企业 推荐职位 最新职位 必须要写sql语句 热门企业列表 推荐职位列表 最新职位列表 问答微服务开发 中间表 联查 中间表 @Query的用法 直接写sql语句 直接写jpq语句 现在sqlyog上写 然后在代码里面写 文章微服务开发springData JPA的联系结束 springdata 的使用 dockketfile ​ 0 想其他的方法决绝现在面对 springCache的使用 springdataRedis的使用、 Docker 安装redisdocker pull redis docker image redis mkdir /redis/data docker run -p 6379:6379 -v /redis/data : /data -d redis redis-server docker exec -it xxx redis-cli /bin/bash 1docker exec -it 43f7a65ec7f8 redis-cli spring Cache 缓存无法设置过期时间 运用的时间 场景 spring Cache 不破坏模块 面试题 缓存 项目那一部分用到缓存 一个redis 一个springCache 需不需要设置过期时间 单点登录global session 不是真的全局session 整个servlet 一个session对应一个servlet容器 不能一个session被多个servlet共用 项目代码 第三方的分布式项目 jwt cas 有状态的登录服务端储存信息 追随者无状态的流行 jwt 无状态登录jwt越来越流行了 一次登录 在分布式的任何形式都可以使用 qq音乐，一系列的软件都登录了 流行了 服务器端不存 登录信息 存的话 session存 客户端 服务端 客户端 cookie 服务器端 session cookie的id 和session的id你对应 服务端没东西 没有状态 知识验证有没有这个东西 day03 的文档型数据库复习 新的知识点 mongodb 有可能问到 随着大数据的普及 mongoDB就是为了大数据而生的 大数据班 大数据工程师 java的高级工程师 完全不会java做大数据 不行的 深圳 最早开大数据 一直到项目二 到了最后转了大数据的相关的东西 mongoDB的特点和体系结构 实际开发 不会直接敲命令操作 java端操作mongodb springDataMongoDB完成吐槽服务的开发 最重要的 MongoDB 什么时候用Oracle Mysql 关系型数据库 redis 菲关系型数据库 mongoDB菲关系数据库 但是最像关系型数据库 关系型数据库和非关系型数据库之间的数据库的关系关系型数据库 表与表之间有关系 一对多 对对多 多对多 中间表关联 什么时候用mongoDB1 数据量大 2 写入操作频繁 3 价值低 淘宝买东西 什么是MongoDB跨平台 面向文档 java 面向对象 java中 关系型数据库 一个集合 MongoDb中的一个文档 类似json的Bson格式 对json的扩展 操作的很多数据可以认为是你json MongoDb的特点支持很多种语言 Oracle Connection 对象 username password 这些语言不能用Oracle、 mysql 对mongoDb的支持 java javascript操作 一般不建议 Ertlong和.net语言基本不怎么用了 主流语言java 体系结构 集合多个文档的集合 表 document collection database MongoDB的数据类型的介绍null {“x”:null} true false {“x”:true} 数值 {“x”:3.14} {“x”:3} bson中 都是浮点型对于数值更加严格{“x:NumberLong(3)} 字符串： UTF-8的字符串 表示 {x：”/[abc]”/} ^$ 数组 {“x”:[a,b,c]} 对象ID 安装mongodb 安装 配置环境变量 bin 然后启动服务端 启动客户端 mongo命令 ![1569739678347]SocialProject/1569739678347.png) 表的结构 直接写id mongodb相当主键 关系型数据库 吐槽业务的说明吐槽 丢了 无伤大雅 选型mongoDB mongoDB的吞吐量比关系型数据库大的多 id 内容 日期 回复数 可见 上级id 无限制吐槽 复杂 类似于Oracle数据库的emp表 树型结构 一张表展现出n层的结构 优先考虑emp表 吐槽 回复 回复的人 再次回复 创建数据库和集合docker 连接Mongo ![1569740634113]SocialProject/1569740634113.png) ![1569740793266]SocialProject/1569740793266.png) 插入数据 1234 db.spit.insert(&#123;_id:"1",content:"我还是没有想明白到底为啥出错",userid:"1012",nickname:"小明",visits:NumberInt(2020)&#125;);db.spit.insert(&#123;_id:"2",content:"加班到半夜",userid:"1013",nickname:"凯撒",visits:NumberInt(1023)&#125;);db.spit.insert(&#123;_id:"3",content:"手机流量超了咋办？",userid:"1013",nickname:"凯撒",visits:NumberInt(111)&#125;);db.spit.insert(&#123;_id:"4",content:"坚持就是胜利",userid:"1014",nickname:"诺诺",visits:NumberInt(1223)&#125;); docker 构建mongo 123456docker ps -a在机器上启动mongodocker run -p 27017:27017 -v /mongo/db:/data/db -d mongo + 镜像id在容器上启动客户端docker run -it mongo mongo --host 192.168.2.110 docker run -di –name=tensquare_mongo -p 27017:27017 mongo Day04ElasticSearch安装直接解压 9200端口和9300端口 直接解压进入bin目录 开箱即用 elasticsearch Restfule操作ES head插件的安装Day05 rabbitMQRabbitMQ的简介kafka&gt; RabbitMQ &gt;activeMQ 哪个快 哪个安全 RabbitMQ 中间点 大数据 kafka java RabbitMQ 消息队列 知道几种 最安全的rabbitmq的 电商用RabbitMQ也有 但是比较少 保证订单足够的安全 \使用ActiveMQ 追求效率高的话RabbitMQ 金融可以用RabbitMQ RabbitMQ Erlang AMQP 高级消息队列 可靠 灵活 支持消息集群 管理界面 rabbitMQ的架构发送消息 RabbitServer 送快递 100个人 ![1569743989061]SocialProject/1569743989061.png) 消息的发送者 消息的接受者 ActiveMQ 100个消息 一个队列 消息发给交换器 直接 分裂 主题 三种连接模式 经过交换器 RabbitMQ和ActiveMQ的最重要的区别 主要的概念RabbitMQ server Producer Consumer Exchange Queuue Routing Key windows条件下安装rabbitMQ Docker 安装rabbitMq、docker pull rabbitmq ​ b9e17734a1b2 1docker run -d -p 5672:5672 -p 15672:15672 --name rabbitmq rabbitmq + 镜像 12docker run ‐di ‐‐name=tensquare_rabbitmq ‐p 5671:5617 ‐p 5672:5672 ‐p4369:4369 ‐p 15671:15671 ‐p 15672:15672 ‐p 25672:25672 rabbitmq]]></content>
      <categories>
        <category>javaEE</category>
      </categories>
      <tags>
        <tag>项目实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卡扣项目]]></title>
    <url>%2Fblog4%2F2019%2F06%2F12%2FCarsListenerProject%2F</url>
    <content type="text"><![CDATA[车流量监控项目1 车流量监控介绍1.1 数据的采集1.1.1 数据从哪里来埋点卡口信息的话 每次拍摄信息都会传到服务端 网站或者页面设置埋点 socket后台获取 和前端开发人员联系好 卡口数据 厂商约定 Flume监控指定的文件夹 转移到HDFS里 大部分在Hive Hive有计算的negligible 还有一条流程 实时数据 实时数据的话 通常在分布式消息队列中读取，如kafka 实时的log 实时的写入消息队列，实时从kafka读取数据 log日志 Flume1.2 模块介绍1.2.1 卡流量分析 SparkCore功能点 top5 获得卡扣号通过的车最多 使用架构 卡口车流量转化率 Spark Core 各区域车流量最高top5的道路统计 Spark SQL 稽查布控 道路实时拥堵统计 Spark Streaming 一个卡扣号对应多个摄像头 基础数据介绍 大数据开发流程 需求分析 功能需求]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>项目实战</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作系统]]></title>
    <url>%2Fblog4%2F2019%2F05%2F12%2FOperatingSystem%2F</url>
    <content type="text"><![CDATA[操作系统第一章 概述1课程概述 1.1 课程介绍基本概念及原理 中断及系统调用 进程和线程 内存管理 进程及线程 调度 同步 1.2 操作系统实验 准备 启动 物理内存的管理 虚拟内存 进程及线程 内核线程管理 用户进程管理 cpu调度 同步和互斥 文件系统 #################### 如何设计文件系统 操作系统和真实环境的比较 真正的比较计算机 1.3 预备知识计算机结构原理 数据结构 C和汇编语言 2 什么是操作系统没有一个精确的定义 2.1 功能上 用户角度 控制软件 ​ 管理应用程序 ​ 服务 ​ 杀死应用程序 对下 资源管理 分配资源 管理外设 2.2 功能的抽象 2.3 操作系统的层次架构 软件 应用软件 系统软件 系统 功能 操作系统 编译器 共享的程序库 操作系统 硬件管理和控制 面向应用程序 shell windows GUI 操作系统暴露的接口 kernel在shell之下 操作系统内核的角度看问题 cpu 内存 磁盘 内存 物理内存 虚拟内存 技术手段 在有限的物理内存之下 disk 磁盘块 底层 抽象出文件系统 中断处理 和设备驱动 2.4 操作系统的特征1 并发 一个cpu 并行 同时执行 多个cpu 存在多个应用程序 OS管理和调度 2 共享 同时访问资源 互斥共享 A B 两个块 如何共享 3 虚拟 多道程序技术 一个计算机 虚拟出多台机器 4 异步 什么时候结束 输出的结果是相同的 程序走走停停的 但是OS要保证运行的结果相同 3 为什么学习操作系统3.1综合课程编程语言 数据结构 算法 计算机的体系结构 材料 操作系统概念和原理 源代码 技能 操作系统的设计和实现 3.2 已经有何很多操作系统 要不要学操作系统 底层的操作 操作系统进行设计 改进和扩展 分析了解和掌握 很多知识 能力 Windows Linux 硬件发展不断的变化 一直向前发展 学习 atguigu sudo atguigu用户 ip地址和防火墙 我听发哦的我会忘记 3.3 操作系统 计算机研究的基石之一 顶级的计算机操作系统 庞大的操作系统 支持底层的架构 ACM 学术界 工业界 顶级会议 实际的操作系统 很大 很复杂 并发导致的编码困难 硬件的错误 管理计算机系统的核心软件 对算法的设计提出了要求 操作系统出错 意味着计算机出错 操作系统 是安全的基础 原理和概念 不代表操作系统主要关注的点 算法 关键部分的一个小部分 IO的磁盘调度 进程调度 关注小了 滞后当前研究和产业现状 权衡 时间和空间 空间和时间 操作系统需要权衡 中断 异常 IO 如何工作 如何对硬件的特征处理 上层的概念和理解 理解C代码和汇编代码 系统级的平衡 具体的角度 4 如何学习操作系统我看到的我能记住 我能做的 更深入的了解 动手操作 实现 尝试完成实验提出的要求 5 操作系统实例 面向桌面的操作系统 面向服务器的操作系统 手机 移动中断 工控领域 Unix BSD操作系统 Unix 基于C语言的Linux系统 改变世界 BSD 在unix的基础上发展 Linux BSD 发行软件 在网站的协议 广大的发展 基于Unix BSD Solaris IOS 惠普 Linux操作系统 一个学生学习计算机知识 用一个操作系统 经历了漫长的发展 很多的操作系统基于Linux Ubuntu Linux内核 都是基于Linux内核开发的 Windows操作系统 最早是dos IBM 个人计算机 GUI 图形化界面 桌面 服务器 手机 服务器和终端 微软 —-》》 大众完成特定的事情 计算机的推广 实施操作系统 6 操作系统的历史和发展计算机硬件 1921 2012 年 计算机发生了很大的变化 科学家 —-》》 日常 6.1 早期的计算机计算机 早期的计算机 输入 计算机 输出 单用户的处理过程 CPU的计算能力强 计算过程流水线化 批处理阶段 6.2 顺序执行和批处理 6.3 内存容量增大 CPU执行多个程序 cpu很贵 程序执行 操作 IO IO的操作效率 远远比不了CPU的速度 多个IO操作 Read 让Write 使用CPU的资源 中断 通知操作系统 程序2 停止 程序1 执行 调度 切换 交互性不够好 与计算机进行交互 分时 千分之一秒产生一次分时 第一个程序占 交给第二个 程序 人的反应速度慢 将时间分成很小的时间段 有了分时调度 人们可以更方便的执行计算机程序 中断 6.4 个人电脑操作系统80 年代 一个一般用户用一台电脑 价钱提高 功能提升 文字处理 数据存储 提高IO的交互性 两个趋势 集成电路的发展 一个cpu集成多个cpu核 网络得到了飞速的发展 分布式操作系统 用户前端 数据中心 Internet Internet 松紧 耦合 系统 操作系统的演变 6.5 计算机的未来的发展趋势大数据 云计算 大量的嵌入式设备 很多的工作交给计算机来处理 给人的学习方便 6 7 操作系统结构简单的操作系统 比较弱 操作系统 x80 8086机器 硬件的限制 计算机很难有突破 早期面向个人的计算机 服务器 计算机 更高层次的设计 Unix操作系统 C语言 实现 C语言方便移植 获得图灵奖和美国总统访问 向下 向上 分析uCore操作系统 实际的OS在硬件上是如何发展的 把内核尽量变的尽量小巧 微内核的设计 是一种服务的形式实现 松耦合的架构 地址隔离 无法 影响彼此 数据导内核 内核 产业界 很少采用微内核 性能 操作系统的设计分成两块 Office 面向office 的Linux Kernel IOS Kernel 完成对硬件管理 速度 OS 虚拟出操作出多台操作系统 计算机的发展 CPU的计算能力快速发展 充分分发挥计算机的效率 8 小结什么是操作系统 为什么学习 如何学习操作系统 操作系统实例 操作系统结构 第二章2.1 启动 中断 异常 和 系统调用 启动 接口 控制外设 中断 异常 系统调用 实现 中断 异常 和 系统调用 CPU Memory IO Disk 存放OS BIOS 基本IO处理系统 检查外射 加载软件 OS 放在OS Bootloader 加载OS的 BIOS 干什么 特定的地址开始执行 地址是固定的 CS 和 IP CS 段寄存器 IP 指令指针寄存器 POST（加电自检） 寻找显卡 外射 把bootloader 硬盘放到内存中去 bootloader 代码块 加载到内存 控制权交给os os的最起始的地址 操作系统与设备程序交互 interface 中断IO 系统调用 异常 外射 中断 外设 异常和系统调用 应用程序 操作系统 特殊的软件 可以信任的软件 屏蔽底层的复杂性 中断来源于外设 键盘 鼠标 网卡 声卡 异常 应用程序意向不到的行为 系统调用 应用程序强求操作提供服务 读写文件 网络包 系统调用 中断 异步 异常 同步 系统调用 异步或者同步 发出请求 返回的时间是异步的 发数据 马上做其他的事情 完成异步发送完成的请求 你做一件事 等一会返回结果 做其他的事情 系统调用不会重复的使用 2.2 中断 异常和系统调用 表 key 中断号 特定的编号 对应的地址 软件 保存当前的处理状态 终端服务程序 清除中断标记 异常 异常编号 保存 现场 异常处理 杀死异常的程序 重新执行异常指令 恢复线程 接口 接口 系统调用 应用程序 成功 失败 程序访问使用 API实现相应的操作系统的服务 定义哪些系统调用 操作系统的实现 应用程序 提供什么样的功能 Library 访问系统调用的接口 用户态 应用程序执行的特权级的状态 不能完全控制计算机 内核态 CPU运行状态 可以执行任何的指令 完全控制 用户调用兄消退给你调用 CPU将系统调用的权利返回给用户 函数调用 系统调用 用户态和内核态的转换 开销 相应 的汇报 安全 可靠 跨越操作系统的边界 操作系统 外设 整个系统安全可靠 正常的处理应用程序 操作系统有自己的堆栈 维护堆栈 退出 堆栈保存 进入 恢复 应用程序 操作系统不信任程序 对参数做检查 基于安全层面的考虑 从内核态导入用户态 内存空间的数据拷贝 第三章 计算机操作系统及 内存分配体系3.1 计算机操作系统及 内存分配体系主要的3部分 CPU 内存 程序代码 IO 外设 内存的层次结构 抽象 内存中运行不用考虑底层的细节 访问一个联系的地址空间 逻辑地址空间 保护 有个不同的应用程序 隔离的机制的实现 进程之间的交互 当内存中放了很多应用程序 最需要内存的数据放在内存中 其余临时放到磁盘上去 应用程序透明 P1 P2 P3 P4 P4的数据没必要放到内存 逻辑地址空间 物理地址空间 程序重定位 分段 虚拟内存 按需分页虚拟内存 3.2 地址空间与地址生成地址空间 地址生成 、地址空间定义 地址生成 安全检查 地址空间 物理 硬件 内存条 磁盘 逻辑地址 一维线性 映射空间 C程序 汇编程序 C程序 变量的名字 地址 汇编程序 符号 代表函数和名字 汇编器 机器语言 大的程序 小的程序 Linker 小的变成大的程序 Loader 符号的地址 —–》》》》 具体的逻辑地址 指令取出来 逻辑地址 查找逻辑地址 MMU 存在着映射关系 具体的物理地址在什么地方 一个指令 MMU 查找逻辑地址映射表 找到 主存 —-》》总线—-》》》CPU 起始地址 长度 cpu执行指令 逻辑地址是否满足区域的限制 地址安全监测 3.3 连续内存分配 3个内存分配算法内存的碎片空间 内部碎片 外部碎片 数据分配空间 分配算法 首次适配算法 最优分配算法 最差分配算法 最优适配算法那 避免了 3.4 连续分配空间 压缩式和交换式碎片 非连续分配和连续分配 非连续分配的缺点 如何建立虚拟地址和物理地址之间的转换 软件方案 硬件方案 分段机制 分页机制 分段的管理机制 计算机程序 各种各样的段组成 各种子程序 程序的分段地址空间 分段的寻址方案 段 更好的分离和共享 相应的分离代码段 数据的相对的隔离 左边连续的逻辑地址 右边分散的物理地址 逻辑地址空间 连续的字节流 代码 数据 堆 栈 软件实现 软件进行映射 开销很大 通过硬件的知识 分段地址方案 地址 一维的逻辑地址 表示的方法 段 Segment 段机制寻址的方式 Segment number 段寄存器和地址寄存器 都可以存在 映射机制 一维线性 映射到多为的物理地址 段的起始地址 段的长度 一般 地址在一个合法的空间之内的 段地址 + offsett CPU为了寻址做了很多工作 第四章 虚拟内存5.1 虚拟内存的起因 起因 程序运行的时候越来越不够用 电子游戏 小游戏 —》》 大游戏 计算机性能 想办法 更多程序跑在有限的内存中去 很大的存储器 更大 更快 更便宜的非易失性存储器 更有效的管理物理内训 更大 更快 更便宜 受限于内存的特质 新的内存 满足这个条件 更大 更快 更便宜 更不容易丢失内存 5.2 覆盖技术80年代产生 小的内存 运行大的程序 640k的大小 虚拟出更大的空间 常用功能 必选部分 可选部分 不存在调用关系 分时 例子 代码 只读 释放那空间 程序员进行管理 高出 开销 程序员 设计的开销 换入 换出操作 时间的开销 时间 早期的时候 使得大型软件的使用得以实现 Dos 各种各样的东西 5.3交换技术目标 内存管理单元 进程的地址空间 换出 外存中的数据 换出 进程的地址空间 内存到外存 换入 外存进内存 内存导出去 后端存储 硬盘 硬盘中的数据导回来 具体的实现复杂 交换技术考虑什么问题 交换时机的确定 ​ 内存空间不够的时候 多大的内存空间 ​ 换出空间被占用了 程序换入的重定位 动态地址映射 覆盖技术和交换技术 覆盖 共享一块内存区域 代价程序员的手动指定 交换 程序之间 换入换出一个程序 操作系统的内部完成 开销比较大 5.4虚拟技术覆盖技术的问题 程序员的管理 分析 告诉相应的处理技术 粒度太大 交换技术 进程的地址空间都交换出来 增加了处理器的开销 虚拟技术—目标 进程的部分内容 交换 不需要程序员的干涉 操作系统和MMU 程序具有局部性 程序在较短的时间范围之内 指令地址和指令的操作数地址 时间局部性 空间局部性 程序的局部性很好 程序的效率很高 操作系统利用局部性 虚拟内存的理想的管理状态 什么是缺页中断 第七章 进程进程的描述 进程的状态 线程 进程间通信 线程互斥与同步 死锁问题 进程 什么是进程 为什么用进程 执行的程序执行过程 跑一个程序 跑多个程序 程序的概念表示 多个程序的实例 更好的表示程序的执行的过程 编译 执行程序 代码段 数据段 执行文件的形式存在 动态执行的过程 7.1 进程的组成进程执行的功能 程序是产生进程的基础 多次执行程序]]></content>
      <categories>
        <category>计算机必备知识</category>
      </categories>
      <tags>
        <tag>计算机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka 的学习]]></title>
    <url>%2Fblog4%2F2019%2F03%2F12%2Fkafka%2F</url>
    <content type="text"><![CDATA[1 kafka概述2 kafka的集群部署3 Kafka的工作流程分析4 kafka的API实战4.1 环境准备4.1.1 zk集群和kafka集群启动 kafka集群打开一个消费者4.1.2创建maven 工程kafkaAPIpom.xml 12 ​ ​ ​ org.apache.kafka ​ kafka-clients ​ 0.11.0.0 ​ ​ ​ ​ org.apache.kafka ​ kafka_2.12 ​ 0.11.0.0 ​ 观察源码学习 替换 com.atguigu.lafka 4.2 生产者javaAPI4.3 消费者javaAPI5Kafka的producer烂机器6KafkaStreams7 扩展]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据框架</tag>
      </tags>
  </entry>
</search>
